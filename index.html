<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>bertifier_modeling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="BERTIFIER_Modeling_files/libs/clipboard/clipboard.min.js"></script>
<script src="BERTIFIER_Modeling_files/libs/quarto-html/quarto.js"></script>
<script src="BERTIFIER_Modeling_files/libs/quarto-html/popper.min.js"></script>
<script src="BERTIFIER_Modeling_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="BERTIFIER_Modeling_files/libs/quarto-html/anchor.min.js"></script>
<link href="BERTIFIER_Modeling_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="BERTIFIER_Modeling_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="BERTIFIER_Modeling_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="BERTIFIER_Modeling_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="BERTIFIER_Modeling_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<div class="cell" data-cellview="form" data-outputid="7e6aaa2b-e3de-4726-e0bd-2d82c5ee8a2f" data-execution_count="203">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#@title &amp;nbsp;</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display_html</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>style <span class="op">=</span> <span class="st">''''</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="st">    @import url('https://fonts.googleapis.com/css2?family=Comfortaa:wght@400;500;700&amp;display=swap');</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="st">    body {</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="st">        display: flex;</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="st">        justify-content: center;</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="st">        align-items: center;</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="st">        min-height: 100vh;</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="st">        margin: 0;</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="st">    .container {</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="st">        display: flex;</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="st">        justify-content: center;</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="st">        align-items: center;</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="st">        gap: 20px;</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="st">        margin-top: 30px;</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="st">    img {</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="st">        max-width: 100%;</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="st">        height: auto;</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="st">    h1 {</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="st">        margin: 0;</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="st">        size: 20px;</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>display_html(<span class="ss">f'''</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;style&gt;</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span><span class="sc">{</span>style<span class="sc">}</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;/style&gt;</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;div class='container'&gt;</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="ss">    &lt;img src='https://gcdnb.pbrd.co/images/hgNPk95VoyK6.png?o=1', width=200&gt;</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="ss">    &lt;h1&gt;Modeling&lt;/h1&gt;</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;/div&gt;</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;br&gt;</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;br&gt;</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="ss">'''</span>, raw<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    '
    @import url('https://fonts.googleapis.com/css2?family=Comfortaa:wght@400;500;700&display=swap');

    body {
        display: flex;
        justify-content: center;
        align-items: center;
        min-height: 100vh;
        margin: 0;
    }

    .container {
        display: flex;
        justify-content: center;
        align-items: center;
        gap: 20px;
        margin-top: 30px;
    }

    img {
        max-width: 100%;
        height: auto;
    }

    h1 {
        margin: 0;
        size: 20px;
    }


</style>

<div class="container">
    <img src="https://gcdnb.pbrd.co/images/hgNPk95VoyK6.png?o=1" ,="" width="200">
    <h1>Modeling</h1>
</div>
<br>
<br>
</div>
</div>
<section id="setting-up" class="level1">
<h1>Setting up</h1>
<div class="cell" data-execution_count="204">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install transformers demoji loguru</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="61a60c92-4f6d-4eaf-8c7a-a0072bc02023" data-execution_count="205">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> WordPunctTokenizer</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem <span class="im">import</span> WordNetLemmatizer</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> textwrap <span class="im">import</span> wrap</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> loguru <span class="im">import</span> logger</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> cuda</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> demoji</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    get_linear_schedule_with_warmup,</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    BertTokenizer,</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    BertConfig,</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    BertModel</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> (</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    SequentialSampler,</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    RandomSampler,</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    DataLoader,</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    Dataset</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> (</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    classification_report,</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>    confusion_matrix,</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>drive.mount(<span class="st">'/content/drive'</span>)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'/content/drive/MyDrive/Colab Notebooks/data/reviews_preprocessed.csv'</span>)</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>df.dropna(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> df.<span class="bu">all</span> <span class="op">!=</span> <span class="va">None</span>:</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Dataset imported successfully ðŸŽ‰'</span>)</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Dataset import failed â˜¹'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).
Dataset imported successfully ðŸŽ‰</code></pre>
</div>
</div>
</section>
<section id="quick-eda" class="level1">
<h1>Quick EDA</h1>
<div class="cell" data-outputid="a5e125f7-2bca-4ddb-caec-52378dd3a724" data-execution_count="206">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'stopwords'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="206">
<pre><code>True</code></pre>
</div>
</div>
<div class="cell" data-execution_count="207">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>stopwords <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">'portuguese'</span>))</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>punctuations <span class="op">=</span> <span class="bu">set</span>(string.punctuation)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>tokens_frequencies <span class="op">=</span> Counter([item <span class="cf">for</span> sublist <span class="kw">in</span> df[<span class="st">'tokens'</span>] <span class="cf">for</span> item <span class="kw">in</span> sublist.split() <span class="cf">if</span> item <span class="kw">not</span> <span class="kw">in</span> stopwords <span class="kw">and</span> item <span class="kw">not</span> <span class="kw">in</span> punctuations])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="722c9be7-e42c-4939-e330-58535ef09fee" data-execution_count="208">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>aux <span class="op">=</span> pd.DataFrame(tokens_frequencies.most_common(<span class="dv">10</span>), columns<span class="op">=</span>[<span class="st">'Item'</span>, <span class="st">'Quantidade'</span>])</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.bar(</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    aux,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">'Item'</span>,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span><span class="st">'Quantidade'</span>,</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">'Quantidade'</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>fig.layout.template <span class="op">=</span> <span class="st">'plotly_dark'</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>fig.layout.title <span class="op">=</span> <span class="st">'FrequÃªncia de itens comentados'</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">


<meta charset="utf-8">

    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.18.2.min.js"></script>                <div id="1c30e6e4-219d-4985-9ee9-bdba5d26fe46" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("1c30e6e4-219d-4985-9ee9-bdba5d26fe46")) {                    Plotly.newPlot(                        "1c30e6e4-219d-4985-9ee9-bdba5d26fe46",                        [{"alignmentgroup":"True","hovertemplate":"Item=%{x}<br>Quantidade=%{marker.color}<extra></extra>","legendgroup":"","marker":{"color":[4181,2826,2354,2018,1799,1434,1333,1209,1164,1131],"coloraxis":"coloraxis","pattern":{"shape":""}},"name":"","offsetgroup":"","orientation":"v","showlegend":false,"textposition":"auto","x":["'app',","'pedido',","'ma',","'aplicativo',","'pra',","'entrega',","'bom',","['o',","'vezes',","'op\u00e7\u00e3o',"],"xaxis":"x","y":[4181,2826,2354,2018,1799,1434,1333,1209,1164,1131],"yaxis":"y","type":"bar"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#f2f5fa"},"error_y":{"color":"#f2f5fa"},"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"baxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"line":{"color":"#283442"}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"marker":{"line":{"color":"#283442"}},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#506784"},"line":{"color":"rgb(17,17,17)"}},"header":{"fill":{"color":"#2a3f5f"},"line":{"color":"rgb(17,17,17)"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#f2f5fa","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#f2f5fa"},"geo":{"bgcolor":"rgb(17,17,17)","lakecolor":"rgb(17,17,17)","landcolor":"rgb(17,17,17)","showlakes":true,"showland":true,"subunitcolor":"#506784"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"dark"},"paper_bgcolor":"rgb(17,17,17)","plot_bgcolor":"rgb(17,17,17)","polar":{"angularaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","radialaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"yaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"zaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"}},"shapedefaults":{"line":{"color":"#f2f5fa"}},"sliderdefaults":{"bgcolor":"#C8D4E3","bordercolor":"rgb(17,17,17)","borderwidth":1,"tickwidth":0},"ternary":{"aaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"baxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","caxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"title":{"x":0.05},"updatemenudefaults":{"bgcolor":"#506784","borderwidth":0},"xaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Item"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Quantidade"}},"coloraxis":{"colorbar":{"title":{"text":"Quantidade"}},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"legend":{"tracegroupgap":0},"margin":{"t":60},"barmode":"relative","title":{"text":"Frequ\u00eancia de itens comentados"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('1c30e6e4-219d-4985-9ee9-bdba5d26fe46');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>


</div>
</div>
<div class="cell" data-outputid="611e1d94-21da-4150-b4ad-7d69d44001ea" data-execution_count="209">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>tokens_amount <span class="op">=</span> [<span class="bu">len</span>(tokens) <span class="cf">for</span> tokens <span class="kw">in</span> df.tokens.values]</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.histogram(</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    tokens_amount,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    nbins<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    color_discrete_sequence<span class="op">=</span>[<span class="st">'orange'</span>]</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>fig.layout.title <span class="op">=</span> <span class="st">'Distribution of token amount'</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>fig.layout.template <span class="op">=</span> <span class="st">'plotly_dark'</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">


<meta charset="utf-8">

    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.18.2.min.js"></script>                <div id="ef6ec8ae-66ff-43b2-9bf6-d0f06fb74d94" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("ef6ec8ae-66ff-43b2-9bf6-d0f06fb74d94")) {                    Plotly.newPlot(                        "ef6ec8ae-66ff-43b2-9bf6-d0f06fb74d94",                        [{"alignmentgroup":"True","bingroup":"x","hovertemplate":"variable=0<br>value=%{x}<br>count=%{y}<extra></extra>","legendgroup":"0","marker":{"color":"orange","pattern":{"shape":""}},"name":"0","nbinsx":100,"offsetgroup":"0","orientation":"v","showlegend":true,"x":[355,441,403,445,489,400,462,339,312,289,416,260,536,508,360,493,358,216,397,387,471,390,364,469,397,465,520,327,369,523,427,267,478,545,384,379,453,486,357,446,408,342,443,414,291,425,333,300,348,345,482,323,582,482,538,338,382,500,453,230,488,357,302,320,369,532,368,302,540,334,343,493,325,441,533,503,428,533,398,330,536,502,382,325,469,523,463,274,242,468,572,313,462,316,390,257,354,246,423,472,160,536,367,431,110,147,228,59,247,165,206,71,307,423,43,46,48,94,126,434,44,498,176,96,164,213,174,19,141,33,41,165,216,104,208,52,590,171,37,258,522,64,193,23,19,65,200,462,403,53,130,146,165,188,343,124,102,104,68,134,85,479,63,313,579,96,206,235,40,188,172,183,47,130,144,157,90,239,98,89,50,128,325,51,77,312,117,86,66,42,66,155,128,110,58,520,318,104,310,133,419,362,357,494,510,481,513,470,433,528,484,349,459,373,472,363,437,478,416,410,401,479,385,449,460,560,638,454,401,341,382,354,349,411,260,247,263,361,464,428,328,320,479,262,382,383,498,254,484,340,278,371,537,406,393,264,589,220,293,255,502,520,234,507,206,510,385,561,191,350,484,293,312,377,318,298,216,197,243,355,231,183,208,189,249,217,283,123,202,191,289,295,262,174,260,184,568,211,267,143,200,128,86,127,43,44,18,30,140,184,210,191,61,183,354,106,361,23,118,61,28,141,68,93,487,113,7,190,185,101,149,64,118,98,176,341,44,82,225,84,46,105,122,173,27,262,95,7,112,260,103,64,165,143,61,35,316,123,175,229,48,6,206,34,104,94,42,174,217,93,278,208,130,87,12,247,382,7,100,31,90,15,143,320,355,197,262,129,56,428,133,14,113,65,189,121,25,241,263,283,469,508,461,449,484,551,397,486,466,499,508,494,511,400,575,363,436,430,627,457,353,411,373,486,779,329,537,331,380,463,372,339,558,461,359,432,321,377,248,390,418,510,511,323,344,303,353,236,464,271,479,481,311,539,258,286,194,343,317,462,542,353,288,272,299,567,252,209,337,213,558,301,245,299,368,467,499,212,505,169,222,221,268,296,209,193,260,203,167,410,213,208,224,151,263,292,177,148,275,206,442,271,313,446,314,119,363,371,83,406,350,192,177,164,258,155,214,329,207,283,318,326,319,231,229,268,171,254,317,201,362,166,419,228,196,208,153,228,274,151,297,263,205,362,309,122,173,212,191,262,531,157,223,312,197,164,152,173,542,218,494,438,372,368,494,536,568,376,477,439,217,105,515,333,296,174,185,378,384,128,140,292,144,118,121,188,446,335,362,197,252,574,248,130,336,510,376,437,107,131,46,20,64,81,107,29,61,169,235,117,27,29,96,7,19,138,7,86,49,76,19,14,222,104,34,177,6,67,35,23,10,35,79,16,93,9,174,43,26,286,9,9,54,49,13,48,7,124,54,59,100,52,7,39,86,101,24,60,132,23,22,7,15,20,50,9,296,311,127,60,61,36,212,116,124,38,128,98,164,47,209,119,14,43,58,7,260,167,41,88,85,321,359,121,9,9,14,107,124,143,119,461,7,28,11,499,36,279,68,183,7,53,48,296,30,19,11,193,16,101,45,147,7,221,91,55,22,31,10,25,47,7,43,285,12,59,268,95,505,539,22,59,9,258,30,72,130,7,208,102,51,390,377,26,37,206,21,54,34,70,305,21,83,46,64,8,71,467,7,194,140,418,104,93,7,157,15,148,464,7,65,7,21,78,7,294,29,7,54,13,140,85,188,125,43,99,120,36,272,18,546,466,464,428,556,448,510,513,461,480,414,502,514,363,418,380,382,478,413,337,624,436,543,356,499,459,333,578,528,232,213,349,522,226,210,304,193,248,243,210,212,381,350,339,463,203,467,217,319,428,373,426,357,281,398,320,196,157,465,255,302,190,321,305,306,253,512,202,215,187,310,156,197,230,436,279,155,241,175,201,243,144,195,190,210,115,109,246,172,399,163,282,154,191,172,232,263,432,190,313,105,69,25,21,17,86,42,49,59,151,62,20,30,56,74,9,20,99,7,43,62,108,7,21,175,16,32,27,76,49,33,38,105,13,7,58,11,23,10,83,106,11,47,56,7,35,62,64,9,113,21,83,96,7,18,321,6,15,32,17,7,10,7,7,79,151,62,17,46,210,223,6,20,29,144,75,30,21,113,129,87,26,14,9,7,7,102,89,26,73,28,22,7,43,7,7,42,9,201,20,507,247,269,232,437,359,237,282,351,250,257,194,185,345,145,338,353,378,278,270,226,378,194,549,192,545,246,187,253,476,404,342,210,171,214,388,281,295,177,234,163,157,191,150,281,163,228,246,116,159,127,205,140,178,392,238,196,202,203,265,264,333,177,210,139,511,300,284,186,361,224,226,337,167,384,98,250,155,102,156,146,143,218,184,170,206,213,147,193,143,157,323,457,265,167,194,225,116,192,177,43,9,35,8,6,9,23,67,34,54,9,13,19,91,54,7,27,8,19,48,93,7,51,16,7,20,8,31,49,30,55,41,67,50,7,15,7,49,33,31,24,75,74,7,56,9,86,106,53,11,20,29,13,34,17,93,45,26,56,14,82,41,47,31,53,16,7,42,53,13,49,13,16,7,9,15,35,66,63,6,36,7,77,9,14,41,136,246,19,19,14,9,55,6,10,66,9,56,32,9,523,544,228,253,329,220,330,434,232,190,337,178,434,276,307,178,552,204,194,146,246,187,268,417,445,297,373,405,434,208,512,178,343,264,215,476,235,191,302,195,136,244,165,231,273,203,237,393,505,115,216,169,182,165,302,216,404,436,375,338,443,527,375,161,191,418,373,297,338,139,425,536,426,343,130,262,126,198,309,441,235,317,397,290,118,83,105,103,123,89,142,403,129,147,140,233,557,88,500,353,25,176,19,307,41,244,79,89,544,57,198,11,51,552,129,146,399,74,142,434,20,151,291,499,204,37,403,204,86,178,48,523,42,130,232,89,190,131,147,27,28,194,434,42,109,520,187,233,15,171,198,44,456,43,178,57,103,139,32,337,61,11,121,31,16,44,36,220,123,136,97,34,36,65,63,178,228,196,112,83,329,121,417,77,266,8,118,75,75,302,19,137,51,76,369,61,7,124,276,27,344,430,326,188,305,130,500,293,373,347,169,294,238,185,160,524,145,427,238,188,409,286,152,472,437,62,488,384,263,260,353,390,291,231,361,181,273,192,486,157,211,170,325,147,232,74,89,283,145,223,472,341,310,199,261,228,279,275,326,220,468,284,307,229,152,203,301,240,198,257,206,207,227,157,286,193,151,239,177,147,124,226,129,200,147,208,250,165,168,149,99,306,149,224,71,181,127,222,391,197,305,42,62,188,83,145,52,137,27,430,48,9,130,84,10,60,344,238,160,89,524,31,326,40,169,153,28,99,185,50,36,21,99,26,188,183,32,24,9,143,74,71,71,55,500,84,30,97,177,152,46,71,232,12,58,152,56,343,157,10,44,20,192,79,40,94,187,61,488,42,147,134,48,38,64,8,325,73,45,384,148,25,132,65,192,52,147,181,104,58,75,263,156,231,62,7,208,230,124,28,515,331,201,379,157,334,176,214,120,298,109,471,400,493,379,162,319,236,252,170,246,267,478,453,504,298,71,173,147,106,305,136,341,310,137,106,105,314,238,325,72,245,234,522,223,207,280,243,211,266,252,238,224,214,243,143,289,139,113,101,111,133,213,126,72,316,98,147,99,185,196,154,203,209,260,303,209,302,196,182,186,182,185,241,244,192,245,160,229,420,171,66,146,142,241,238,187,238,235,247,125,249,123,140,114,141,165,345,142,473,140,167,115,486,90,47,68,516,162,173,382,504,144,415,507,46,108,397,558,155,151,157,526,268,458,114,492,508,283,514,524,432,490,292,284,543,383,389,464,502,502,275,294,399,229,302,320,250,115,111,173,284,334,329,320,282,329,288,213,371,300,112,325,114,367,379,123,73,152,98,428,376,341,277,537,346,61,502,288,535,318,248,237,130,442,133,109,174,84,114,101,238,73,52,515,39,37,68,47,101,120,20,126,157,75,74,64,27,106,14,106,201,331,7,173,90,298,46,61,34,72,48,109,23,72,106,176,71,40,138,11,108,7,23,66,101,75,93,86,18,265,33,108,334,22,214,37,96,111,105,61,45,21,113,14,76,162,26,44,106,37,99,70,379,46,59,137,64,21,43,165,135,61,24,59,147,36,72,59,98,87,34,38,64,108,86,62,319,246,86,75,67,147,32,98,65,60,12,298,114,54,61,133,35,136,173,38,248,54,51,165,504,60,215,57,109,64,114,128,243,379,60,73,139,103,280,125,74,144,176,133,22,71,45,53,185,40,23,91,453,91,41,224,69,84,185,22,174,40,234,252,247,243,316,52,114,13,245,205,305,82,44,12,62,27,213,223,143,10,7,111,139,48,244,142,76,211,85,106,31,130,39,20,98,214,238,42,8,18,207,445,492,217,153,521,158,224,79,349,85,77,127,350,72,73,77,108,393,216,246,131,171,303,476,259,256,241,403,85,153,207,258,169,209,266,205,328,235,107,71,178,205,167,275,193,117,192,55,157,187,203,208,439,211,144,157,170,266,160,150,241,160,215,212,170,135,226,159,119,178,148,115,160,206,148,132,455,202,103,197,470,121,65,138,103,475,134,218,143,526,102,356,42,407,141,82,251,194,64,303,129,77,21,59,79,55,17,34,77,42,7,55,86,85,47,141,237,38,73,42,21,117,45,69,72,131,91,38,79,64,83,33,14,102,331,51,35,7,31,9,51,29,108,35,60,8,55,15,24,78,45,95,18,7,40,54,23,59,217,85,60,22,68,16,17,167,22,63,71,21,41,42,79,23,78,153,124,16,169,25,7,63,56,14,24,52,31,58,9,42,107,27,7,54,31,11,36,54,80,28,371,396,171,234,304,178,128,167,190,266,198,125,574,149,168,216,99,126,381,145,169,113,142,124,142,174,137,216,131,237,146,124,98,121,78,108,76,74,96,113,106,59,103,93,97,83,106,113,69,75,78,78,78,207,50,62,79,178,117,154,76,152,141,46,227,52,62,54,105,393,53,232,50,63,72,147,173,305,141,120,65,83,108,79,40,87,61,142,44,71,62,115,85,123,82,336,257,422,298,149,53,135,7,101,16,21,64,26,61,174,33,7,7,25,41,7,21,188,20,36,34,13,34,18,24,97,53,38,58,7,63,54,27,16,137,21,21,34,33,14,24,41,16,48,7,20,10,65,7,35,13,53,39,29,39,15,20,61,14,167,50,7,19,14,102,20,56,9,13,27,9,23,7,9,17,17,9,56,52,21,15,24,7,84,9,16,68,19,91,9,104,31,31,7,16,10,40,9,20,33,358,478,338,431,214,281,413,322,194,311,308,362,246,402,276,393,285,391,343,463,544,162,390,262,297,173,324,477,498,425,233,361,501,559,474,264,212,326,196,292,288,225,513,346,442,332,226,338,376,311,186,280,540,200,208,458,259,236,483,252,293,407,513,211,178,280,451,170,289,329,239,253,249,235,166,395,198,333,230,576,169,345,162,164,229,107,110,480,468,328,322,200,179,184,331,227,155,213,299,500,72,338,139,173,10,19,73,183,37,101,96,21,37,208,16,169,70,100,324,70,103,95,194,362,358,25,46,184,63,480,29,27,12,41,55,72,476,73,43,413,30,89,311,204,16,19,54,107,17,365,14,100,114,42,22,158,226,297,169,378,239,116,98,111,12,44,26,322,10,17,30,902,30,150,58,74,102,431,43,214,30,362,62,119,50,540,28,281,165,198,105,402,112,225,12,193,105,276,202,71,136,199,209,193,110,415,316,101,235,326,138,208,295,149,229,125,338,314,149,512,84,154,84,112,131,263,131,79,127,166,109,93,245,351,380,165,188,240,370,238,205,134,272,386,130,166,324,212,186,189,221,194,51,141,280,234,157,82,365,277,305,27,375,485,209,594,298,54,145,125,86,157,448,145,298,54,125,59,261,189,127,592,278,517,109,64,80,36,491,192,314,146,461,111,52,82,309,264,121,220,12,61,104,27,199,82,193,110,91,26,84,65,136,209,138,54,338,208,22,82,36,109,397,55,33,150,166,22,57,112,80,86,41,64,131,263,36,84,54,229,179,109,154,26,82,19,205,8,9,111,20,54,34,28,127,125,52,26,9,14,235,52,93,131,166,21,10,9,125,51,25,130,149,101,77,22,81,62,116,79,25,149,370,141,6,38,295,12,59,16,15,72,22,189,261,125,49,6,237,221,497,364,488,239,159,201,390,142,201,342,144,119,69,156,51,241,77,69,195,91,549,183,189,96,492,98,494,305,118,41,313,158,225,57,70,394,243,406,226,264,174,224,141,248,545,139,297,153,120,92,122,134,98,88,62,76,102,231,270,317,258,463,105,278,335,116,239,172,236,452,286,464,51,174,252,102,249,73,354,207,151,69,244,299,102,209,273,190,214,122,93,47,25,57,70,281,276,197,248,246,428,177,389,377,197,176,164,205,195,331,153,257,348,227,329,243,191,198,138,223,263,278,340,228,171,198,199,269,189,187,144,270,321,213,180,205,196,257,342,231,322,150,238,177,246,193,163,176,236,218,347,189,173,166,256,252,211,261,261,205,207,174,176,272,170,235,201,129,203,173,209,233,352,186,228,290,133,70,139,114,101,135,212,124,210,278,232,283,176,101,152,142,251,126,258,44,335,167,178,119,239,158,327,6,25,91,38,69,45,41,70,33,118,10,7,57,9,98,102,28,159,41,7,156,65,142,20,51,364,57,70,8,85,497,69,59,549,69,224,41,488,77,70,119,7,174,7,18,20,96,22,201,51,144,31,18,62,6,23,47,7,94,28,7,201,7,9,70,28,36,26,35,44,16,7,26,335,48,36,9,33,203,141,61,62,33,76,21,12,35,46,26,7,7,27,6,39,87,50,28,102,6,20,7,81,7,30,53,45,75,24,47,23,95,17,39,116,342,129,113,29,98,20,120,9,32,48,42,33,29,73,11,70,139,195,6,14,102,22,35,12,21,25,134,183,17,42,88,141,21,14,93,346,7,16,76,72,95,16,89,44,170,122,9,151,45,92,153,105,87,25,260,26,57,122,39,38,7,189,23,11,7,390,10,241,99,40,354,17,6,18,7,22,36,33,396,43,79,16,453,498,619,150,207,173,100,77,107,137,89,218,174,83,173,109,78,389,307,285,185,115,117,127,141,159,29,292,72,189,234,99,62,267,284,501,324,134,137,206,297,159,294,98,49,53,542,98,60,31,298,239,32,81,45,138,102,228,159,142,136,137,99,129,69,60,198,324,298,57,311,211,179,255,54,91,47,237,210,303,150,73,186,261,53,44,34,38,136,176,181,194,205,359,310,165,450,151,311,252,44,18,80,12,498,53,48,29,7,20,34,22,89,77,9,9,115,38,72,453,137,19,218,107,99,24,12,36,36,9,9,159,207,542,98,63,53,99,9,31,60,44,12,47,109,49,7,6,77,26,22,25,619,174,13,150,10,116,300,235,78,9,173,134,7,7,25,9,13,21,46,62,8,81,159,6,9,117,24,9,91,83,173,14,21,45,10,127,6,32,100,22,17,14,16,98,7,40,17,5,307,352,318,110,549,559,88,88,291,109,89,119,134,116,66,101,52,128,173,456,138,157,92,137,80,206,238,214,528,129,535,534,67,312,81,52,234,60,57,53,401,75,404,205,100,55,60,58,337,88,324,268,34,94,41,264,74,120,103,89,35,45,38,55,357,35,88,90,56,68,66,74,62,64,72,20,459,410,44,33,94,76,219,55,131,31,108,48,45,51,75,32,182,64,47,217,78,46,94,21,15,29,15,22,12,89,71,20,21,38,7,7,56,52,7,75,50,140,25,7,30,156,109,9,17,6,30,14,9,13,46,18,21,98,35,7,98,13,7,41,9,18,7,68,35,88,15,7,18,57,7,307,6,16,7,9,42,22,100,16,116,115,26,41,15,6,20,9,34,9,20,8,39,22,9,28,9,7,35,8,9,110,7,7,9,7,20,31,19,32,7,34,7,41,21,35,28,9,13,18,340,511,507,397,442,461,509,430,345,529,500,436,390,361,318,425,364,539,201,368,335,389,222,289,398,502,438,517,333,547,308,380,338,482,262,502,270,312,283,444,432,234,315,474,314,369,251,265,181,283,325,373,283,412,317,326,278,414,250,244,523,306,344,582,571,207,387,573,224,500,241,217,209,401,391,340,164,286,269,511,312,232,204,213,162,177,188,203,105,181,173,276,220,242,160,114,336,283,384,184,92,589,45,472,111,164,18,23,177,62,234,67,211,32,19,181,122,136,181,358,110,21,368,23,40,79,283,119,539,12,68,408,41,31,52,340,220,149,189,26,128,27,489,188,68,130,48,114,80,81,81,127,125,227,46,59,17,162,177,582,130,131,519,289,541,214,364,325,75,103,17,114,114,217,412,64,39,203,30,362,19,444,48,492,511,129,98,146,248,30,437,123,41,42,196,200,81,284,143,391,497,399,570,525,355,365,389,495,562,420,469,122,183,393,266,151,243,128,157,376,530,305,358,448,204,193,166,213,156,265,129,231,106,155,209,116,182,346,138,260,125,147,97,97,406,210,43,207,158,181,88,244,74,103,109,129,92,129,124,65,102,119,70,143,224,245,236,278,209,257,212,148,198,205,137,167,125,113,135,339,46,429,64,440,158,115,391,116,366,483,74,201,207,519,105,272,528,146,80,328,157,74,55,74,46,106,60,19,35,125,39,138,50,495,339,97,147,70,43,80,129,183,122,62,36,97,201,365,43,151,128,11,266,51,153,57,92,73,43,28,8,129,420,24,74,243,49,73,40,193,38,469,28,166,119,50,30,29,19,112,11,29,16,50,562,116,103,126,389,9,182,31,156,47,158,65,204,250,18,56,38,155,121,448,358,64,20,124,19,88,129,102,12,87,109,213,34,19,45,27,491,419,383,401,434,490,269,538,227,265,245,211,171,126,178,181,354,192,172,254,143,133,343,220,249,196,147,570,185,146,178,121,143,170,165,226,198,137,89,124,178,160,93,85,444,82,75,61,90,85,77,72,131,369,54,202,199,73,86,174,146,196,50,95,106,67,343,241,160,158,244,318,187,212,178,164,100,117,139,232,149,102,143,175,175,129,84,64,379,78,527,284,123,104,392,372,362,442,343,56,369,110,43,106,408,338,99,275,41,77,57,32,41,321,94,465,43,112,117,545,109,313,70,430,354,69,273,114,298,514,111,78,338,356,319,367,481,404,275,386,443,341,296,502,294,364,402,424,323,227,538,291,339,320,305,262,455,271,367,325,313,320,341,307,256,227,318,271,258,427,304,429,396,226,250,235,306,210,366,405,205,300,551,291,217,269,330,570,57,468,377,237,210,72,421,228,312,563,441,268,57,21,85,75,44,41,147,68,73,23,181,41,63,131,19,126,490,77,43,48,57,9,30,90,77,93,245,67,211,41,124,121,89,84,37,178,57,227,9,171,43,41,72,85,202,32,56,74,54,61,6,56,82,269,56,40,52,165,29,7,52,34,63,50,146,137,21,220,10,117,7,104,69,57,102,192,150,39,185,95,34,29,20,226,80,99,36,538,23,21,66,78,172,57,65,178,160,132,265,178,27,133,254,64,42,170,110,198,249,343,124,86,81,64,196,106,146,7,70,9,28,9,135,143,29,42,45,44,11,42,143,99,39,38,92,47,106,67,60,36,52,88,354,174,109,123,111,111,19,196,94,34,33,49,98,68,86,126,68,369,78,28,72,55,87,46,175,11,343,44,82,109,139,39,68,175,28,53,130,244,110,53,74,93,444,62,59,114,216,45,86,25,40,76,36,54,81,112,343,30,457,456,380,381,327,257,509,369,243,218,173,290,196,130,153,198,219,133,185,162,511,128,129,178,118,79,357,195,468,442,87,207,151,101,273,82,431,189,100,90,91,56,90,133,60,106,88,214,64,91,164,296,145,140,189,110,36,127,88,67,103,90,75,54,95,82,72,48,63,118,340,129,255,300,133,132,144,82,54,169,90,71,124,48,50,143,82,143,72,141,505,354,62,107,115,121,36,381,386,49,22,36,51,62,29,118,44,17,26,38,164,56,83,10,56,19,49,39,25,20,34,107,257,31,130,218,42,100,101,24,23,10,48,90,106,13,28,185,21,21,21,16,88,381,21,78,32,64,90,34,6,91,20,63,82,246,29,290,60,139,196,133,102,7,62,243,73,7,24,48,327,442,153,60,61,509,6,207,30,17,21,162,110,17,151,87,12,40,12,129,39,178,82,54,91,103,95,129,83,76,523,217,469,211,300,211,178,257,264,206,140,117,214,218,218,195,130,135,196,161,194,153,341,205,96,85,134,266,166,95,120,91,172,438,193,117,94,79,100,116,122,130,84,140,111,159,131,110,64,78,67,84,63,52,66,57,72,127,96,95,95,54,104,42,64,59,118,50,131,349,111,70,38,94,101,113,106,136,36,158,39,46,126,101,103,103,98,60,132,128,84,145,202,78,80,71,114,101,71,93,7,17,61,7,16,30,16,7,7,56,111,7,178,15,23,72,16,40,24,9,25,138,35,39,16,32,60,97,41,7,73,50,29,14,10,63,39,16,23,9,20,24,53,13,30,16,16,7,41,19,9,57,28,23,16,32,91,23,41,36,22,7,33,57,44,9,22,110,7,7,7,15,68,19,31,44,64,34,29,36,7,53,56,12,46,54,13,21,28,26,7,7,18,14,21,28,33,31,36,501,410,433,435,322,131,220,298,194,238,199,187,309,537,417,173,195,409,291,283,451,298,107,129,427,260,345,107,100,151,73,91,173,156,369,400,219,237,385,259,352,295,272,258,282,200,217,281,205,267,178,410,163,372,155,494,367,377,178,419,341,453,396,547,532,349,525,530,457,360,466,45,130,434,560,380,399,114,374,471,372,513,409,454,483,507,488,429,463,340,448,474,443,392,347,399,460,506,416,511,45,173,220,107,345,131,100,117,410,45,74,15,155,271,151,53,194,199,60,59,107,129,185,404,202,469,168,403,187,173,427,435,294,298,73,128,156,137,180,51,245,527,238,91,322,50,273,65,195,124,143,360,129,46,369,267,32,165,160,64,205,56,206,141,457,309,30,112,271,471,157,136,417,107,148,60,13,120,12,387,192,245,288,281,123,715,31,282,442,65,107,272,309,85,36,54,385,239,380,230,409,299,461,190,197,378,406,344,131,336,526,351,328,366,444,527,523,308,318,330,415,379,339,493,407,517,410,220,411,313,259,265,271,312,362,485,293,357,450,282,201,404,296,154,260,436,438,280,398,274,494,355,332,543,430,537,372,529,220,483,278,319,405,422,236,524,248,200,431,404,476,341,413,445,211,344,386,533,260,542,338,523,228,411,318,431,458,230,355,485,520,195,424,290,486,470,494,239,499,288,299,182,197,163,40,190,461,180,154,72,378,45,220,131,243,277,26,196,61,126,147,404,225,372,75,147,206,350,450,240,175,126,138,241,329,111,292,266,58,57,188,10,92,140,167,314,121,208,54,156,355,72,244,151,97,66,137,69,470,493,470,328,407,147,189,145,200,48,67,470,132,499,214,73,206,371,118,121,147,111,204,75,344,177,205,35,386,55,415,69,587,77,194,209,91,58,267,59,40,46,518,167,453,545,488,350,359,512,435,478,394,516,480,531,443,308,384,313,326,338,338,456,447,290,288,310,304,391,378,270,316,481,360,283,445,360,300,485,298,257,393,331,298,377,365,270,493,249,475,280,234,457,214,209,162,381,304,549,418,346,236,204,431,474,421,264,320,263,287,329,416,188,375,260,193,275,223,350,303,265,234,223,279,245,310,414,373,300,320,268,350,487,433,262,290,324,445,391,407,220,230,192,218,219,402,277,234,316,245,184,410,354,505,234,386,339,176,309,340,444,555,287,207,224,295,202,226,196,239,229,187,296,381,221,256,209,209,129,321,267,250,170,222,476,265,164,373,250,240,202,344,165,337,238,282,202,276,129,316,306,274,325,225,316,194,273,215,400,154,220,415,251,306,412,158,387,357,142,225,222,282,544,264,577,238,135,250,169,301,263,245,198,168,254,300,228,164,193,188,240,516,167,518,151,140,176,51,67,99,31,393,47,129,24,88,68,88,9,66,245,7,229,154,31,227,336,352,31,154,10,124,32,275,132,272,354,90,169,40,23,8,38,80,203,60,295,64,110,91,136,96,200,512,142,179,62,38,475,98,44,81,120,11,53,21,16,17,474,96,7,59,57,189,57,89,52,55,487,400,41,135,283,126,28,7,300,7,17,30,134,359,140,53,43,120,443,57,252,22,21,156,216,72,176,344,148,12,45,78,43,82,96,116,123,7,216,146,13,108,218,120,57,14,205,198,53,73,219,42,46,12,513,199,164,143,9,95,32,96,160,77,9,135,387,260,241,99,39,303,52,273,177,134,50,90,109,169,18,96,95,7,325,112,391,72,64,20,20,20,211,44,124,256,77,83,14,238,111,196,35,144,55,433,20,50,27,20,58,64,47,226,74,276,9,66,88,41,90,124,55,332,383,358,407,309,328,504,339,488,289,355,355,559,338,251,259,420,294,271,327,355,537,322,233,563,332,408,316,522,298,250,448,518,541,184,248,301,342,161,306,183,349,244,203,449,425,195,252,438,259,183,286,209,282,534,344,215,289,189,355,277,267,337,377,363,277,336,228,206,223,273,303,294,223,384,195,194,464,264,353,172,263,197,232,360,247,240,300,214,134,228,272,284,232,157,186,168,190,291,265,19,128,96,70,44,138,52,40,147,243,6,220,360,116,203,96,97,468,20,48,64,68,110,363,69,41,28,35,145,16,141,201,172,51,223,121,34,21,152,333,58,522,16,31,57,237,80,136,67,306,14,30,7,169,76,7,117,208,82,7,68,50,173,99,52,7,107,7,7,70,71,282,129,29,104,35,7,7,45,123,16,208,116,24,7,141,117,58,23,15,30,140,30,7,262,108,12,360,20,16,518,442,480,395,347,549,397,526,294,306,442,527,357,473,349,422,306,268,218,308,322,478,333,263,440,208,461,226,195,577,518,284,214,219,538,181,517,233,340,548,304,262,253,323,249,201,182,237,252,217,473,282,520,194,556,322,362,218,302,359,268,163,57,474,199,173,156,236,192,392,241,215,165,271,179,258,263,318,323,183,271,148,165,216,290,225,362,245,162,203,195,218,158,406,253,136,196,183,470,174,10,17,57,45,9,201,7,39,7,22,181,35,28,60,25,88,14,7,9,9,28,55,59,7,17,7,49,7,7,24,7,27,38,7,43,20,95,79,102,7,50,31,15,29,142,18,268,19,73,31,16,108,11,16,9,49,16,118,51,64,132,112,40,6,16,58,13,34,208,27,19,47,34,9,7,77,122,90,7,7,6,53,10,16,48,179,19,7,9,105,149,36,21,122,174,7,86,14,34,88,509,505,370,450,480,428,357,447,501,376,467,500,384,350,461,514,430,449,403,326,452,346,560,516,477,425,503,507,556,445,462,450,358,475,339,431,308,502,375,451,426,520,440,486,411,459,529,380,439,362,523,539,466,540,458,386,473,438,398,371,389,564,514,285,458,493,457,360,397,483,464,329,504,480,539,457,444,335,538,310,336,338,515,381,297,499,291,284,317,368,530,295,556,491,536,424,199,284,231,240,84,524,22,111,97,37,444,39,297,221,30,499,21,19,514,201,56,9,25,41,458,52,219,146,90,381,69,406,457,331,97,71,251,239,491,107,50,109,98,45,55,502,66,285,420,90,22,31,66,112,77,169,55,37,126,8,483,212,136,62,111,185,493,78,103,406,131,102,216,53,128,47,138,92,93,530,55,126,78,186,298,50,538,120,39,165,317,55,531,75,205,295,217,416,154,501,24,33,77,160,521,468,391,367,377,515,458,378,441,469,389,422,367,459,370,326,504,404,392,427,521,366,361,392,276,418,324,233,305,248,309,514,138,134,349,181,82,442,524,103,228,100,168,333,77,510,442,354,496,369,511,383,363,375,363,475,374,499,436,507,423,539,514,197,253,344,508,496,389,404,499,458,479,494,546,460,523,299,518,384,550,467,184,435,474,383,367,369,484,471,516,258,496,326,394,400,369,384,531,298,392,33,44,82,8,103,276,134,135,138,418,305,37,127,77,18,102,248,47,67,206,285,41,181,324,38,228,233,100,119,44,132,138,309,184,105,46,197,461,382,499,32,155,349,41,168,47,84,35,123,163,514,524,333,120,312,74,160,130,35,511,79,150,85,202,153,75,16,92,118,85,197,170,13,48,379,62,273,81,417,41,72,159,67,235,139,44,262,390,31,321,68,204,247,186,199,327,40,144,434,376,436,526,535,458,386,462,405,499,321,361,341,424,435,477,424,319,495,381,193,228,166,397,352,309,335,308,323,500,134,111,98,254,465,389,163,89,527,516,554,314,450,471,495,353,467,412,140,420,393,485,463,310,356,553,380,357,321,519,347,292,353,248,478,404,518,522,502,527,371,385,492,539,283,521,391,361,433,332,485,490,518,567,370,388,544,499,271,464,331,390,517,341,386,392,479,357,515,429,474,287,549,429,528,397,273,425,400,375,488,365,279,459,410,569,541,491,401,427,411,387,507,501,307,428,399,498,428,465,358,445,511,491,410,446,397,462,346,382,450,478,343,328,543,445,316,419,307,484,456,266,288,380,291,351,298,304,395,348,210,300,257,477,282,297,373,264,504,393,295,426,365,286,365,314,324,224,286,373,498,327,475,238,234,438,444,533,499,362,551,473,319,276,246,307,509,408,283,377,443,13,67,37,166,42,98,381,30,224,97,76,41,49,98,228,7,193,75,53,495,86,228,51,28,321,319,111,24,89,192,134,54,53,465,37,7,309,60,40,106,50,335,45,352,73,500,156,397,140,64,51,235,18,43,7,91,361,308,7,41,22,254,163,308,94,323,49,62,289,93,74,165,30,95,35,23,124,95,148,202,167,50,341,58,194,79,149,111,192,160,153,57,34,16,56,82,203,153,109,17,148,18,58,6,59,82,6,76,161,228,101,146,266,154,94,8,144,126,186,75,554,103,60,80,365,516,185,287,72,79,101,8,8,89,155,65,211,7,118,195,134,90,166,81,42,132,261,517,9,48,104,71,60,48,50,87,99,53,19,518,195,9,195,132,62,55,73,213,116,554,212,30,76,84,44,92,13,166,327,79,102,157,74,106,116,128,127,59,104,169,58,90,229,39,204,55,71,154,207,78,444,431,359,420,537,389,447,523,556,503,378,394,342,500,434,140,185,194,143,444,260,361,166,385,305,222,220,101,153,126,147,100,132,75,71,86,242,174,140,66,147,198,67,98,426,159,56,347,508,392,528,434,385,322,470,65,546,543,289,449,292,566,377,454,363,983,378,388,300,497,483,534,498,356,516,513,384,380,278,163,324,474,112,398,335,509,286,490,389,360,345,336,365,265,240,312,317,484,496,451,60,98,16,9,140,500,75,147,185,16,45,147,101,71,60,26,143,100,260,8,194,33,20,9,16,434,166,153,222,82,49,40,21,10,35,488,188,65,7,16,7,67,17,30,132,6,7,38,61,166,444,43,66,126,33,7,86,56,60,219,34,28,60,26,54,49,64,7,381,37,16,63,14,59,61,30,72,140,39,177,37,19,41,159,51,24,41,69,61,7,124,13,37,385,47,198,163,49,162,120,396,343,450,507,302,197,92,399,132,77,369,217,128,124,77,125,104,108,119,102,95,54,49,71,124,92,73,60,120,46,43,112,195,231,167,105,32,195,87,53,32,155,140,390,182,367,51,527,22,544,148,380,140,598,394,156,560,420,491,36,37,518,480,416,503,416,99,66,553,116,398,362,364,477,463,423,346,483,351,543,458,400,314,378,349,374,444,315,428,540,312,474,473,385,514,499,472,427,405,509,41,13,15,132,18,7,77,92,13,46,23,29,15,135,11,14,7,26,9,8,8,13,16,13,14,45,20,105,99,77,7,56,16,45,13,66,197,9,32,35,48,9,106,112,36,31,16,62,14,13,102,26,22,43,7,54,42,31,128,35,34,7,41,77,95,399,16,65,140,28,53,7,20,40,58,13,48,49,31,13,31,22,13,132,18,33,15,7,17,16,7,9,22,25,45,124,14,21,13,9,502,398,500,392,463,340,535,504,559,522,443,439,375,525,794,479,517,542,463,522,433,387,383,335,462,531,365,310,338,345,412,295,426,333,531,500,393,524,317,302,335,457,486,410,507,506,360,227,312,491,248,370,523,463,287,457,422,494,280,281,488,445,341,519,444,433,327,293,324,320,476,275,474,474,301,348,160,520,544,225,414,438,231,323,509,513,344,527,340,574,306,293,317,239,508,509,368,511,228,188,330,62,95,227,335,57,324,199,76,160,11,462,110,124,225,268,19,295,266,124,22,407,52,243,90,103,489,345,170,436,184,77,214,149,166,59,31,311,19,383,476,33,280,166,310,426,511,188,239,112,69,94,80,109,180,217,293,540,449,112,144,185,166,228,33,19,43,128,25,79,137,42,173,30,217,159,126,134,279,473,193,167,41,231,28,242,130,387,67,141,270,40,11,560,50,158,15,279,88,264,485,403,479,444,410,512,506,388,360,370,525,373,357,494,410,494,362,336,362,318,481,490,458,333,285,297,239,574,167,141,212,355,512,118,395,243,256,170,178,126,278,541,410,155,531,510,460,534,314,301,341,427,496,214,180,139,209,214,156,54,492,251,214,176,226,463,414,88,501,430,405,336,292,331,533,443,459,451,528,502,314,285,518,566,457,317,413,582,340,491,289,122,315,246,496,332,239,337,431,528,12,20,54,158,141,161,126,362,239,336,333,301,243,118,492,214,256,17,209,156,170,167,297,463,203,39,315,315,57,24,574,26,318,122,285,285,212,60,119,458,490,481,178,141,155,72,355,180,57,134,139,278,214,226,93,85,21,541,48,90,510,84,132,163,314,206,195,239,176,88,45,534,301,50,30,531,86,246,341,95,200,512,184,77,496,51,251,35,73,214,166,100,243,410,52,149,32,177,460,21,539,406,447,338,353,403,451,413,404,227,238,232,215,506,495,510,412,366,145,557,234,224,107,252,432,141,313,208,159,447,126,55,555,82,131,217,86,132,247,319,179,378,188,213,86,124,185,374,517,338,514,533,512,526,438,510,382,317,358,540,378,514,360,465,412,122,311,491,397,456,343,543,391,533,436,349,387,24,53,524,133,276,442,497,506,457,503,445,533,349,461,499,349,371,346,401,304,444,322,451,393,358,454,299,252,333,274,327,389,337,282,433,425,475,549,439,326,349,387,398,318,241,463,536,483,334,463,323,276,341,511,363,555,564,527,418,241,327,329,409,298,384,355,306,315,276,416,270,301,370,359,556,260,421,305,288,335,268,257,412,593,456,390,490,501,298,269,308,256,291,437,425,493,376,396,511,318,465,307,507,516,296,283,303,316,321,357,286,450,309,242,461,296,272,217,521,311,328,286,387,250,55,232,227,126,510,135,159,24,82,53,17,177,122,81,145,234,131,132,63,56,11,224,49,76,404,217,107,14,65,108,53,57,187,86,413,208,506,17,59,107,215,238,23,432,86,137,141,43,27,366,412,86,54,689,22,94,188,134,366,29,124,109,495,12,555,382,179,133,23,118,247,52,196,555,16,122,41,17,18,128,77,57,132,105,67,120,313,447,319,76,106,213,135,252,378,51,105,71,59,185,311,40,23,92,557,211,111,58,143,56,69,175,92,22,272,255,196,164,128,369,152,138,49,167,30,195,50,118,76,107,176,92,103,118,39,236,284,145,123,118,90,178,214,52,107,51,152,142,24,91,144,82,52,140,38,48,168,200,229,28,71,89,64,87,269,66,366,281,197,431,49,374,164,143,162,44,74,56,239,225,131,139,204,238,115,29,13,44,7,314,271,109,93,78,232,168,178,141,23,369,499,416,234,372,449,464,197,305,201,559,179,297,231,171,183,148,198,186,180,229,106,218,135,186,169,159,220,199,161,223,83,72,92,110,124,106,56,156,200,142,144,184,118,116,175,82,497,450,466,310,507,515,505,521,305,172,156,502,413,381,481,356,389,350,57,561,541,524,522,428,432,254,364,523,576,490,503,534,284,141,317,519,377,368,506,269,342,396,365,506,535,277,390,341,332,556,316,540,329,175,559,143,20,149,305,106,20,416,18,108,234,66,56,73,183,372,92,122,110,83,89,62,98,99,171,72,449,20,197,21,54,67,159,201,79,26,297,32,198,139,148,82,57,464,7,78,179,58,24,135,223,20,41,89,68,186,87,33,7,44,30,52,7,26,106,186,7,124,180,231,9,7,68,161,138,58,33,52,22,66,7,84,184,62,63,142,101,172,44,128,58,254,171,199,60,118,220,88,46,505,281,314,345,270,283,253,286,371,349,147,114,138,311,184,136,160,111,141,209,164,187,117,106,300,217,262,52,133,143,124,249,263,95,158,251,287,209,154,109,111,75,72,91,88,113,78,102,125,71,68,123,86,112,100,123,55,91,173,111,183,359,452,158,56,112,75,117,116,62,152,68,77,93,399,46,552,438,305,60,38,103,367,135,363,341,34,66,33,58,35,114,450,51,443,311,394,391,432,365,147,18,25,58,443,9,33,9,16,15,28,16,184,15,16,7,51,7,314,35,22,20,82,38,66,17,114,40,38,114,41,281,21,136,17,15,8,70,253,75,225,16,16,79,38,35,31,28,16,7,33,111,8,30,23,32,19,58,75,117,164,14,52,9,22,14,136,62,61,72,48,177,18,47,30,117,16,58,17,7,62,92,26,187,6,14,48,41,23,8,29,9,30,138,23,9,262,34,9,23,526,568,345,431,336,494,242,315,531,298,305,288,305,257,335,508,418,402,443,440,370,456,409,178,324,250,444,342,178,223,198,195,216,214,210,208,177,239,189,237,235,444,339,174,233,176,248,278,221,541,162,183,536,350,311,274,260,312,215,479,167,181,312,204,175,216,200,205,263,193,224,227,243,163,155,201,293,247,310,287,425,541,366,504,540,390,173,165,276,117,277,154,165,153,237,164,124,355,202,142,27,9,111,24,208,16,100,187,160,70,57,26,319,86,230,126,184,85,12,69,94,90,42,77,8,138,59,75,54,59,365,177,75,131,37,69,114,339,44,17,308,84,363,176,58,124,119,257,242,69,19,271,526,144,49,149,241,119,237,75,97,202,276,191,7,15,112,298,74,223,136,117,125,21,198,125,162,113,196,135,178,124,226,75,149,236,295,200,216,108,210,24,32,56,48,21,444,105,317,133,418,472,494,316,323,246,437,307,185,390,226,490,265,340,206,183,216,203,169,258,141,399,318,297,164,98,328,105,383,119,154,471,148,157,158,172,97,77,91,76,90,165,80,145,265,185,141,202,268,190,219,184,70,189,176,93,99,195,124,476,359,505,316,505,532,137,417,552,491,494,550,380,535,493,319,486,443,267,429,267,328,277,270,480,269,147,139,281,267,65,72,345,540,70,509,164,115,138,67,362,31,56,164,185,30,37,6,44,55,418,60,93,28,42,65,28,89,66,105,34,233,390,316,142,36,60,70,72,172,183,97,235,246,99,63,213,226,90,76,87,494,36,44,85,47,165,77,9,11,23,14,67,437,80,323,70,145,472,72,45,141,107,198,82,83,119,91,107,154,74,68,98,307,410,114,41,164,19,96,157,39,176,169,46,471,125,19,45,12,124,139,39,34,64,206,148,42,158,14,38,370,448,159,162,197,178,526,535,341,242,198,268,172,162,101,226,162,131,136,79,94,251,163,126,194,260,196,133,174,101,162,173,127,182,212,144,132,113,116,310,141,492,40,486,45,272,303,325,472,243,171,429,226,272,102,307,398,68,204,92,278,122,44,253,138,206,9,444,48,319,112,347,42,196,39,37,240,508,193,72,46,222,77,67,195,212,194,190,36,81,306,271,169,282,511,404,290,463,203,62,226,227,163,183,576,269,262,168,157,182,31,51,226,177,271,158,219,297,362,255,320,183,182,224,237,267,207,249,233,149,246,220,304,273,311,601,128,191,304,155,224,212,220,153,236,232,147,222,115,287,123,181,334,354,517,132,226,129,419,188,146,337,271,235,191,238,176,236,247,160,172,317,161,203,179,92,518,219,200,261,203,197,137,122,164,182,117,275,228,273,128,43,169,218,133,157,154,209,23,222,72,92,162,159,43,136,45,9,94,79,9,17,57,35,44,101,73,163,74,14,174,62,172,9,162,7,212,37,39,242,178,198,18,12,196,370,81,51,23,126,182,36,197,18,42,11,7,113,101,195,39,26,40,29,7,75,60,131,47,226,102,268,162,14,127,448,68,7,12,11,123,10,404,48,310,122,492,49,162,132,28,171,138,341,28,45,112,98,77,101,92,161,58,43,35,72,193,141,38,29,24,89,58,46,29,173,163,67,31,56,251,44,67,18,34,56,119,12,17,23,29,7,6,526,133,260,48,78,27,59,535,47,26,50,37,18,196,43,194,18,6,18,15,144,7,34,116,21,26,183,212,271,25,72,133,9,42,14,40,71,20,16,9,16,168,181,157,20,21,7,56,72,9,46,125,235,117,60,38,38,147,132,444,16,15,25,36,63,145,10,68,45,122,5,17,52,42,61,87,93,461,494,206,326,216,275,297,236,100,179,213,196,215,220,78,153,501,171,65,73,107,254,72,166,117,383,241,348,265,64,73,211,90,293,69,414,256,52,88,411,87,91,84,83,526,199,111,74,425,457,170,92,571,187,301,46,213,233,501,205,164,229,214,264,243,245,207,199,133,474,183,219,172,307,184,244,212,141,154,226,146,145,168,223,242,135,143,179,112,182,473,257,175,193,269,165,202,180,199,60,90,89,7,19,53,72,46,27,206,34,216,461,29,213,179,20,153,69,9,494,100,12,9,7,9,78,38,107,7,26,60,9,7,16,166,275,73,23,15,13,65,9,10,32,326,12,7,23,18,12,9,21,52,16,199,28,51,7,15,7,103,56,28,28,64,220,7,6,73,30,84,69,27,9,6,32,501,40,69,91,83,319,215,236,61,51,39,58,16,37,54,62,61,49,92,7,38,10,5,442,474,302,214,214,196,104,124,333,147,163,88,81,146,40,86,61,134,180,62,70,65,74,170,98,430,284,213,110,197,215,62,89,102,70,37,117,394,60,175,111,87,73,46,436,84,108,85,45,119,45,66,145,42,641,184,66,92,318,62,69,35,34,30,22,36,27,47,26,53,73,48,76,79,21,146,381,31,33,107,238,25,100,59,56,41,280,86,114,137,87,196,55,137,70,48,97,111,91,367,16,85,12,18,27,16,127,73,9,47,24,7,22,46,20,7,10,7,33,21,101,7,9,84,7,9,8,7,9,7,38,159,7,34,21,18,38,16,19,41,16,9,146,20,9,32,7,10,9,15,26,9,30,68,13,33,19,21,184,17,7,6,9,8,13,39,34,7,7,9,70,9,9,40,9,29,13,22,9,23,15,9,13,48,81,7,7,7,7,7,7,41,43,43,88,29,9,111,16,306,539,553,291,270,363,524,475,466,261,264,318,262,315,193,308,304,250,295,210,181,328,536,245,197,281,405,301,272,302,206,263,245,187,148,211,256,128,305,181,378,177,155,317,388,307,317,217,233,155,450,598,273,299,406,301,138,164,207,436,149,514,197,160,157,159,249,232,129,296,437,209,373,126,245,322,212,397,94,148,295,346,372,296,236,144,165,141,101,191,113,85,148,191,140,79,169,302,134,104,217,97,76,172,295,232,141,60,62,114,28,25,101,53,315,176,191,164,51,524,25,49,94,116,129,264,17,59,233,165,115,79,65,40,129,69,120,85,31,536,328,193,181,47,58,69,21,197,91,139,126,346,64,245,207,138,77,22,159,27,29,236,107,280,79,63,19,31,25,70,22,433,78,38,55,25,169,317,88,148,148,41,29,43,113,11,22,31,261,155,126,23,45,47,56,27,36,24,143,38,431,457,211,381,210,210,333,448,316,263,163,145,574,505,246,139,105,141,299,181,182,278,169,143,97,126,129,66,156,200,138,93,232,237,162,107,168,155,92,175,358,137,455,157,379,417,81,505,477,368,499,389,299,464,289,475,507,481,334,313,344,517,526,469,385,468,382,487,461,270,360,130,297,365,545,480,364,512,239,440,400,196,280,151,102,92,115,369,127,109,391,317,141,66,83,71,311,537,197,466,92,102,28,30,92,43,143,64,139,311,105,109,37,461,71,138,187,83,56,102,163,103,141,71,263,98,210,156,457,17,30,164,52,129,86,51,145,211,81,29,129,58,66,107,51,111,93,126,200,210,169,431,66,333,97,108,574,181,381,26,151,76,141,358,21,151,232,137,38,155,148,51,12,311,168,246,39,239,316,147,100,74,278,127,182,133,115,98,44,117,115,67,42,237,86,53,91,16,193,115,516,289,353,298,201,255,204,117,200,343,184,241,125,189,159,111,118,231,98,123,53,59,139,129,215,183,312,541,98,336,120,50,211,296,124,142,268,2115,365,500,142,182,447,123,455,422,473,91,254,99,162,473,481,565,372,507,549,360,501,314,332,524,449,199,258,342,256,282,246,356,469,388,354,269,74,483,280,200,382,372,203,162,85,394,116,207,530,324,273,487,339,312,394,214,317,386,443,405,278,462,115,513,132,50,107,107,55,64,217,329,410,257,267,350,126,227,96,98,497,419,487,187,230,256,264,462,252,343,329,284,110,193,264,289,379,301,276,284,215,122,253,251,214,336,352,310,198,136,167,223,162,323,202,319,99,275,230,303,176,160,247,357,170,220,75,224,274,240,118,311,351,366,248,265,219,174,236,236,475,293,250,220,268,255,292,418,339,243,202,194,185,199,276,135,355,158,278,211,194,306,125,117,200,353,99,343,184,118,98,99,241,41,101,51,53,201,59,129,91,54,107,55,159,264,55,123,63,50,204,9,231,123,64,119,111,21,289,246,107,50,98,516,95,189,336,132,115,255,139,85,42,121,298,162,153,541,36,17,312,74,48,183,7,116,142,196,15,7,96,356,42,124,29,142,42,93,74,110,215,78,122,75,115,126,38,117,120,296,88,64,51,136,182,75,61,211,98,36,83,203,227,168,162,60,36,268,52,36,94,162,144,178,23,206,38,487,79,223,37,94,116,95,152,13,26,86,56,117,118,113,54,98,112,121,329,173,286,160,129,97,81,134,24,59,96,121,77,29,167,117,394,30,259,14,184,447,207,53,30,127,145,204,93,158,73,25,44,118,225,122,92,21,13,97,90,230,123,27,145,32,95,118,124,150,39,198,241,68,257,185,7,59,360,7,116,133,175,16,29,36,416,350,557,430,507,287,258,225,184,168,133,120,307,268,166,106,123,107,109,107,300,280,104,44,127,96,46,99,90,97,94,121,211,337,266,223,69,414,253,548,97,244,283,66,354,324,443,455,266,512,332,513,532,409,352,317,363,271,491,340,409,325,296,210,272,483,328,330,282,302,386,428,370,253,298,449,232,515,245,449,356,304,466,391,421,126,362,217,477,174,435,270,319,341,108,181,218,249,97,248,46,7,65,29,44,32,104,225,123,66,46,557,127,22,268,244,430,96,65,97,166,14,109,6,97,16,31,106,150,78,121,507,79,107,7,19,94,40,16,168,69,73,258,99,90,67,184,120,44,21,143,42,107,67,38,74,17,287,31,60,59,16,307,184,88,133,168,34,97,18,337,173,42,498,12,80,37,56,21,39,59,62,83,47,41,58,12,86,99,138,19,9,38,7,47,62,90,280,61,7,520,373,483,290,391,262,260,204,171,197,168,642,133,263,130,392,144,135,180,177,167,517,267,144,147,104,87,550,108,171,181,259,136,97,109,154,104,102,90,110,314,86,297,385,108,76,68,57,62,175,91,115,76,79,84,57,89,66,83,178,104,93,70,74,95,83,139,94,62,77,102,84,99,61,52,64,81,49,64,97,50,69,206,182,301,200,202,160,170,126,107,67,42,37,52,49,76,75,133,65,43,88,111,26,16,16,68,23,43,16,61,7,9,7,24,49,22,31,11,49,38,15,16,104,42,22,13,20,9,9,9,7,34,47,14,41,31,15,15,52,16,27,7,16,16,260,16,18,13,7,28,57,9,32,27,20,17,13,45,27,33,16,7,23,27,34,16,196,53,58,17,16,24,13,26,62,16,16,30,147,8,191,45,31,57,22,16,385,16,29,97,16,16,13,7,13,23,108,14,7],"xaxis":"x","yaxis":"y","type":"histogram"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#f2f5fa"},"error_y":{"color":"#f2f5fa"},"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"baxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"line":{"color":"#283442"}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"marker":{"line":{"color":"#283442"}},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#506784"},"line":{"color":"rgb(17,17,17)"}},"header":{"fill":{"color":"#2a3f5f"},"line":{"color":"rgb(17,17,17)"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#f2f5fa","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#f2f5fa"},"geo":{"bgcolor":"rgb(17,17,17)","lakecolor":"rgb(17,17,17)","landcolor":"rgb(17,17,17)","showlakes":true,"showland":true,"subunitcolor":"#506784"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"dark"},"paper_bgcolor":"rgb(17,17,17)","plot_bgcolor":"rgb(17,17,17)","polar":{"angularaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","radialaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"yaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"zaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"}},"shapedefaults":{"line":{"color":"#f2f5fa"}},"sliderdefaults":{"bgcolor":"#C8D4E3","bordercolor":"rgb(17,17,17)","borderwidth":1,"tickwidth":0},"ternary":{"aaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"baxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","caxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"title":{"x":0.05},"updatemenudefaults":{"bgcolor":"#506784","borderwidth":0},"xaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"value"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"count"}},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"margin":{"t":60},"barmode":"relative","title":{"text":"Distribution of token amount"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('ef6ec8ae-66ff-43b2-9bf6-d0f06fb74d94');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>


</div>
</div>
<div class="cell" data-execution_count="210">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'sentimento'</span>] <span class="op">=</span> df.sentimento.<span class="bu">map</span>(<span class="bu">dict</span>({</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'negative'</span>: <span class="dv">0</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'neutral'</span>: <span class="dv">1</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'positive'</span>: <span class="dv">2</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>}))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="getting-the-bert-tokens-for-the-whole-dataset" class="level1">
<h1>Getting the BERT tokens for the whole dataset</h1>
<div class="cell" data-outputid="570d5cf6-bd32-453a-e03b-f2a71e8022d3" data-execution_count="211">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">'neuralmind/bert-base-portuguese-cased'</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>tt <span class="op">=</span> <span class="st">'OlÃ¡, esse Ã© um teste! &lt;3'</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>test_tokens <span class="op">=</span> tokenizer.encode_plus(</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    tt,</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    max_length<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    add_special_tokens<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    return_attention_mask<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    return_token_type_ids<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    padding<span class="op">=</span><span class="st">'max_length'</span>,</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    return_tensors<span class="op">=</span><span class="st">'pt'</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'INPUT IDS'</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_tokens[<span class="st">'input_ids'</span>])</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">ATTENTION MASK'</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_tokens[<span class="st">'attention_mask'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>INPUT IDS
tensor([[  101,  1651, 22303,   117,  1966,   253,   222,  3515,   106,   133,
           511,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0]])

ATTENTION MASK
tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="212">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_tokens(text):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  encodes <span class="op">=</span> tokenizer.encode_plus(</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>      text,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>      max_length<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>      add_special_tokens<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>      return_attention_mask<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>      return_token_type_ids<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>      truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>      padding<span class="op">=</span><span class="st">'max_length'</span>,</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>      return_tensors<span class="op">=</span><span class="st">'pt'</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="bu">dict</span>({</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>      <span class="st">'input_ids'</span>: encodes[<span class="st">'input_ids'</span>].squeeze(),</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>      <span class="st">'attention_mask'</span>: encodes[<span class="st">'attention_mask'</span>].squeeze()</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'input_ids'</span>], df[<span class="st">'attention_mask'</span>] <span class="op">=</span> df.texto_refinado.<span class="bu">apply</span>(<span class="kw">lambda</span> x: get_tokens(x)[<span class="st">'input_ids'</span>]), df.texto_refinado.<span class="bu">apply</span>(<span class="kw">lambda</span> y: get_tokens(y)[<span class="st">'attention_mask'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="e4a5bd0e-0df3-41d2-bb1b-f8403c6e2775" data-execution_count="213">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>sizes <span class="op">=</span> <span class="bu">list</span>([])</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> df.input_ids:</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  sizes.append(i.size()[<span class="dv">0</span>])</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.array(sizes).<span class="bu">max</span>(), np.array(sizes).<span class="bu">min</span>(), np.array(sizes).mean())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>512 512 512.0</code></pre>
</div>
</div>
<div class="cell" data-outputid="5b4b5e94-cc57-45bd-e13e-e832c1d9cea2" data-execution_count="214">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>test_tokens[<span class="st">'input_ids'</span>].squeeze().size()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="214">
<pre><code>torch.Size([32])</code></pre>
</div>
</div>
</section>
<section id="creating-the-pytorch-dataset-class" class="level1">
<h1>Creating the Pytorch dataset class</h1>
<div class="cell" data-execution_count="215">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomDF(Dataset):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, text, targets, tokenizer, max_len):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.text <span class="op">=</span> text</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.targets <span class="op">=</span> targets</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.max_len <span class="op">=</span> max_len</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.text)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, item):</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> <span class="bu">str</span>(<span class="va">self</span>.text[item])</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    target <span class="op">=</span> <span class="va">self</span>.targets[item]</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    encoding <span class="op">=</span> <span class="va">self</span>.tokenizer.encode_plus(</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        text,</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        add_special_tokens<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span><span class="va">self</span>.max_len,</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        return_token_type_ids<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">'max_length'</span>,</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        return_attention_mask<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        return_tensors<span class="op">=</span><span class="st">'pt'</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">dict</span>({</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        <span class="st">'text'</span>: text,</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        <span class="st">'input_ids'</span>: encoding[<span class="st">'input_ids'</span>].flatten(),</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>        <span class="st">'attention_mask'</span>: encoding[<span class="st">'attention_mask'</span>].flatten(),</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>        <span class="st">'targets'</span>: torch.tensor(target, dtype<span class="op">=</span>torch.int64)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>    })</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="splitting-the-data" class="level1">
<h1>Splitting the data</h1>
<div class="cell" data-outputid="9a103f28-ec4c-4aa9-8952-550b6419c444" data-execution_count="216">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>SEED <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>train_df, test_df <span class="op">=</span> train_test_split(df, test_size<span class="op">=</span><span class="fl">.2</span>, random_state<span class="op">=</span>SEED)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>val_df, test_df <span class="op">=</span> train_test_split(test_df, test_size<span class="op">=</span><span class="fl">.5</span>, random_state<span class="op">=</span>SEED)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Amount of entries for train: </span><span class="sc">{</span>train_df<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Amount of entries for test: </span><span class="sc">{</span>test_df<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Amount of entries for validation: </span><span class="sc">{</span>val_df<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Amount of entries for train: 8636
Amount of entries for test: 1080
Amount of entries for validation: 1080</code></pre>
</div>
</div>
</section>
<section id="creating-a-function-to-create-dataloaders" class="level1">
<h1>Creating a function to create dataloaders</h1>
<div class="cell" data-execution_count="217">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_data_loader(df, tokenizer, max_len, batch_size):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  dataset <span class="op">=</span> CustomDF(</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>      text<span class="op">=</span>df.texto_refinado.to_numpy(),</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>      targets<span class="op">=</span>df.sentimento.to_numpy(dtype<span class="op">=</span><span class="bu">int</span>),</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>      tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>      max_len<span class="op">=</span>max_len</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> DataLoader(</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>      dataset<span class="op">=</span>dataset,</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>      batch_size<span class="op">=</span>batch_size,</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>      num_workers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>      drop_last<span class="op">=</span><span class="va">True</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>MAX_LEN <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>train_data_loader <span class="op">=</span> create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>test_data_loader <span class="op">=</span> create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>val_data_loader <span class="op">=</span> create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="14111521-2c11-4d85-a07e-10f283876b7a" data-execution_count="218">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_data_loader))</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>train_data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="218">
<pre><code>{'text': ['a nova atualizaÃ§Ã£o pile_of_poo ficou horrÃ­vel navegaÃ§Ã£o app pÃ©ssima nÃ£o contar empresas pÃµem valor mÃ­nimo compra poder assim fazer venda casada obrigando cliente comprar precisa faturar valores entrega muitas vezes alto prÃ³prio pedido onde viu paga 30 00 entrega ??? loucura',
  'pÃ©ssimo todos sentidos gostaria conhecer gÃªnio inventou fechar refrigetante papel poi entrega nao entorne aÃ­ pede pedido feito restaurante 5km distÃ¢ncia tendo lado ). resumindo sempre chega frio molhado jÃ¡ app permite cancelar pedido atendimento demora 3h poderia escrever ma limite caracteres permite',
  'a navegaÃ§Ã£o site lenta aplicativo miui traz experiÃªncia conflitante muito demorado carrega fazer qualquer coisa fazer primeiro pedido beira frustraÃ§Ã£o difÃ­cil novo usuÃ¡rio entender acontecendo taxas cadastrar coisas bÃ¡sicas endereÃ§o nÃºmero telefone tanta lentidÃ£o dÃºvida chega questionar pedido realmente feito fora estÃ©tica excelente layout sÃ³ comprei desconto',
  'pop',
  'app legal porÃ©m muitos restaurantes gostaria pedir dizem estarem disponÃ­veis prÃ³pria cidade alguns pedidos limite forma pagamento cash cartÃ£o ), quero pagar achar melhor e lugares vendem coisa entregam outra ma culpam uber erro e preÃ§os alto promoÃ§Ãµes ilusÃ³rias app legal sim quando problema',
  'infelizmente tido problemas todos pedidos recentes fiz jÃ¡ dei nova chance restaurantes pedindo outros apps pedidos vieram direitinho nÃ£o sei forma passado pedido restaurante confusa porÃ©m evitado selecionar itens adicionais promoÃ§Ãµes especÃ­ficas combo diferentes quanto valor entregue errado sempre ressarcida demora fica dor cabeÃ§a ter adaptar cardÃ¡pio pedido recebi',
  'aplicativo bugado',
  'bom ma falta un cupons',
  'foi bom tempo mudanÃ§as usei',
  'falta melhorar opÃ§Ãµes compartilhar pedido opÃ§Ãµes selecionar pedidos anteriores ter ir restaurante escolhar tudo denovo aplicativo falha muita vezes dando erro aplicativo informaÃ§Ã£o entrega 3 estrelas porque ... fim contas comida chega come beleza ma perguntar bom ... pra bom falta ainda entrega mÃ­nimo mÃ­nimo bem fraco',
  'nÃ£o nd dizer pessoas nn sabe usar app ma fÃ¡cil',
  'o app deixa pagar hora entrega ....',
  'eu gosto app shopping ventura deram batata salgada socorro puro sal ksksk voltei loja fechados sei reclamar',
  'layout poderia prÃ¡tico mudar endereÃ§o trocar opÃ§Ãµes zera carrinho nÃ£o sei time usuÃ¡rio olhou',
  'o app bom mas dei 4 estrelas poderem sempre melhorar',
  'o app simplesmente horrÃ­vel consigo usar direito jÃ¡ faz tempo aparece opÃ§Ãµes restaurantes prÃ³ximos pÃ¡gina inicial preciso ir busca entrar cada categoria ver agora digito nome restaurante aparece contar cupons comprei aparecendo todos restaurantes usar',
  'excelente',
  'o app bom ma vou pegar cupons sempre aparece atingiu limite cupons ma atingi sugiro arrume bug',
  'excelente app tudo fÃ¡cil rapido',
  'antigamente otimo app agora quer entra app ruim',
  'nÃ£o achei opÃ§Ã£o contato restaurante precisei modificar pedido apÃ³s fazÃª lo consegui',
  'estÃ¡ hora colocarem campos endereÃ§os tipo padrÃ£o casa outro alternativo assim facilita vida pessoas velhas tÃªm dificuldades fora lixo fica histÃ³rico endereÃ§o',
  'gostava bastante opÃ§Ã£o pagamento online android Ã©poca 10 cashback pago via picpay aÃ­ tiraram es 10 %, ma tudo bem ), ma infelizmente tiraram sei motivo procedimento ficou menos prÃ¡tico uma pena gostaria saber justificativa retirada opÃ§Ã£o pagamento online',
  'nÃ£o gostei pago pix vcs estornam pagar crÃ©dito pra dar certo pedido amo esfihas habbs',
  'fiz compra supermercado entregador conseguiu passa la caixa poi algum problema sistema app tive cancelar pedido vou ter esperar 10 dia receber reembolso ainda vou ficar comida casa poi precisando des produtos urgentemente bem ainda dinheiro reserva imagina passando dificuldade ?! ficaria passando fome causa incompetÃªncia desse aplicativo pÃ©ssimo nÃ£o caiam nessa roubada',
  'precisa melhorar nÃ£o opÃ§Ã£o pedir troco isso atrapalha clientes entregadores poderia ter aplicativo opÃ§Ã£o troco prÃ³prio restaurante mandar porque facilitava ter ir atrÃ¡s destrocar dinheiro tomar tempo entregadores comeÃ§ando dinheiro ainda',
  'gosto serviÃ§o',
  'Ã© bom ma pode melhorar o app bom boa promoÃ§Ãµes o pode melhorar ter opÃ§Ã£o classificar estabelecimento porque compramos ter noÃ§Ã£o local bom ruim',
  'frete caro preÃ§os conta alÃ©m valor mÃ­nimo pedido ter aumentado outros aplicativos conta desinstalei app',
  'bom serviÃ§o simple utilizar prÃ¡tico',
  'aplicativo proposta boa ma comprar modalidade peÃ§a retire hora pagamento opÃ§Ãµes pagamento dÃ©bito crÃ©dito simplesmente abrem toda outras formas pagamento abrem menos duas',
  'essa atualizaÃ§Ã£o ta bugando app nao acesso'],
 'input_ids': tensor([[  101,   123,   940,  ...,     0,     0,     0],
         [  101, 20938,  1211,  ...,     0,     0,     0],
         [  101,   123, 11300,  ...,     0,     0,     0],
         ...,
         [  101,  4062,  2576,  ...,     0,     0,     0],
         [  101, 16357,  4562,  ...,     0,     0,     0],
         [  101,  1921, 14429,  ...,     0,     0,     0]]),
 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
         [1, 1, 1,  ..., 0, 0, 0],
         [1, 1, 1,  ..., 0, 0, 0],
         ...,
         [1, 1, 1,  ..., 0, 0, 0],
         [1, 1, 1,  ..., 0, 0, 0],
         [1, 1, 1,  ..., 0, 0, 0]]),
 'targets': tensor([0, 0, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 0, 2, 0, 2, 0, 2, 1, 1, 0, 1, 1,
         0, 1, 2, 2, 0, 2, 1, 0])}</code></pre>
</div>
</div>
<div class="cell" data-outputid="6ef4ee8b-8632-423e-be04-d396b627aae1" data-execution_count="219">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_data[<span class="st">'input_ids'</span>].size())</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_data[<span class="st">'attention_mask'</span>].size())</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_data[<span class="st">'targets'</span>].size())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 128])
torch.Size([32, 128])
torch.Size([32])</code></pre>
</div>
</div>
</section>
<section id="modeling" class="level1">
<h1>Modeling</h1>
<div class="cell" data-execution_count="220">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SentimentClassifier(nn.Module):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_classes):</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>(SentimentClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.bert <span class="op">=</span> BertModel.from_pretrained(<span class="st">'neuralmind/bert-base-portuguese-cased'</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.drop <span class="op">=</span> nn.Dropout(p<span class="op">=</span><span class="fl">.3</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.out <span class="op">=</span> nn.Linear(<span class="va">self</span>.bert.config.hidden_size, n_classes)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, input_ids, attention_mask):</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    bmodel <span class="op">=</span> <span class="va">self</span>.bert(</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        input_ids<span class="op">=</span>input_ids,</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>        attention_mask<span class="op">=</span>attention_mask</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    attentions <span class="op">=</span> bmodel.attentions</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    cross_attentions <span class="op">=</span> bmodel.cross_attentions</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    hidden_states <span class="op">=</span> bmodel.hidden_states</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    last_hidden_state <span class="op">=</span> bmodel.last_hidden_state</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    past_key_values <span class="op">=</span> bmodel.past_key_values</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    pooler_output <span class="op">=</span> bmodel.pooler_output</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> <span class="va">self</span>.drop(pooler_output)</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.out(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="4a397f33-fcd5-4974-dc1c-55384a095787" data-execution_count="221">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.sentimento.unique())</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> <span class="bu">list</span>([<span class="st">'negative'</span>, <span class="st">'neutral'</span>, <span class="st">'positive'</span>])</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SentimentClassifier(<span class="bu">len</span>(class_names))</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'The device will be using </span><span class="sc">{</span><span class="bu">str</span><span class="sc">.</span>upper(device)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0 1 2]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>The device will be using CUDA</code></pre>
</div>
</div>
</section>
<section id="training" class="level1">
<h1>Training</h1>
<p>Reproducing the procedure from the original BERT paper using the AdamW optimizer by Hugging Face. It corrects weight decay. Iâ€™ll be also using a linear scheduler with no warmup steps.</p>
<div class="cell" data-execution_count="222">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    model.parameters(),</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">5e-5</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>total_steps <span class="op">=</span> <span class="bu">len</span>(train_data_loader) <span class="op">*</span> EPOCHS</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> get_linear_schedule_with_warmup(</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    optimizer,</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    num_warmup_steps<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    num_training_steps<span class="op">=</span>total_steps</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss().to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="bert-authors-recommendations-for-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="bert-authors-recommendations-for-fine-tuning">BERT authors recommendations for fine-tuning</h2>
<table class="table">
<thead>
<tr class="header">
<th>Hyperparameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Batch size</td>
<td><code>16</code>, <code>32</code></td>
</tr>
<tr class="even">
<td>Learning rate (Adam)</td>
<td><code>5e-5</code>, <code>3e-5</code>, <code>2e-5</code></td>
</tr>
<tr class="odd">
<td>Number of epochs</td>
<td><code>2</code>, <code>3</code>, <code>4</code></td>
</tr>
</tbody>
</table>
<p>Iâ€™ll ignore the number of epochs recommendation but stick with the rest.</p>
<blockquote class="blockquote">
<p>NOTE: Increasing the batch size reduces the training time significantly, but gives lower accuracy.</p>
</blockquote>
<div class="cell" data-execution_count="223">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>  model <span class="op">=</span> model.train()</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>  losses <span class="op">=</span> <span class="bu">list</span>([])</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>  correct_predictions <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> data <span class="kw">in</span> data_loader:</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> data[<span class="st">'input_ids'</span>].to(device)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    attention_mask <span class="op">=</span> data[<span class="st">'attention_mask'</span>].to(device)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> data[<span class="st">'targets'</span>].to(device)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        input_ids<span class="op">=</span>input_ids,</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>        attention_mask<span class="op">=</span>attention_mask</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>    _, pred <span class="op">=</span> torch.<span class="bu">max</span>(output, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(output, targets)</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>    correct_predictions <span class="op">+=</span> torch.<span class="bu">sum</span>(pred <span class="op">==</span> targets)</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    losses.append(loss.item())</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    nn.utils.clip_grad_norm_(model.parameters(), max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>    scheduler.step()</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> correct_predictions <span class="op">/</span> n_examples, np.mean(losses)</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_model(model, data_loader, loss_fn, device, n_examples):</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>  model <span class="op">=</span> model.<span class="bu">eval</span>()</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>  losses <span class="op">=</span> <span class="bu">list</span>([])</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>  correct_predictions <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>  <span class="cf">with</span> torch.no_grad():</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> data <span class="kw">in</span> data_loader:</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>      input_ids <span class="op">=</span> data[<span class="st">'input_ids'</span>].to(device)</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>      attention_mask <span class="op">=</span> data[<span class="st">'attention_mask'</span>].to(device)</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>      targets <span class="op">=</span> data[<span class="st">'targets'</span>].to(device)</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>      output <span class="op">=</span> model(</span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>          input_ids<span class="op">=</span>input_ids,</span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a>          attention_mask<span class="op">=</span>attention_mask</span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a>      )</span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a>      _, pred <span class="op">=</span> torch.<span class="bu">max</span>(output, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a>      loss <span class="op">=</span> loss_fn(output, targets)</span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a>      correct_predictions <span class="op">+=</span> torch.<span class="bu">sum</span>(pred <span class="op">==</span> targets)</span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a>      losses.append(loss.item())</span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> correct_predictions <span class="op">/</span> n_examples, np.mean(losses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="6dda6fab-8d14-4796-ef7a-5f6492430077">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>best_accuracy <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCHS):</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="ss">f'EPOCH </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>EPOCHS<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>    train_acc, train_loss <span class="op">=</span> train_epoch(</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        train_data_loader,</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        loss_fn,</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>        optimizer,</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>        device,</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        scheduler,</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">len</span>(train_df)</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="ss">f'TRAIN LOSS: </span><span class="sc">{</span>train_loss<span class="sc">}</span><span class="ss">  |  ACCURACY: </span><span class="sc">{</span>train_acc<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>    val_acc, val_loss <span class="op">=</span> eval_model(</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>        val_data_loader,</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>        loss_fn,</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>        device,</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">len</span>(val_df)</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="ss">f'VALIDATION LOSS: </span><span class="sc">{</span>val_loss<span class="sc">}</span><span class="ss">  |  ACCURACY: </span><span class="sc">{</span>val_acc<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">'</span>)</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'train_acc'</span>].append(train_acc)</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'train_loss'</span>].append(train_loss)</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'val_acc'</span>].append(val_acc)</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'val_loss'</span>].append(val_loss)</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val_acc <span class="op">&gt;</span> best_accuracy:</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>      best_accuracy <span class="op">=</span> val_acc</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>      torch.save(model.state_dict(), <span class="st">'/content/drive/MyDrive/Colab Notebooks/models/best_model_state.bin'</span>)</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>  logger.success(<span class="ss">f'MODEL TRAINED WITH </span><span class="sc">{</span>EPOCHS<span class="sc">}</span><span class="ss"> EPOCHS AND ACHIEVED </span><span class="sc">{</span>best_accuracy<span class="sc">}</span><span class="ss"> OF BEST ACCURACY ðŸŽ‰'</span>)</span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a>  logger.error(<span class="ss">f'ERROR: </span><span class="sc">{</span><span class="bu">str</span><span class="sc">.</span>upper(<span class="bu">str</span>(e))<span class="sc">}</span><span class="ch">\n</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2023-05-10 01:03:09.343 | INFO     | __main__:&lt;module&gt;:6 - EPOCH 1/8
2023-05-10 01:05:54.132 | INFO     | __main__:&lt;module&gt;:18 - TRAIN LOSS: 0.8204962819482314  |  ACCURACY: 0.6082677245140076
2023-05-10 01:06:01.636 | INFO     | __main__:&lt;module&gt;:28 - VALIDATION LOSS: 0.792832124413866  |  ACCURACY: 0.6268518567085266


2023-05-10 01:06:03.247 | INFO     | __main__:&lt;module&gt;:6 - EPOCH 2/8
2023-05-10 01:08:55.383 | INFO     | __main__:&lt;module&gt;:18 - TRAIN LOSS: 0.604567931265636  |  ACCURACY: 0.7361046671867371
2023-05-10 01:09:03.050 | INFO     | __main__:&lt;module&gt;:28 - VALIDATION LOSS: 0.906921948447372  |  ACCURACY: 0.6222221851348877


2023-05-10 01:09:03.057 | INFO     | __main__:&lt;module&gt;:6 - EPOCH 3/8
2023-05-10 01:11:57.064 | INFO     | __main__:&lt;module&gt;:18 - TRAIN LOSS: 0.4040720313233514  |  ACCURACY: 0.8418248891830444
2023-05-10 01:12:04.752 | INFO     | __main__:&lt;module&gt;:28 - VALIDATION LOSS: 1.0174239657141946  |  ACCURACY: 0.6472222208976746


2023-05-10 01:12:06.610 | INFO     | __main__:&lt;module&gt;:6 - EPOCH 4/8
2023-05-10 01:15:00.639 | INFO     | __main__:&lt;module&gt;:18 - TRAIN LOSS: 0.282453167449586  |  ACCURACY: 0.8996062874794006
2023-05-10 01:15:08.266 | INFO     | __main__:&lt;module&gt;:28 - VALIDATION LOSS: 1.3024714841987148  |  ACCURACY: 0.6314814686775208


2023-05-10 01:15:08.268 | INFO     | __main__:&lt;module&gt;:6 - EPOCH 5/8
2023-05-10 01:18:02.294 | INFO     | __main__:&lt;module&gt;:18 - TRAIN LOSS: 0.20513046395075143  |  ACCURACY: 0.9279758930206299
2023-05-10 01:18:09.958 | INFO     | __main__:&lt;module&gt;:28 - VALIDATION LOSS: 1.3614619554895344  |  ACCURACY: 0.6379629373550415


2023-05-10 01:18:09.961 | INFO     | __main__:&lt;module&gt;:6 - EPOCH 6/8</code></pre>
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>