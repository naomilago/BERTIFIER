<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>bertifier_modeling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="BERTIFIER_Modeling_files/libs/clipboard/clipboard.min.js"></script>
<script src="BERTIFIER_Modeling_files/libs/quarto-html/quarto.js"></script>
<script src="BERTIFIER_Modeling_files/libs/quarto-html/popper.min.js"></script>
<script src="BERTIFIER_Modeling_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="BERTIFIER_Modeling_files/libs/quarto-html/anchor.min.js"></script>
<link href="BERTIFIER_Modeling_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="BERTIFIER_Modeling_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="BERTIFIER_Modeling_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="BERTIFIER_Modeling_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="BERTIFIER_Modeling_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<div class="cell" data-cellview="form" data-outputid="7e6aaa2b-e3de-4726-e0bd-2d82c5ee8a2f" data-execution_count="203">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#@title &amp;nbsp;</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display_html</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>style <span class="op">=</span> <span class="st">''''</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="st">    @import url('https://fonts.googleapis.com/css2?family=Comfortaa:wght@400;500;700&amp;display=swap');</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="st">    body {</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="st">        display: flex;</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="st">        justify-content: center;</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="st">        align-items: center;</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="st">        min-height: 100vh;</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="st">        margin: 0;</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="st">    .container {</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="st">        display: flex;</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="st">        justify-content: center;</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="st">        align-items: center;</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="st">        gap: 20px;</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="st">        margin-top: 30px;</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="st">    img {</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="st">        max-width: 100%;</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="st">        height: auto;</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="st">    h1 {</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="st">        margin: 0;</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="st">        size: 20px;</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="st">    }</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>display_html(<span class="ss">f'''</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;style&gt;</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span><span class="sc">{</span>style<span class="sc">}</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;/style&gt;</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;div class='container'&gt;</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="ss">    &lt;img src='https://gcdnb.pbrd.co/images/hgNPk95VoyK6.png?o=1', width=200&gt;</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="ss">    &lt;h1&gt;Modeling&lt;/h1&gt;</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;/div&gt;</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;br&gt;</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="ss">&lt;br&gt;</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="ss">'''</span>, raw<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    '
    @import url('https://fonts.googleapis.com/css2?family=Comfortaa:wght@400;500;700&display=swap');

    body {
        display: flex;
        justify-content: center;
        align-items: center;
        min-height: 100vh;
        margin: 0;
    }

    .container {
        display: flex;
        justify-content: center;
        align-items: center;
        gap: 20px;
        margin-top: 30px;
    }

    img {
        max-width: 100%;
        height: auto;
    }

    h1 {
        margin: 0;
        size: 20px;
    }


</style>

<div class="container">
    <img src="https://gcdnb.pbrd.co/images/hgNPk95VoyK6.png?o=1" ,="" width="200">
    <h1>Modeling</h1>
</div>
<br>
<br>
</div>
</div>
<section id="setting-up" class="level1">
<h1>Setting up</h1>
<div class="cell" data-execution_count="204">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install transformers demoji loguru</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="61a60c92-4f6d-4eaf-8c7a-a0072bc02023" data-execution_count="205">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> WordPunctTokenizer</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem <span class="im">import</span> WordNetLemmatizer</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> textwrap <span class="im">import</span> wrap</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> loguru <span class="im">import</span> logger</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> cuda</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> demoji</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    get_linear_schedule_with_warmup,</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    BertTokenizer,</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    BertConfig,</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    BertModel</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> (</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    SequentialSampler,</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    RandomSampler,</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    DataLoader,</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    Dataset</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> (</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    classification_report,</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>    confusion_matrix,</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>drive.mount(<span class="st">'/content/drive'</span>)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'/content/drive/MyDrive/Colab Notebooks/data/reviews_preprocessed.csv'</span>)</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>df.dropna(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> df.<span class="bu">all</span> <span class="op">!=</span> <span class="va">None</span>:</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Dataset imported successfully 🎉'</span>)</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Dataset import failed ☹'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).
Dataset imported successfully 🎉</code></pre>
</div>
</div>
</section>
<section id="quick-eda" class="level1">
<h1>Quick EDA</h1>
<div class="cell" data-outputid="a5e125f7-2bca-4ddb-caec-52378dd3a724" data-execution_count="206">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'stopwords'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="206">
<pre><code>True</code></pre>
</div>
</div>
<div class="cell" data-execution_count="207">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>stopwords <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">'portuguese'</span>))</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>punctuations <span class="op">=</span> <span class="bu">set</span>(string.punctuation)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>tokens_frequencies <span class="op">=</span> Counter([item <span class="cf">for</span> sublist <span class="kw">in</span> df[<span class="st">'tokens'</span>] <span class="cf">for</span> item <span class="kw">in</span> sublist.split() <span class="cf">if</span> item <span class="kw">not</span> <span class="kw">in</span> stopwords <span class="kw">and</span> item <span class="kw">not</span> <span class="kw">in</span> punctuations])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="722c9be7-e42c-4939-e330-58535ef09fee" data-execution_count="208">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>aux <span class="op">=</span> pd.DataFrame(tokens_frequencies.most_common(<span class="dv">10</span>), columns<span class="op">=</span>[<span class="st">'Item'</span>, <span class="st">'Quantidade'</span>])</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.bar(</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    aux,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">'Item'</span>,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span><span class="st">'Quantidade'</span>,</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">'Quantidade'</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>fig.layout.template <span class="op">=</span> <span class="st">'plotly_dark'</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>fig.layout.title <span class="op">=</span> <span class="st">'Frequência de itens comentados'</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">


<meta charset="utf-8">

    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.18.2.min.js"></script>                <div id="1c30e6e4-219d-4985-9ee9-bdba5d26fe46" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("1c30e6e4-219d-4985-9ee9-bdba5d26fe46")) {                    Plotly.newPlot(                        "1c30e6e4-219d-4985-9ee9-bdba5d26fe46",                        [{"alignmentgroup":"True","hovertemplate":"Item=%{x}<br>Quantidade=%{marker.color}<extra></extra>","legendgroup":"","marker":{"color":[4181,2826,2354,2018,1799,1434,1333,1209,1164,1131],"coloraxis":"coloraxis","pattern":{"shape":""}},"name":"","offsetgroup":"","orientation":"v","showlegend":false,"textposition":"auto","x":["'app',","'pedido',","'ma',","'aplicativo',","'pra',","'entrega',","'bom',","['o',","'vezes',","'op\u00e7\u00e3o',"],"xaxis":"x","y":[4181,2826,2354,2018,1799,1434,1333,1209,1164,1131],"yaxis":"y","type":"bar"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#f2f5fa"},"error_y":{"color":"#f2f5fa"},"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"baxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"line":{"color":"#283442"}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"marker":{"line":{"color":"#283442"}},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#506784"},"line":{"color":"rgb(17,17,17)"}},"header":{"fill":{"color":"#2a3f5f"},"line":{"color":"rgb(17,17,17)"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#f2f5fa","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#f2f5fa"},"geo":{"bgcolor":"rgb(17,17,17)","lakecolor":"rgb(17,17,17)","landcolor":"rgb(17,17,17)","showlakes":true,"showland":true,"subunitcolor":"#506784"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"dark"},"paper_bgcolor":"rgb(17,17,17)","plot_bgcolor":"rgb(17,17,17)","polar":{"angularaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","radialaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"yaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"zaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"}},"shapedefaults":{"line":{"color":"#f2f5fa"}},"sliderdefaults":{"bgcolor":"#C8D4E3","bordercolor":"rgb(17,17,17)","borderwidth":1,"tickwidth":0},"ternary":{"aaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"baxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","caxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"title":{"x":0.05},"updatemenudefaults":{"bgcolor":"#506784","borderwidth":0},"xaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Item"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Quantidade"}},"coloraxis":{"colorbar":{"title":{"text":"Quantidade"}},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"legend":{"tracegroupgap":0},"margin":{"t":60},"barmode":"relative","title":{"text":"Frequ\u00eancia de itens comentados"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('1c30e6e4-219d-4985-9ee9-bdba5d26fe46');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>


</div>
</div>
<div class="cell" data-outputid="611e1d94-21da-4150-b4ad-7d69d44001ea" data-execution_count="209">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>tokens_amount <span class="op">=</span> [<span class="bu">len</span>(tokens) <span class="cf">for</span> tokens <span class="kw">in</span> df.tokens.values]</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.histogram(</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    tokens_amount,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    nbins<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    color_discrete_sequence<span class="op">=</span>[<span class="st">'orange'</span>]</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>fig.layout.title <span class="op">=</span> <span class="st">'Distribution of token amount'</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>fig.layout.template <span class="op">=</span> <span class="st">'plotly_dark'</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">


<meta charset="utf-8">

    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.18.2.min.js"></script>                <div id="ef6ec8ae-66ff-43b2-9bf6-d0f06fb74d94" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("ef6ec8ae-66ff-43b2-9bf6-d0f06fb74d94")) {                    Plotly.newPlot(                        "ef6ec8ae-66ff-43b2-9bf6-d0f06fb74d94",                        [{"alignmentgroup":"True","bingroup":"x","hovertemplate":"variable=0<br>value=%{x}<br>count=%{y}<extra></extra>","legendgroup":"0","marker":{"color":"orange","pattern":{"shape":""}},"name":"0","nbinsx":100,"offsetgroup":"0","orientation":"v","showlegend":true,"x":[355,441,403,445,489,400,462,339,312,289,416,260,536,508,360,493,358,216,397,387,471,390,364,469,397,465,520,327,369,523,427,267,478,545,384,379,453,486,357,446,408,342,443,414,291,425,333,300,348,345,482,323,582,482,538,338,382,500,453,230,488,357,302,320,369,532,368,302,540,334,343,493,325,441,533,503,428,533,398,330,536,502,382,325,469,523,463,274,242,468,572,313,462,316,390,257,354,246,423,472,160,536,367,431,110,147,228,59,247,165,206,71,307,423,43,46,48,94,126,434,44,498,176,96,164,213,174,19,141,33,41,165,216,104,208,52,590,171,37,258,522,64,193,23,19,65,200,462,403,53,130,146,165,188,343,124,102,104,68,134,85,479,63,313,579,96,206,235,40,188,172,183,47,130,144,157,90,239,98,89,50,128,325,51,77,312,117,86,66,42,66,155,128,110,58,520,318,104,310,133,419,362,357,494,510,481,513,470,433,528,484,349,459,373,472,363,437,478,416,410,401,479,385,449,460,560,638,454,401,341,382,354,349,411,260,247,263,361,464,428,328,320,479,262,382,383,498,254,484,340,278,371,537,406,393,264,589,220,293,255,502,520,234,507,206,510,385,561,191,350,484,293,312,377,318,298,216,197,243,355,231,183,208,189,249,217,283,123,202,191,289,295,262,174,260,184,568,211,267,143,200,128,86,127,43,44,18,30,140,184,210,191,61,183,354,106,361,23,118,61,28,141,68,93,487,113,7,190,185,101,149,64,118,98,176,341,44,82,225,84,46,105,122,173,27,262,95,7,112,260,103,64,165,143,61,35,316,123,175,229,48,6,206,34,104,94,42,174,217,93,278,208,130,87,12,247,382,7,100,31,90,15,143,320,355,197,262,129,56,428,133,14,113,65,189,121,25,241,263,283,469,508,461,449,484,551,397,486,466,499,508,494,511,400,575,363,436,430,627,457,353,411,373,486,779,329,537,331,380,463,372,339,558,461,359,432,321,377,248,390,418,510,511,323,344,303,353,236,464,271,479,481,311,539,258,286,194,343,317,462,542,353,288,272,299,567,252,209,337,213,558,301,245,299,368,467,499,212,505,169,222,221,268,296,209,193,260,203,167,410,213,208,224,151,263,292,177,148,275,206,442,271,313,446,314,119,363,371,83,406,350,192,177,164,258,155,214,329,207,283,318,326,319,231,229,268,171,254,317,201,362,166,419,228,196,208,153,228,274,151,297,263,205,362,309,122,173,212,191,262,531,157,223,312,197,164,152,173,542,218,494,438,372,368,494,536,568,376,477,439,217,105,515,333,296,174,185,378,384,128,140,292,144,118,121,188,446,335,362,197,252,574,248,130,336,510,376,437,107,131,46,20,64,81,107,29,61,169,235,117,27,29,96,7,19,138,7,86,49,76,19,14,222,104,34,177,6,67,35,23,10,35,79,16,93,9,174,43,26,286,9,9,54,49,13,48,7,124,54,59,100,52,7,39,86,101,24,60,132,23,22,7,15,20,50,9,296,311,127,60,61,36,212,116,124,38,128,98,164,47,209,119,14,43,58,7,260,167,41,88,85,321,359,121,9,9,14,107,124,143,119,461,7,28,11,499,36,279,68,183,7,53,48,296,30,19,11,193,16,101,45,147,7,221,91,55,22,31,10,25,47,7,43,285,12,59,268,95,505,539,22,59,9,258,30,72,130,7,208,102,51,390,377,26,37,206,21,54,34,70,305,21,83,46,64,8,71,467,7,194,140,418,104,93,7,157,15,148,464,7,65,7,21,78,7,294,29,7,54,13,140,85,188,125,43,99,120,36,272,18,546,466,464,428,556,448,510,513,461,480,414,502,514,363,418,380,382,478,413,337,624,436,543,356,499,459,333,578,528,232,213,349,522,226,210,304,193,248,243,210,212,381,350,339,463,203,467,217,319,428,373,426,357,281,398,320,196,157,465,255,302,190,321,305,306,253,512,202,215,187,310,156,197,230,436,279,155,241,175,201,243,144,195,190,210,115,109,246,172,399,163,282,154,191,172,232,263,432,190,313,105,69,25,21,17,86,42,49,59,151,62,20,30,56,74,9,20,99,7,43,62,108,7,21,175,16,32,27,76,49,33,38,105,13,7,58,11,23,10,83,106,11,47,56,7,35,62,64,9,113,21,83,96,7,18,321,6,15,32,17,7,10,7,7,79,151,62,17,46,210,223,6,20,29,144,75,30,21,113,129,87,26,14,9,7,7,102,89,26,73,28,22,7,43,7,7,42,9,201,20,507,247,269,232,437,359,237,282,351,250,257,194,185,345,145,338,353,378,278,270,226,378,194,549,192,545,246,187,253,476,404,342,210,171,214,388,281,295,177,234,163,157,191,150,281,163,228,246,116,159,127,205,140,178,392,238,196,202,203,265,264,333,177,210,139,511,300,284,186,361,224,226,337,167,384,98,250,155,102,156,146,143,218,184,170,206,213,147,193,143,157,323,457,265,167,194,225,116,192,177,43,9,35,8,6,9,23,67,34,54,9,13,19,91,54,7,27,8,19,48,93,7,51,16,7,20,8,31,49,30,55,41,67,50,7,15,7,49,33,31,24,75,74,7,56,9,86,106,53,11,20,29,13,34,17,93,45,26,56,14,82,41,47,31,53,16,7,42,53,13,49,13,16,7,9,15,35,66,63,6,36,7,77,9,14,41,136,246,19,19,14,9,55,6,10,66,9,56,32,9,523,544,228,253,329,220,330,434,232,190,337,178,434,276,307,178,552,204,194,146,246,187,268,417,445,297,373,405,434,208,512,178,343,264,215,476,235,191,302,195,136,244,165,231,273,203,237,393,505,115,216,169,182,165,302,216,404,436,375,338,443,527,375,161,191,418,373,297,338,139,425,536,426,343,130,262,126,198,309,441,235,317,397,290,118,83,105,103,123,89,142,403,129,147,140,233,557,88,500,353,25,176,19,307,41,244,79,89,544,57,198,11,51,552,129,146,399,74,142,434,20,151,291,499,204,37,403,204,86,178,48,523,42,130,232,89,190,131,147,27,28,194,434,42,109,520,187,233,15,171,198,44,456,43,178,57,103,139,32,337,61,11,121,31,16,44,36,220,123,136,97,34,36,65,63,178,228,196,112,83,329,121,417,77,266,8,118,75,75,302,19,137,51,76,369,61,7,124,276,27,344,430,326,188,305,130,500,293,373,347,169,294,238,185,160,524,145,427,238,188,409,286,152,472,437,62,488,384,263,260,353,390,291,231,361,181,273,192,486,157,211,170,325,147,232,74,89,283,145,223,472,341,310,199,261,228,279,275,326,220,468,284,307,229,152,203,301,240,198,257,206,207,227,157,286,193,151,239,177,147,124,226,129,200,147,208,250,165,168,149,99,306,149,224,71,181,127,222,391,197,305,42,62,188,83,145,52,137,27,430,48,9,130,84,10,60,344,238,160,89,524,31,326,40,169,153,28,99,185,50,36,21,99,26,188,183,32,24,9,143,74,71,71,55,500,84,30,97,177,152,46,71,232,12,58,152,56,343,157,10,44,20,192,79,40,94,187,61,488,42,147,134,48,38,64,8,325,73,45,384,148,25,132,65,192,52,147,181,104,58,75,263,156,231,62,7,208,230,124,28,515,331,201,379,157,334,176,214,120,298,109,471,400,493,379,162,319,236,252,170,246,267,478,453,504,298,71,173,147,106,305,136,341,310,137,106,105,314,238,325,72,245,234,522,223,207,280,243,211,266,252,238,224,214,243,143,289,139,113,101,111,133,213,126,72,316,98,147,99,185,196,154,203,209,260,303,209,302,196,182,186,182,185,241,244,192,245,160,229,420,171,66,146,142,241,238,187,238,235,247,125,249,123,140,114,141,165,345,142,473,140,167,115,486,90,47,68,516,162,173,382,504,144,415,507,46,108,397,558,155,151,157,526,268,458,114,492,508,283,514,524,432,490,292,284,543,383,389,464,502,502,275,294,399,229,302,320,250,115,111,173,284,334,329,320,282,329,288,213,371,300,112,325,114,367,379,123,73,152,98,428,376,341,277,537,346,61,502,288,535,318,248,237,130,442,133,109,174,84,114,101,238,73,52,515,39,37,68,47,101,120,20,126,157,75,74,64,27,106,14,106,201,331,7,173,90,298,46,61,34,72,48,109,23,72,106,176,71,40,138,11,108,7,23,66,101,75,93,86,18,265,33,108,334,22,214,37,96,111,105,61,45,21,113,14,76,162,26,44,106,37,99,70,379,46,59,137,64,21,43,165,135,61,24,59,147,36,72,59,98,87,34,38,64,108,86,62,319,246,86,75,67,147,32,98,65,60,12,298,114,54,61,133,35,136,173,38,248,54,51,165,504,60,215,57,109,64,114,128,243,379,60,73,139,103,280,125,74,144,176,133,22,71,45,53,185,40,23,91,453,91,41,224,69,84,185,22,174,40,234,252,247,243,316,52,114,13,245,205,305,82,44,12,62,27,213,223,143,10,7,111,139,48,244,142,76,211,85,106,31,130,39,20,98,214,238,42,8,18,207,445,492,217,153,521,158,224,79,349,85,77,127,350,72,73,77,108,393,216,246,131,171,303,476,259,256,241,403,85,153,207,258,169,209,266,205,328,235,107,71,178,205,167,275,193,117,192,55,157,187,203,208,439,211,144,157,170,266,160,150,241,160,215,212,170,135,226,159,119,178,148,115,160,206,148,132,455,202,103,197,470,121,65,138,103,475,134,218,143,526,102,356,42,407,141,82,251,194,64,303,129,77,21,59,79,55,17,34,77,42,7,55,86,85,47,141,237,38,73,42,21,117,45,69,72,131,91,38,79,64,83,33,14,102,331,51,35,7,31,9,51,29,108,35,60,8,55,15,24,78,45,95,18,7,40,54,23,59,217,85,60,22,68,16,17,167,22,63,71,21,41,42,79,23,78,153,124,16,169,25,7,63,56,14,24,52,31,58,9,42,107,27,7,54,31,11,36,54,80,28,371,396,171,234,304,178,128,167,190,266,198,125,574,149,168,216,99,126,381,145,169,113,142,124,142,174,137,216,131,237,146,124,98,121,78,108,76,74,96,113,106,59,103,93,97,83,106,113,69,75,78,78,78,207,50,62,79,178,117,154,76,152,141,46,227,52,62,54,105,393,53,232,50,63,72,147,173,305,141,120,65,83,108,79,40,87,61,142,44,71,62,115,85,123,82,336,257,422,298,149,53,135,7,101,16,21,64,26,61,174,33,7,7,25,41,7,21,188,20,36,34,13,34,18,24,97,53,38,58,7,63,54,27,16,137,21,21,34,33,14,24,41,16,48,7,20,10,65,7,35,13,53,39,29,39,15,20,61,14,167,50,7,19,14,102,20,56,9,13,27,9,23,7,9,17,17,9,56,52,21,15,24,7,84,9,16,68,19,91,9,104,31,31,7,16,10,40,9,20,33,358,478,338,431,214,281,413,322,194,311,308,362,246,402,276,393,285,391,343,463,544,162,390,262,297,173,324,477,498,425,233,361,501,559,474,264,212,326,196,292,288,225,513,346,442,332,226,338,376,311,186,280,540,200,208,458,259,236,483,252,293,407,513,211,178,280,451,170,289,329,239,253,249,235,166,395,198,333,230,576,169,345,162,164,229,107,110,480,468,328,322,200,179,184,331,227,155,213,299,500,72,338,139,173,10,19,73,183,37,101,96,21,37,208,16,169,70,100,324,70,103,95,194,362,358,25,46,184,63,480,29,27,12,41,55,72,476,73,43,413,30,89,311,204,16,19,54,107,17,365,14,100,114,42,22,158,226,297,169,378,239,116,98,111,12,44,26,322,10,17,30,902,30,150,58,74,102,431,43,214,30,362,62,119,50,540,28,281,165,198,105,402,112,225,12,193,105,276,202,71,136,199,209,193,110,415,316,101,235,326,138,208,295,149,229,125,338,314,149,512,84,154,84,112,131,263,131,79,127,166,109,93,245,351,380,165,188,240,370,238,205,134,272,386,130,166,324,212,186,189,221,194,51,141,280,234,157,82,365,277,305,27,375,485,209,594,298,54,145,125,86,157,448,145,298,54,125,59,261,189,127,592,278,517,109,64,80,36,491,192,314,146,461,111,52,82,309,264,121,220,12,61,104,27,199,82,193,110,91,26,84,65,136,209,138,54,338,208,22,82,36,109,397,55,33,150,166,22,57,112,80,86,41,64,131,263,36,84,54,229,179,109,154,26,82,19,205,8,9,111,20,54,34,28,127,125,52,26,9,14,235,52,93,131,166,21,10,9,125,51,25,130,149,101,77,22,81,62,116,79,25,149,370,141,6,38,295,12,59,16,15,72,22,189,261,125,49,6,237,221,497,364,488,239,159,201,390,142,201,342,144,119,69,156,51,241,77,69,195,91,549,183,189,96,492,98,494,305,118,41,313,158,225,57,70,394,243,406,226,264,174,224,141,248,545,139,297,153,120,92,122,134,98,88,62,76,102,231,270,317,258,463,105,278,335,116,239,172,236,452,286,464,51,174,252,102,249,73,354,207,151,69,244,299,102,209,273,190,214,122,93,47,25,57,70,281,276,197,248,246,428,177,389,377,197,176,164,205,195,331,153,257,348,227,329,243,191,198,138,223,263,278,340,228,171,198,199,269,189,187,144,270,321,213,180,205,196,257,342,231,322,150,238,177,246,193,163,176,236,218,347,189,173,166,256,252,211,261,261,205,207,174,176,272,170,235,201,129,203,173,209,233,352,186,228,290,133,70,139,114,101,135,212,124,210,278,232,283,176,101,152,142,251,126,258,44,335,167,178,119,239,158,327,6,25,91,38,69,45,41,70,33,118,10,7,57,9,98,102,28,159,41,7,156,65,142,20,51,364,57,70,8,85,497,69,59,549,69,224,41,488,77,70,119,7,174,7,18,20,96,22,201,51,144,31,18,62,6,23,47,7,94,28,7,201,7,9,70,28,36,26,35,44,16,7,26,335,48,36,9,33,203,141,61,62,33,76,21,12,35,46,26,7,7,27,6,39,87,50,28,102,6,20,7,81,7,30,53,45,75,24,47,23,95,17,39,116,342,129,113,29,98,20,120,9,32,48,42,33,29,73,11,70,139,195,6,14,102,22,35,12,21,25,134,183,17,42,88,141,21,14,93,346,7,16,76,72,95,16,89,44,170,122,9,151,45,92,153,105,87,25,260,26,57,122,39,38,7,189,23,11,7,390,10,241,99,40,354,17,6,18,7,22,36,33,396,43,79,16,453,498,619,150,207,173,100,77,107,137,89,218,174,83,173,109,78,389,307,285,185,115,117,127,141,159,29,292,72,189,234,99,62,267,284,501,324,134,137,206,297,159,294,98,49,53,542,98,60,31,298,239,32,81,45,138,102,228,159,142,136,137,99,129,69,60,198,324,298,57,311,211,179,255,54,91,47,237,210,303,150,73,186,261,53,44,34,38,136,176,181,194,205,359,310,165,450,151,311,252,44,18,80,12,498,53,48,29,7,20,34,22,89,77,9,9,115,38,72,453,137,19,218,107,99,24,12,36,36,9,9,159,207,542,98,63,53,99,9,31,60,44,12,47,109,49,7,6,77,26,22,25,619,174,13,150,10,116,300,235,78,9,173,134,7,7,25,9,13,21,46,62,8,81,159,6,9,117,24,9,91,83,173,14,21,45,10,127,6,32,100,22,17,14,16,98,7,40,17,5,307,352,318,110,549,559,88,88,291,109,89,119,134,116,66,101,52,128,173,456,138,157,92,137,80,206,238,214,528,129,535,534,67,312,81,52,234,60,57,53,401,75,404,205,100,55,60,58,337,88,324,268,34,94,41,264,74,120,103,89,35,45,38,55,357,35,88,90,56,68,66,74,62,64,72,20,459,410,44,33,94,76,219,55,131,31,108,48,45,51,75,32,182,64,47,217,78,46,94,21,15,29,15,22,12,89,71,20,21,38,7,7,56,52,7,75,50,140,25,7,30,156,109,9,17,6,30,14,9,13,46,18,21,98,35,7,98,13,7,41,9,18,7,68,35,88,15,7,18,57,7,307,6,16,7,9,42,22,100,16,116,115,26,41,15,6,20,9,34,9,20,8,39,22,9,28,9,7,35,8,9,110,7,7,9,7,20,31,19,32,7,34,7,41,21,35,28,9,13,18,340,511,507,397,442,461,509,430,345,529,500,436,390,361,318,425,364,539,201,368,335,389,222,289,398,502,438,517,333,547,308,380,338,482,262,502,270,312,283,444,432,234,315,474,314,369,251,265,181,283,325,373,283,412,317,326,278,414,250,244,523,306,344,582,571,207,387,573,224,500,241,217,209,401,391,340,164,286,269,511,312,232,204,213,162,177,188,203,105,181,173,276,220,242,160,114,336,283,384,184,92,589,45,472,111,164,18,23,177,62,234,67,211,32,19,181,122,136,181,358,110,21,368,23,40,79,283,119,539,12,68,408,41,31,52,340,220,149,189,26,128,27,489,188,68,130,48,114,80,81,81,127,125,227,46,59,17,162,177,582,130,131,519,289,541,214,364,325,75,103,17,114,114,217,412,64,39,203,30,362,19,444,48,492,511,129,98,146,248,30,437,123,41,42,196,200,81,284,143,391,497,399,570,525,355,365,389,495,562,420,469,122,183,393,266,151,243,128,157,376,530,305,358,448,204,193,166,213,156,265,129,231,106,155,209,116,182,346,138,260,125,147,97,97,406,210,43,207,158,181,88,244,74,103,109,129,92,129,124,65,102,119,70,143,224,245,236,278,209,257,212,148,198,205,137,167,125,113,135,339,46,429,64,440,158,115,391,116,366,483,74,201,207,519,105,272,528,146,80,328,157,74,55,74,46,106,60,19,35,125,39,138,50,495,339,97,147,70,43,80,129,183,122,62,36,97,201,365,43,151,128,11,266,51,153,57,92,73,43,28,8,129,420,24,74,243,49,73,40,193,38,469,28,166,119,50,30,29,19,112,11,29,16,50,562,116,103,126,389,9,182,31,156,47,158,65,204,250,18,56,38,155,121,448,358,64,20,124,19,88,129,102,12,87,109,213,34,19,45,27,491,419,383,401,434,490,269,538,227,265,245,211,171,126,178,181,354,192,172,254,143,133,343,220,249,196,147,570,185,146,178,121,143,170,165,226,198,137,89,124,178,160,93,85,444,82,75,61,90,85,77,72,131,369,54,202,199,73,86,174,146,196,50,95,106,67,343,241,160,158,244,318,187,212,178,164,100,117,139,232,149,102,143,175,175,129,84,64,379,78,527,284,123,104,392,372,362,442,343,56,369,110,43,106,408,338,99,275,41,77,57,32,41,321,94,465,43,112,117,545,109,313,70,430,354,69,273,114,298,514,111,78,338,356,319,367,481,404,275,386,443,341,296,502,294,364,402,424,323,227,538,291,339,320,305,262,455,271,367,325,313,320,341,307,256,227,318,271,258,427,304,429,396,226,250,235,306,210,366,405,205,300,551,291,217,269,330,570,57,468,377,237,210,72,421,228,312,563,441,268,57,21,85,75,44,41,147,68,73,23,181,41,63,131,19,126,490,77,43,48,57,9,30,90,77,93,245,67,211,41,124,121,89,84,37,178,57,227,9,171,43,41,72,85,202,32,56,74,54,61,6,56,82,269,56,40,52,165,29,7,52,34,63,50,146,137,21,220,10,117,7,104,69,57,102,192,150,39,185,95,34,29,20,226,80,99,36,538,23,21,66,78,172,57,65,178,160,132,265,178,27,133,254,64,42,170,110,198,249,343,124,86,81,64,196,106,146,7,70,9,28,9,135,143,29,42,45,44,11,42,143,99,39,38,92,47,106,67,60,36,52,88,354,174,109,123,111,111,19,196,94,34,33,49,98,68,86,126,68,369,78,28,72,55,87,46,175,11,343,44,82,109,139,39,68,175,28,53,130,244,110,53,74,93,444,62,59,114,216,45,86,25,40,76,36,54,81,112,343,30,457,456,380,381,327,257,509,369,243,218,173,290,196,130,153,198,219,133,185,162,511,128,129,178,118,79,357,195,468,442,87,207,151,101,273,82,431,189,100,90,91,56,90,133,60,106,88,214,64,91,164,296,145,140,189,110,36,127,88,67,103,90,75,54,95,82,72,48,63,118,340,129,255,300,133,132,144,82,54,169,90,71,124,48,50,143,82,143,72,141,505,354,62,107,115,121,36,381,386,49,22,36,51,62,29,118,44,17,26,38,164,56,83,10,56,19,49,39,25,20,34,107,257,31,130,218,42,100,101,24,23,10,48,90,106,13,28,185,21,21,21,16,88,381,21,78,32,64,90,34,6,91,20,63,82,246,29,290,60,139,196,133,102,7,62,243,73,7,24,48,327,442,153,60,61,509,6,207,30,17,21,162,110,17,151,87,12,40,12,129,39,178,82,54,91,103,95,129,83,76,523,217,469,211,300,211,178,257,264,206,140,117,214,218,218,195,130,135,196,161,194,153,341,205,96,85,134,266,166,95,120,91,172,438,193,117,94,79,100,116,122,130,84,140,111,159,131,110,64,78,67,84,63,52,66,57,72,127,96,95,95,54,104,42,64,59,118,50,131,349,111,70,38,94,101,113,106,136,36,158,39,46,126,101,103,103,98,60,132,128,84,145,202,78,80,71,114,101,71,93,7,17,61,7,16,30,16,7,7,56,111,7,178,15,23,72,16,40,24,9,25,138,35,39,16,32,60,97,41,7,73,50,29,14,10,63,39,16,23,9,20,24,53,13,30,16,16,7,41,19,9,57,28,23,16,32,91,23,41,36,22,7,33,57,44,9,22,110,7,7,7,15,68,19,31,44,64,34,29,36,7,53,56,12,46,54,13,21,28,26,7,7,18,14,21,28,33,31,36,501,410,433,435,322,131,220,298,194,238,199,187,309,537,417,173,195,409,291,283,451,298,107,129,427,260,345,107,100,151,73,91,173,156,369,400,219,237,385,259,352,295,272,258,282,200,217,281,205,267,178,410,163,372,155,494,367,377,178,419,341,453,396,547,532,349,525,530,457,360,466,45,130,434,560,380,399,114,374,471,372,513,409,454,483,507,488,429,463,340,448,474,443,392,347,399,460,506,416,511,45,173,220,107,345,131,100,117,410,45,74,15,155,271,151,53,194,199,60,59,107,129,185,404,202,469,168,403,187,173,427,435,294,298,73,128,156,137,180,51,245,527,238,91,322,50,273,65,195,124,143,360,129,46,369,267,32,165,160,64,205,56,206,141,457,309,30,112,271,471,157,136,417,107,148,60,13,120,12,387,192,245,288,281,123,715,31,282,442,65,107,272,309,85,36,54,385,239,380,230,409,299,461,190,197,378,406,344,131,336,526,351,328,366,444,527,523,308,318,330,415,379,339,493,407,517,410,220,411,313,259,265,271,312,362,485,293,357,450,282,201,404,296,154,260,436,438,280,398,274,494,355,332,543,430,537,372,529,220,483,278,319,405,422,236,524,248,200,431,404,476,341,413,445,211,344,386,533,260,542,338,523,228,411,318,431,458,230,355,485,520,195,424,290,486,470,494,239,499,288,299,182,197,163,40,190,461,180,154,72,378,45,220,131,243,277,26,196,61,126,147,404,225,372,75,147,206,350,450,240,175,126,138,241,329,111,292,266,58,57,188,10,92,140,167,314,121,208,54,156,355,72,244,151,97,66,137,69,470,493,470,328,407,147,189,145,200,48,67,470,132,499,214,73,206,371,118,121,147,111,204,75,344,177,205,35,386,55,415,69,587,77,194,209,91,58,267,59,40,46,518,167,453,545,488,350,359,512,435,478,394,516,480,531,443,308,384,313,326,338,338,456,447,290,288,310,304,391,378,270,316,481,360,283,445,360,300,485,298,257,393,331,298,377,365,270,493,249,475,280,234,457,214,209,162,381,304,549,418,346,236,204,431,474,421,264,320,263,287,329,416,188,375,260,193,275,223,350,303,265,234,223,279,245,310,414,373,300,320,268,350,487,433,262,290,324,445,391,407,220,230,192,218,219,402,277,234,316,245,184,410,354,505,234,386,339,176,309,340,444,555,287,207,224,295,202,226,196,239,229,187,296,381,221,256,209,209,129,321,267,250,170,222,476,265,164,373,250,240,202,344,165,337,238,282,202,276,129,316,306,274,325,225,316,194,273,215,400,154,220,415,251,306,412,158,387,357,142,225,222,282,544,264,577,238,135,250,169,301,263,245,198,168,254,300,228,164,193,188,240,516,167,518,151,140,176,51,67,99,31,393,47,129,24,88,68,88,9,66,245,7,229,154,31,227,336,352,31,154,10,124,32,275,132,272,354,90,169,40,23,8,38,80,203,60,295,64,110,91,136,96,200,512,142,179,62,38,475,98,44,81,120,11,53,21,16,17,474,96,7,59,57,189,57,89,52,55,487,400,41,135,283,126,28,7,300,7,17,30,134,359,140,53,43,120,443,57,252,22,21,156,216,72,176,344,148,12,45,78,43,82,96,116,123,7,216,146,13,108,218,120,57,14,205,198,53,73,219,42,46,12,513,199,164,143,9,95,32,96,160,77,9,135,387,260,241,99,39,303,52,273,177,134,50,90,109,169,18,96,95,7,325,112,391,72,64,20,20,20,211,44,124,256,77,83,14,238,111,196,35,144,55,433,20,50,27,20,58,64,47,226,74,276,9,66,88,41,90,124,55,332,383,358,407,309,328,504,339,488,289,355,355,559,338,251,259,420,294,271,327,355,537,322,233,563,332,408,316,522,298,250,448,518,541,184,248,301,342,161,306,183,349,244,203,449,425,195,252,438,259,183,286,209,282,534,344,215,289,189,355,277,267,337,377,363,277,336,228,206,223,273,303,294,223,384,195,194,464,264,353,172,263,197,232,360,247,240,300,214,134,228,272,284,232,157,186,168,190,291,265,19,128,96,70,44,138,52,40,147,243,6,220,360,116,203,96,97,468,20,48,64,68,110,363,69,41,28,35,145,16,141,201,172,51,223,121,34,21,152,333,58,522,16,31,57,237,80,136,67,306,14,30,7,169,76,7,117,208,82,7,68,50,173,99,52,7,107,7,7,70,71,282,129,29,104,35,7,7,45,123,16,208,116,24,7,141,117,58,23,15,30,140,30,7,262,108,12,360,20,16,518,442,480,395,347,549,397,526,294,306,442,527,357,473,349,422,306,268,218,308,322,478,333,263,440,208,461,226,195,577,518,284,214,219,538,181,517,233,340,548,304,262,253,323,249,201,182,237,252,217,473,282,520,194,556,322,362,218,302,359,268,163,57,474,199,173,156,236,192,392,241,215,165,271,179,258,263,318,323,183,271,148,165,216,290,225,362,245,162,203,195,218,158,406,253,136,196,183,470,174,10,17,57,45,9,201,7,39,7,22,181,35,28,60,25,88,14,7,9,9,28,55,59,7,17,7,49,7,7,24,7,27,38,7,43,20,95,79,102,7,50,31,15,29,142,18,268,19,73,31,16,108,11,16,9,49,16,118,51,64,132,112,40,6,16,58,13,34,208,27,19,47,34,9,7,77,122,90,7,7,6,53,10,16,48,179,19,7,9,105,149,36,21,122,174,7,86,14,34,88,509,505,370,450,480,428,357,447,501,376,467,500,384,350,461,514,430,449,403,326,452,346,560,516,477,425,503,507,556,445,462,450,358,475,339,431,308,502,375,451,426,520,440,486,411,459,529,380,439,362,523,539,466,540,458,386,473,438,398,371,389,564,514,285,458,493,457,360,397,483,464,329,504,480,539,457,444,335,538,310,336,338,515,381,297,499,291,284,317,368,530,295,556,491,536,424,199,284,231,240,84,524,22,111,97,37,444,39,297,221,30,499,21,19,514,201,56,9,25,41,458,52,219,146,90,381,69,406,457,331,97,71,251,239,491,107,50,109,98,45,55,502,66,285,420,90,22,31,66,112,77,169,55,37,126,8,483,212,136,62,111,185,493,78,103,406,131,102,216,53,128,47,138,92,93,530,55,126,78,186,298,50,538,120,39,165,317,55,531,75,205,295,217,416,154,501,24,33,77,160,521,468,391,367,377,515,458,378,441,469,389,422,367,459,370,326,504,404,392,427,521,366,361,392,276,418,324,233,305,248,309,514,138,134,349,181,82,442,524,103,228,100,168,333,77,510,442,354,496,369,511,383,363,375,363,475,374,499,436,507,423,539,514,197,253,344,508,496,389,404,499,458,479,494,546,460,523,299,518,384,550,467,184,435,474,383,367,369,484,471,516,258,496,326,394,400,369,384,531,298,392,33,44,82,8,103,276,134,135,138,418,305,37,127,77,18,102,248,47,67,206,285,41,181,324,38,228,233,100,119,44,132,138,309,184,105,46,197,461,382,499,32,155,349,41,168,47,84,35,123,163,514,524,333,120,312,74,160,130,35,511,79,150,85,202,153,75,16,92,118,85,197,170,13,48,379,62,273,81,417,41,72,159,67,235,139,44,262,390,31,321,68,204,247,186,199,327,40,144,434,376,436,526,535,458,386,462,405,499,321,361,341,424,435,477,424,319,495,381,193,228,166,397,352,309,335,308,323,500,134,111,98,254,465,389,163,89,527,516,554,314,450,471,495,353,467,412,140,420,393,485,463,310,356,553,380,357,321,519,347,292,353,248,478,404,518,522,502,527,371,385,492,539,283,521,391,361,433,332,485,490,518,567,370,388,544,499,271,464,331,390,517,341,386,392,479,357,515,429,474,287,549,429,528,397,273,425,400,375,488,365,279,459,410,569,541,491,401,427,411,387,507,501,307,428,399,498,428,465,358,445,511,491,410,446,397,462,346,382,450,478,343,328,543,445,316,419,307,484,456,266,288,380,291,351,298,304,395,348,210,300,257,477,282,297,373,264,504,393,295,426,365,286,365,314,324,224,286,373,498,327,475,238,234,438,444,533,499,362,551,473,319,276,246,307,509,408,283,377,443,13,67,37,166,42,98,381,30,224,97,76,41,49,98,228,7,193,75,53,495,86,228,51,28,321,319,111,24,89,192,134,54,53,465,37,7,309,60,40,106,50,335,45,352,73,500,156,397,140,64,51,235,18,43,7,91,361,308,7,41,22,254,163,308,94,323,49,62,289,93,74,165,30,95,35,23,124,95,148,202,167,50,341,58,194,79,149,111,192,160,153,57,34,16,56,82,203,153,109,17,148,18,58,6,59,82,6,76,161,228,101,146,266,154,94,8,144,126,186,75,554,103,60,80,365,516,185,287,72,79,101,8,8,89,155,65,211,7,118,195,134,90,166,81,42,132,261,517,9,48,104,71,60,48,50,87,99,53,19,518,195,9,195,132,62,55,73,213,116,554,212,30,76,84,44,92,13,166,327,79,102,157,74,106,116,128,127,59,104,169,58,90,229,39,204,55,71,154,207,78,444,431,359,420,537,389,447,523,556,503,378,394,342,500,434,140,185,194,143,444,260,361,166,385,305,222,220,101,153,126,147,100,132,75,71,86,242,174,140,66,147,198,67,98,426,159,56,347,508,392,528,434,385,322,470,65,546,543,289,449,292,566,377,454,363,983,378,388,300,497,483,534,498,356,516,513,384,380,278,163,324,474,112,398,335,509,286,490,389,360,345,336,365,265,240,312,317,484,496,451,60,98,16,9,140,500,75,147,185,16,45,147,101,71,60,26,143,100,260,8,194,33,20,9,16,434,166,153,222,82,49,40,21,10,35,488,188,65,7,16,7,67,17,30,132,6,7,38,61,166,444,43,66,126,33,7,86,56,60,219,34,28,60,26,54,49,64,7,381,37,16,63,14,59,61,30,72,140,39,177,37,19,41,159,51,24,41,69,61,7,124,13,37,385,47,198,163,49,162,120,396,343,450,507,302,197,92,399,132,77,369,217,128,124,77,125,104,108,119,102,95,54,49,71,124,92,73,60,120,46,43,112,195,231,167,105,32,195,87,53,32,155,140,390,182,367,51,527,22,544,148,380,140,598,394,156,560,420,491,36,37,518,480,416,503,416,99,66,553,116,398,362,364,477,463,423,346,483,351,543,458,400,314,378,349,374,444,315,428,540,312,474,473,385,514,499,472,427,405,509,41,13,15,132,18,7,77,92,13,46,23,29,15,135,11,14,7,26,9,8,8,13,16,13,14,45,20,105,99,77,7,56,16,45,13,66,197,9,32,35,48,9,106,112,36,31,16,62,14,13,102,26,22,43,7,54,42,31,128,35,34,7,41,77,95,399,16,65,140,28,53,7,20,40,58,13,48,49,31,13,31,22,13,132,18,33,15,7,17,16,7,9,22,25,45,124,14,21,13,9,502,398,500,392,463,340,535,504,559,522,443,439,375,525,794,479,517,542,463,522,433,387,383,335,462,531,365,310,338,345,412,295,426,333,531,500,393,524,317,302,335,457,486,410,507,506,360,227,312,491,248,370,523,463,287,457,422,494,280,281,488,445,341,519,444,433,327,293,324,320,476,275,474,474,301,348,160,520,544,225,414,438,231,323,509,513,344,527,340,574,306,293,317,239,508,509,368,511,228,188,330,62,95,227,335,57,324,199,76,160,11,462,110,124,225,268,19,295,266,124,22,407,52,243,90,103,489,345,170,436,184,77,214,149,166,59,31,311,19,383,476,33,280,166,310,426,511,188,239,112,69,94,80,109,180,217,293,540,449,112,144,185,166,228,33,19,43,128,25,79,137,42,173,30,217,159,126,134,279,473,193,167,41,231,28,242,130,387,67,141,270,40,11,560,50,158,15,279,88,264,485,403,479,444,410,512,506,388,360,370,525,373,357,494,410,494,362,336,362,318,481,490,458,333,285,297,239,574,167,141,212,355,512,118,395,243,256,170,178,126,278,541,410,155,531,510,460,534,314,301,341,427,496,214,180,139,209,214,156,54,492,251,214,176,226,463,414,88,501,430,405,336,292,331,533,443,459,451,528,502,314,285,518,566,457,317,413,582,340,491,289,122,315,246,496,332,239,337,431,528,12,20,54,158,141,161,126,362,239,336,333,301,243,118,492,214,256,17,209,156,170,167,297,463,203,39,315,315,57,24,574,26,318,122,285,285,212,60,119,458,490,481,178,141,155,72,355,180,57,134,139,278,214,226,93,85,21,541,48,90,510,84,132,163,314,206,195,239,176,88,45,534,301,50,30,531,86,246,341,95,200,512,184,77,496,51,251,35,73,214,166,100,243,410,52,149,32,177,460,21,539,406,447,338,353,403,451,413,404,227,238,232,215,506,495,510,412,366,145,557,234,224,107,252,432,141,313,208,159,447,126,55,555,82,131,217,86,132,247,319,179,378,188,213,86,124,185,374,517,338,514,533,512,526,438,510,382,317,358,540,378,514,360,465,412,122,311,491,397,456,343,543,391,533,436,349,387,24,53,524,133,276,442,497,506,457,503,445,533,349,461,499,349,371,346,401,304,444,322,451,393,358,454,299,252,333,274,327,389,337,282,433,425,475,549,439,326,349,387,398,318,241,463,536,483,334,463,323,276,341,511,363,555,564,527,418,241,327,329,409,298,384,355,306,315,276,416,270,301,370,359,556,260,421,305,288,335,268,257,412,593,456,390,490,501,298,269,308,256,291,437,425,493,376,396,511,318,465,307,507,516,296,283,303,316,321,357,286,450,309,242,461,296,272,217,521,311,328,286,387,250,55,232,227,126,510,135,159,24,82,53,17,177,122,81,145,234,131,132,63,56,11,224,49,76,404,217,107,14,65,108,53,57,187,86,413,208,506,17,59,107,215,238,23,432,86,137,141,43,27,366,412,86,54,689,22,94,188,134,366,29,124,109,495,12,555,382,179,133,23,118,247,52,196,555,16,122,41,17,18,128,77,57,132,105,67,120,313,447,319,76,106,213,135,252,378,51,105,71,59,185,311,40,23,92,557,211,111,58,143,56,69,175,92,22,272,255,196,164,128,369,152,138,49,167,30,195,50,118,76,107,176,92,103,118,39,236,284,145,123,118,90,178,214,52,107,51,152,142,24,91,144,82,52,140,38,48,168,200,229,28,71,89,64,87,269,66,366,281,197,431,49,374,164,143,162,44,74,56,239,225,131,139,204,238,115,29,13,44,7,314,271,109,93,78,232,168,178,141,23,369,499,416,234,372,449,464,197,305,201,559,179,297,231,171,183,148,198,186,180,229,106,218,135,186,169,159,220,199,161,223,83,72,92,110,124,106,56,156,200,142,144,184,118,116,175,82,497,450,466,310,507,515,505,521,305,172,156,502,413,381,481,356,389,350,57,561,541,524,522,428,432,254,364,523,576,490,503,534,284,141,317,519,377,368,506,269,342,396,365,506,535,277,390,341,332,556,316,540,329,175,559,143,20,149,305,106,20,416,18,108,234,66,56,73,183,372,92,122,110,83,89,62,98,99,171,72,449,20,197,21,54,67,159,201,79,26,297,32,198,139,148,82,57,464,7,78,179,58,24,135,223,20,41,89,68,186,87,33,7,44,30,52,7,26,106,186,7,124,180,231,9,7,68,161,138,58,33,52,22,66,7,84,184,62,63,142,101,172,44,128,58,254,171,199,60,118,220,88,46,505,281,314,345,270,283,253,286,371,349,147,114,138,311,184,136,160,111,141,209,164,187,117,106,300,217,262,52,133,143,124,249,263,95,158,251,287,209,154,109,111,75,72,91,88,113,78,102,125,71,68,123,86,112,100,123,55,91,173,111,183,359,452,158,56,112,75,117,116,62,152,68,77,93,399,46,552,438,305,60,38,103,367,135,363,341,34,66,33,58,35,114,450,51,443,311,394,391,432,365,147,18,25,58,443,9,33,9,16,15,28,16,184,15,16,7,51,7,314,35,22,20,82,38,66,17,114,40,38,114,41,281,21,136,17,15,8,70,253,75,225,16,16,79,38,35,31,28,16,7,33,111,8,30,23,32,19,58,75,117,164,14,52,9,22,14,136,62,61,72,48,177,18,47,30,117,16,58,17,7,62,92,26,187,6,14,48,41,23,8,29,9,30,138,23,9,262,34,9,23,526,568,345,431,336,494,242,315,531,298,305,288,305,257,335,508,418,402,443,440,370,456,409,178,324,250,444,342,178,223,198,195,216,214,210,208,177,239,189,237,235,444,339,174,233,176,248,278,221,541,162,183,536,350,311,274,260,312,215,479,167,181,312,204,175,216,200,205,263,193,224,227,243,163,155,201,293,247,310,287,425,541,366,504,540,390,173,165,276,117,277,154,165,153,237,164,124,355,202,142,27,9,111,24,208,16,100,187,160,70,57,26,319,86,230,126,184,85,12,69,94,90,42,77,8,138,59,75,54,59,365,177,75,131,37,69,114,339,44,17,308,84,363,176,58,124,119,257,242,69,19,271,526,144,49,149,241,119,237,75,97,202,276,191,7,15,112,298,74,223,136,117,125,21,198,125,162,113,196,135,178,124,226,75,149,236,295,200,216,108,210,24,32,56,48,21,444,105,317,133,418,472,494,316,323,246,437,307,185,390,226,490,265,340,206,183,216,203,169,258,141,399,318,297,164,98,328,105,383,119,154,471,148,157,158,172,97,77,91,76,90,165,80,145,265,185,141,202,268,190,219,184,70,189,176,93,99,195,124,476,359,505,316,505,532,137,417,552,491,494,550,380,535,493,319,486,443,267,429,267,328,277,270,480,269,147,139,281,267,65,72,345,540,70,509,164,115,138,67,362,31,56,164,185,30,37,6,44,55,418,60,93,28,42,65,28,89,66,105,34,233,390,316,142,36,60,70,72,172,183,97,235,246,99,63,213,226,90,76,87,494,36,44,85,47,165,77,9,11,23,14,67,437,80,323,70,145,472,72,45,141,107,198,82,83,119,91,107,154,74,68,98,307,410,114,41,164,19,96,157,39,176,169,46,471,125,19,45,12,124,139,39,34,64,206,148,42,158,14,38,370,448,159,162,197,178,526,535,341,242,198,268,172,162,101,226,162,131,136,79,94,251,163,126,194,260,196,133,174,101,162,173,127,182,212,144,132,113,116,310,141,492,40,486,45,272,303,325,472,243,171,429,226,272,102,307,398,68,204,92,278,122,44,253,138,206,9,444,48,319,112,347,42,196,39,37,240,508,193,72,46,222,77,67,195,212,194,190,36,81,306,271,169,282,511,404,290,463,203,62,226,227,163,183,576,269,262,168,157,182,31,51,226,177,271,158,219,297,362,255,320,183,182,224,237,267,207,249,233,149,246,220,304,273,311,601,128,191,304,155,224,212,220,153,236,232,147,222,115,287,123,181,334,354,517,132,226,129,419,188,146,337,271,235,191,238,176,236,247,160,172,317,161,203,179,92,518,219,200,261,203,197,137,122,164,182,117,275,228,273,128,43,169,218,133,157,154,209,23,222,72,92,162,159,43,136,45,9,94,79,9,17,57,35,44,101,73,163,74,14,174,62,172,9,162,7,212,37,39,242,178,198,18,12,196,370,81,51,23,126,182,36,197,18,42,11,7,113,101,195,39,26,40,29,7,75,60,131,47,226,102,268,162,14,127,448,68,7,12,11,123,10,404,48,310,122,492,49,162,132,28,171,138,341,28,45,112,98,77,101,92,161,58,43,35,72,193,141,38,29,24,89,58,46,29,173,163,67,31,56,251,44,67,18,34,56,119,12,17,23,29,7,6,526,133,260,48,78,27,59,535,47,26,50,37,18,196,43,194,18,6,18,15,144,7,34,116,21,26,183,212,271,25,72,133,9,42,14,40,71,20,16,9,16,168,181,157,20,21,7,56,72,9,46,125,235,117,60,38,38,147,132,444,16,15,25,36,63,145,10,68,45,122,5,17,52,42,61,87,93,461,494,206,326,216,275,297,236,100,179,213,196,215,220,78,153,501,171,65,73,107,254,72,166,117,383,241,348,265,64,73,211,90,293,69,414,256,52,88,411,87,91,84,83,526,199,111,74,425,457,170,92,571,187,301,46,213,233,501,205,164,229,214,264,243,245,207,199,133,474,183,219,172,307,184,244,212,141,154,226,146,145,168,223,242,135,143,179,112,182,473,257,175,193,269,165,202,180,199,60,90,89,7,19,53,72,46,27,206,34,216,461,29,213,179,20,153,69,9,494,100,12,9,7,9,78,38,107,7,26,60,9,7,16,166,275,73,23,15,13,65,9,10,32,326,12,7,23,18,12,9,21,52,16,199,28,51,7,15,7,103,56,28,28,64,220,7,6,73,30,84,69,27,9,6,32,501,40,69,91,83,319,215,236,61,51,39,58,16,37,54,62,61,49,92,7,38,10,5,442,474,302,214,214,196,104,124,333,147,163,88,81,146,40,86,61,134,180,62,70,65,74,170,98,430,284,213,110,197,215,62,89,102,70,37,117,394,60,175,111,87,73,46,436,84,108,85,45,119,45,66,145,42,641,184,66,92,318,62,69,35,34,30,22,36,27,47,26,53,73,48,76,79,21,146,381,31,33,107,238,25,100,59,56,41,280,86,114,137,87,196,55,137,70,48,97,111,91,367,16,85,12,18,27,16,127,73,9,47,24,7,22,46,20,7,10,7,33,21,101,7,9,84,7,9,8,7,9,7,38,159,7,34,21,18,38,16,19,41,16,9,146,20,9,32,7,10,9,15,26,9,30,68,13,33,19,21,184,17,7,6,9,8,13,39,34,7,7,9,70,9,9,40,9,29,13,22,9,23,15,9,13,48,81,7,7,7,7,7,7,41,43,43,88,29,9,111,16,306,539,553,291,270,363,524,475,466,261,264,318,262,315,193,308,304,250,295,210,181,328,536,245,197,281,405,301,272,302,206,263,245,187,148,211,256,128,305,181,378,177,155,317,388,307,317,217,233,155,450,598,273,299,406,301,138,164,207,436,149,514,197,160,157,159,249,232,129,296,437,209,373,126,245,322,212,397,94,148,295,346,372,296,236,144,165,141,101,191,113,85,148,191,140,79,169,302,134,104,217,97,76,172,295,232,141,60,62,114,28,25,101,53,315,176,191,164,51,524,25,49,94,116,129,264,17,59,233,165,115,79,65,40,129,69,120,85,31,536,328,193,181,47,58,69,21,197,91,139,126,346,64,245,207,138,77,22,159,27,29,236,107,280,79,63,19,31,25,70,22,433,78,38,55,25,169,317,88,148,148,41,29,43,113,11,22,31,261,155,126,23,45,47,56,27,36,24,143,38,431,457,211,381,210,210,333,448,316,263,163,145,574,505,246,139,105,141,299,181,182,278,169,143,97,126,129,66,156,200,138,93,232,237,162,107,168,155,92,175,358,137,455,157,379,417,81,505,477,368,499,389,299,464,289,475,507,481,334,313,344,517,526,469,385,468,382,487,461,270,360,130,297,365,545,480,364,512,239,440,400,196,280,151,102,92,115,369,127,109,391,317,141,66,83,71,311,537,197,466,92,102,28,30,92,43,143,64,139,311,105,109,37,461,71,138,187,83,56,102,163,103,141,71,263,98,210,156,457,17,30,164,52,129,86,51,145,211,81,29,129,58,66,107,51,111,93,126,200,210,169,431,66,333,97,108,574,181,381,26,151,76,141,358,21,151,232,137,38,155,148,51,12,311,168,246,39,239,316,147,100,74,278,127,182,133,115,98,44,117,115,67,42,237,86,53,91,16,193,115,516,289,353,298,201,255,204,117,200,343,184,241,125,189,159,111,118,231,98,123,53,59,139,129,215,183,312,541,98,336,120,50,211,296,124,142,268,2115,365,500,142,182,447,123,455,422,473,91,254,99,162,473,481,565,372,507,549,360,501,314,332,524,449,199,258,342,256,282,246,356,469,388,354,269,74,483,280,200,382,372,203,162,85,394,116,207,530,324,273,487,339,312,394,214,317,386,443,405,278,462,115,513,132,50,107,107,55,64,217,329,410,257,267,350,126,227,96,98,497,419,487,187,230,256,264,462,252,343,329,284,110,193,264,289,379,301,276,284,215,122,253,251,214,336,352,310,198,136,167,223,162,323,202,319,99,275,230,303,176,160,247,357,170,220,75,224,274,240,118,311,351,366,248,265,219,174,236,236,475,293,250,220,268,255,292,418,339,243,202,194,185,199,276,135,355,158,278,211,194,306,125,117,200,353,99,343,184,118,98,99,241,41,101,51,53,201,59,129,91,54,107,55,159,264,55,123,63,50,204,9,231,123,64,119,111,21,289,246,107,50,98,516,95,189,336,132,115,255,139,85,42,121,298,162,153,541,36,17,312,74,48,183,7,116,142,196,15,7,96,356,42,124,29,142,42,93,74,110,215,78,122,75,115,126,38,117,120,296,88,64,51,136,182,75,61,211,98,36,83,203,227,168,162,60,36,268,52,36,94,162,144,178,23,206,38,487,79,223,37,94,116,95,152,13,26,86,56,117,118,113,54,98,112,121,329,173,286,160,129,97,81,134,24,59,96,121,77,29,167,117,394,30,259,14,184,447,207,53,30,127,145,204,93,158,73,25,44,118,225,122,92,21,13,97,90,230,123,27,145,32,95,118,124,150,39,198,241,68,257,185,7,59,360,7,116,133,175,16,29,36,416,350,557,430,507,287,258,225,184,168,133,120,307,268,166,106,123,107,109,107,300,280,104,44,127,96,46,99,90,97,94,121,211,337,266,223,69,414,253,548,97,244,283,66,354,324,443,455,266,512,332,513,532,409,352,317,363,271,491,340,409,325,296,210,272,483,328,330,282,302,386,428,370,253,298,449,232,515,245,449,356,304,466,391,421,126,362,217,477,174,435,270,319,341,108,181,218,249,97,248,46,7,65,29,44,32,104,225,123,66,46,557,127,22,268,244,430,96,65,97,166,14,109,6,97,16,31,106,150,78,121,507,79,107,7,19,94,40,16,168,69,73,258,99,90,67,184,120,44,21,143,42,107,67,38,74,17,287,31,60,59,16,307,184,88,133,168,34,97,18,337,173,42,498,12,80,37,56,21,39,59,62,83,47,41,58,12,86,99,138,19,9,38,7,47,62,90,280,61,7,520,373,483,290,391,262,260,204,171,197,168,642,133,263,130,392,144,135,180,177,167,517,267,144,147,104,87,550,108,171,181,259,136,97,109,154,104,102,90,110,314,86,297,385,108,76,68,57,62,175,91,115,76,79,84,57,89,66,83,178,104,93,70,74,95,83,139,94,62,77,102,84,99,61,52,64,81,49,64,97,50,69,206,182,301,200,202,160,170,126,107,67,42,37,52,49,76,75,133,65,43,88,111,26,16,16,68,23,43,16,61,7,9,7,24,49,22,31,11,49,38,15,16,104,42,22,13,20,9,9,9,7,34,47,14,41,31,15,15,52,16,27,7,16,16,260,16,18,13,7,28,57,9,32,27,20,17,13,45,27,33,16,7,23,27,34,16,196,53,58,17,16,24,13,26,62,16,16,30,147,8,191,45,31,57,22,16,385,16,29,97,16,16,13,7,13,23,108,14,7],"xaxis":"x","yaxis":"y","type":"histogram"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#f2f5fa"},"error_y":{"color":"#f2f5fa"},"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"baxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"line":{"color":"#283442"}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"marker":{"line":{"color":"#283442"}},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#506784"},"line":{"color":"rgb(17,17,17)"}},"header":{"fill":{"color":"#2a3f5f"},"line":{"color":"rgb(17,17,17)"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#f2f5fa","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#f2f5fa"},"geo":{"bgcolor":"rgb(17,17,17)","lakecolor":"rgb(17,17,17)","landcolor":"rgb(17,17,17)","showlakes":true,"showland":true,"subunitcolor":"#506784"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"dark"},"paper_bgcolor":"rgb(17,17,17)","plot_bgcolor":"rgb(17,17,17)","polar":{"angularaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","radialaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"yaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"zaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"}},"shapedefaults":{"line":{"color":"#f2f5fa"}},"sliderdefaults":{"bgcolor":"#C8D4E3","bordercolor":"rgb(17,17,17)","borderwidth":1,"tickwidth":0},"ternary":{"aaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"baxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","caxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"title":{"x":0.05},"updatemenudefaults":{"bgcolor":"#506784","borderwidth":0},"xaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"value"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"count"}},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"margin":{"t":60},"barmode":"relative","title":{"text":"Distribution of token amount"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('ef6ec8ae-66ff-43b2-9bf6-d0f06fb74d94');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>


</div>
</div>
<div class="cell" data-execution_count="210">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'sentimento'</span>] <span class="op">=</span> df.sentimento.<span class="bu">map</span>(<span class="bu">dict</span>({</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'negative'</span>: <span class="dv">0</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'neutral'</span>: <span class="dv">1</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'positive'</span>: <span class="dv">2</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>}))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="getting-the-bert-tokens-for-the-whole-dataset" class="level1">
<h1>Getting the BERT tokens for the whole dataset</h1>
<div class="cell" data-outputid="570d5cf6-bd32-453a-e03b-f2a71e8022d3" data-execution_count="211">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">'neuralmind/bert-base-portuguese-cased'</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>tt <span class="op">=</span> <span class="st">'Olá, esse é um teste! &lt;3'</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>test_tokens <span class="op">=</span> tokenizer.encode_plus(</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    tt,</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    max_length<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    add_special_tokens<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    return_attention_mask<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    return_token_type_ids<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    padding<span class="op">=</span><span class="st">'max_length'</span>,</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    return_tensors<span class="op">=</span><span class="st">'pt'</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'INPUT IDS'</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_tokens[<span class="st">'input_ids'</span>])</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">ATTENTION MASK'</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_tokens[<span class="st">'attention_mask'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>INPUT IDS
tensor([[  101,  1651, 22303,   117,  1966,   253,   222,  3515,   106,   133,
           511,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0]])

ATTENTION MASK
tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="212">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_tokens(text):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  encodes <span class="op">=</span> tokenizer.encode_plus(</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>      text,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>      max_length<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>      add_special_tokens<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>      return_attention_mask<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>      return_token_type_ids<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>      truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>      padding<span class="op">=</span><span class="st">'max_length'</span>,</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>      return_tensors<span class="op">=</span><span class="st">'pt'</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="bu">dict</span>({</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>      <span class="st">'input_ids'</span>: encodes[<span class="st">'input_ids'</span>].squeeze(),</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>      <span class="st">'attention_mask'</span>: encodes[<span class="st">'attention_mask'</span>].squeeze()</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'input_ids'</span>], df[<span class="st">'attention_mask'</span>] <span class="op">=</span> df.texto_refinado.<span class="bu">apply</span>(<span class="kw">lambda</span> x: get_tokens(x)[<span class="st">'input_ids'</span>]), df.texto_refinado.<span class="bu">apply</span>(<span class="kw">lambda</span> y: get_tokens(y)[<span class="st">'attention_mask'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="e4a5bd0e-0df3-41d2-bb1b-f8403c6e2775" data-execution_count="213">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>sizes <span class="op">=</span> <span class="bu">list</span>([])</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> df.input_ids:</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  sizes.append(i.size()[<span class="dv">0</span>])</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.array(sizes).<span class="bu">max</span>(), np.array(sizes).<span class="bu">min</span>(), np.array(sizes).mean())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>512 512 512.0</code></pre>
</div>
</div>
<div class="cell" data-outputid="5b4b5e94-cc57-45bd-e13e-e832c1d9cea2" data-execution_count="214">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>test_tokens[<span class="st">'input_ids'</span>].squeeze().size()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="214">
<pre><code>torch.Size([32])</code></pre>
</div>
</div>
</section>
<section id="creating-the-pytorch-dataset-class" class="level1">
<h1>Creating the Pytorch dataset class</h1>
<div class="cell" data-execution_count="215">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomDF(Dataset):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, text, targets, tokenizer, max_len):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.text <span class="op">=</span> text</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.targets <span class="op">=</span> targets</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.max_len <span class="op">=</span> max_len</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.text)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, item):</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> <span class="bu">str</span>(<span class="va">self</span>.text[item])</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    target <span class="op">=</span> <span class="va">self</span>.targets[item]</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    encoding <span class="op">=</span> <span class="va">self</span>.tokenizer.encode_plus(</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        text,</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        add_special_tokens<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span><span class="va">self</span>.max_len,</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        return_token_type_ids<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">'max_length'</span>,</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        return_attention_mask<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        return_tensors<span class="op">=</span><span class="st">'pt'</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">dict</span>({</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        <span class="st">'text'</span>: text,</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        <span class="st">'input_ids'</span>: encoding[<span class="st">'input_ids'</span>].flatten(),</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>        <span class="st">'attention_mask'</span>: encoding[<span class="st">'attention_mask'</span>].flatten(),</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>        <span class="st">'targets'</span>: torch.tensor(target, dtype<span class="op">=</span>torch.int64)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>    })</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="splitting-the-data" class="level1">
<h1>Splitting the data</h1>
<div class="cell" data-outputid="9a103f28-ec4c-4aa9-8952-550b6419c444" data-execution_count="216">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>SEED <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>train_df, test_df <span class="op">=</span> train_test_split(df, test_size<span class="op">=</span><span class="fl">.2</span>, random_state<span class="op">=</span>SEED)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>val_df, test_df <span class="op">=</span> train_test_split(test_df, test_size<span class="op">=</span><span class="fl">.5</span>, random_state<span class="op">=</span>SEED)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Amount of entries for train: </span><span class="sc">{</span>train_df<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Amount of entries for test: </span><span class="sc">{</span>test_df<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Amount of entries for validation: </span><span class="sc">{</span>val_df<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Amount of entries for train: 8636
Amount of entries for test: 1080
Amount of entries for validation: 1080</code></pre>
</div>
</div>
</section>
<section id="creating-a-function-to-create-dataloaders" class="level1">
<h1>Creating a function to create dataloaders</h1>
<div class="cell" data-execution_count="217">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_data_loader(df, tokenizer, max_len, batch_size):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  dataset <span class="op">=</span> CustomDF(</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>      text<span class="op">=</span>df.texto_refinado.to_numpy(),</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>      targets<span class="op">=</span>df.sentimento.to_numpy(dtype<span class="op">=</span><span class="bu">int</span>),</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>      tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>      max_len<span class="op">=</span>max_len</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> DataLoader(</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>      dataset<span class="op">=</span>dataset,</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>      batch_size<span class="op">=</span>batch_size,</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>      num_workers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>      drop_last<span class="op">=</span><span class="va">True</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>MAX_LEN <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>train_data_loader <span class="op">=</span> create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>test_data_loader <span class="op">=</span> create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>val_data_loader <span class="op">=</span> create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="14111521-2c11-4d85-a07e-10f283876b7a" data-execution_count="218">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_data_loader))</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>train_data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="218">
<pre><code>{'text': ['a nova atualização pile_of_poo ficou horrível navegação app péssima não contar empresas põem valor mínimo compra poder assim fazer venda casada obrigando cliente comprar precisa faturar valores entrega muitas vezes alto próprio pedido onde viu paga 30 00 entrega ??? loucura',
  'péssimo todos sentidos gostaria conhecer gênio inventou fechar refrigetante papel poi entrega nao entorne aí pede pedido feito restaurante 5km distância tendo lado ). resumindo sempre chega frio molhado já app permite cancelar pedido atendimento demora 3h poderia escrever ma limite caracteres permite',
  'a navegação site lenta aplicativo miui traz experiência conflitante muito demorado carrega fazer qualquer coisa fazer primeiro pedido beira frustração difícil novo usuário entender acontecendo taxas cadastrar coisas básicas endereço número telefone tanta lentidão dúvida chega questionar pedido realmente feito fora estética excelente layout só comprei desconto',
  'pop',
  'app legal porém muitos restaurantes gostaria pedir dizem estarem disponíveis própria cidade alguns pedidos limite forma pagamento cash cartão ), quero pagar achar melhor e lugares vendem coisa entregam outra ma culpam uber erro e preços alto promoções ilusórias app legal sim quando problema',
  'infelizmente tido problemas todos pedidos recentes fiz já dei nova chance restaurantes pedindo outros apps pedidos vieram direitinho não sei forma passado pedido restaurante confusa porém evitado selecionar itens adicionais promoções específicas combo diferentes quanto valor entregue errado sempre ressarcida demora fica dor cabeça ter adaptar cardápio pedido recebi',
  'aplicativo bugado',
  'bom ma falta un cupons',
  'foi bom tempo mudanças usei',
  'falta melhorar opções compartilhar pedido opções selecionar pedidos anteriores ter ir restaurante escolhar tudo denovo aplicativo falha muita vezes dando erro aplicativo informação entrega 3 estrelas porque ... fim contas comida chega come beleza ma perguntar bom ... pra bom falta ainda entrega mínimo mínimo bem fraco',
  'não nd dizer pessoas nn sabe usar app ma fácil',
  'o app deixa pagar hora entrega ....',
  'eu gosto app shopping ventura deram batata salgada socorro puro sal ksksk voltei loja fechados sei reclamar',
  'layout poderia prático mudar endereço trocar opções zera carrinho não sei time usuário olhou',
  'o app bom mas dei 4 estrelas poderem sempre melhorar',
  'o app simplesmente horrível consigo usar direito já faz tempo aparece opções restaurantes próximos página inicial preciso ir busca entrar cada categoria ver agora digito nome restaurante aparece contar cupons comprei aparecendo todos restaurantes usar',
  'excelente',
  'o app bom ma vou pegar cupons sempre aparece atingiu limite cupons ma atingi sugiro arrume bug',
  'excelente app tudo fácil rapido',
  'antigamente otimo app agora quer entra app ruim',
  'não achei opção contato restaurante precisei modificar pedido após fazê lo consegui',
  'está hora colocarem campos endereços tipo padrão casa outro alternativo assim facilita vida pessoas velhas têm dificuldades fora lixo fica histórico endereço',
  'gostava bastante opção pagamento online android época 10 cashback pago via picpay aí tiraram es 10 %, ma tudo bem ), ma infelizmente tiraram sei motivo procedimento ficou menos prático uma pena gostaria saber justificativa retirada opção pagamento online',
  'não gostei pago pix vcs estornam pagar crédito pra dar certo pedido amo esfihas habbs',
  'fiz compra supermercado entregador conseguiu passa la caixa poi algum problema sistema app tive cancelar pedido vou ter esperar 10 dia receber reembolso ainda vou ficar comida casa poi precisando des produtos urgentemente bem ainda dinheiro reserva imagina passando dificuldade ?! ficaria passando fome causa incompetência desse aplicativo péssimo não caiam nessa roubada',
  'precisa melhorar não opção pedir troco isso atrapalha clientes entregadores poderia ter aplicativo opção troco próprio restaurante mandar porque facilitava ter ir atrás destrocar dinheiro tomar tempo entregadores começando dinheiro ainda',
  'gosto serviço',
  'é bom ma pode melhorar o app bom boa promoções o pode melhorar ter opção classificar estabelecimento porque compramos ter noção local bom ruim',
  'frete caro preços conta além valor mínimo pedido ter aumentado outros aplicativos conta desinstalei app',
  'bom serviço simple utilizar prático',
  'aplicativo proposta boa ma comprar modalidade peça retire hora pagamento opções pagamento débito crédito simplesmente abrem toda outras formas pagamento abrem menos duas',
  'essa atualização ta bugando app nao acesso'],
 'input_ids': tensor([[  101,   123,   940,  ...,     0,     0,     0],
         [  101, 20938,  1211,  ...,     0,     0,     0],
         [  101,   123, 11300,  ...,     0,     0,     0],
         ...,
         [  101,  4062,  2576,  ...,     0,     0,     0],
         [  101, 16357,  4562,  ...,     0,     0,     0],
         [  101,  1921, 14429,  ...,     0,     0,     0]]),
 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
         [1, 1, 1,  ..., 0, 0, 0],
         [1, 1, 1,  ..., 0, 0, 0],
         ...,
         [1, 1, 1,  ..., 0, 0, 0],
         [1, 1, 1,  ..., 0, 0, 0],
         [1, 1, 1,  ..., 0, 0, 0]]),
 'targets': tensor([0, 0, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 0, 2, 0, 2, 0, 2, 1, 1, 0, 1, 1,
         0, 1, 2, 2, 0, 2, 1, 0])}</code></pre>
</div>
</div>
<div class="cell" data-outputid="6ef4ee8b-8632-423e-be04-d396b627aae1" data-execution_count="219">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_data[<span class="st">'input_ids'</span>].size())</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_data[<span class="st">'attention_mask'</span>].size())</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_data[<span class="st">'targets'</span>].size())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 128])
torch.Size([32, 128])
torch.Size([32])</code></pre>
</div>
</div>
</section>
<section id="modeling" class="level1">
<h1>Modeling</h1>
<div class="cell" data-execution_count="220">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SentimentClassifier(nn.Module):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_classes):</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>(SentimentClassifier, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.bert <span class="op">=</span> BertModel.from_pretrained(<span class="st">'neuralmind/bert-base-portuguese-cased'</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.drop <span class="op">=</span> nn.Dropout(p<span class="op">=</span><span class="fl">.3</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.out <span class="op">=</span> nn.Linear(<span class="va">self</span>.bert.config.hidden_size, n_classes)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, input_ids, attention_mask):</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    bmodel <span class="op">=</span> <span class="va">self</span>.bert(</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        input_ids<span class="op">=</span>input_ids,</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>        attention_mask<span class="op">=</span>attention_mask</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    attentions <span class="op">=</span> bmodel.attentions</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    cross_attentions <span class="op">=</span> bmodel.cross_attentions</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    hidden_states <span class="op">=</span> bmodel.hidden_states</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    last_hidden_state <span class="op">=</span> bmodel.last_hidden_state</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    past_key_values <span class="op">=</span> bmodel.past_key_values</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    pooler_output <span class="op">=</span> bmodel.pooler_output</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> <span class="va">self</span>.drop(pooler_output)</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.out(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="4a397f33-fcd5-4974-dc1c-55384a095787" data-execution_count="221">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.sentimento.unique())</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> <span class="bu">list</span>([<span class="st">'negative'</span>, <span class="st">'neutral'</span>, <span class="st">'positive'</span>])</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SentimentClassifier(<span class="bu">len</span>(class_names))</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'The device will be using </span><span class="sc">{</span><span class="bu">str</span><span class="sc">.</span>upper(device)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0 1 2]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>The device will be using CUDA</code></pre>
</div>
</div>
</section>
<section id="training" class="level1">
<h1>Training</h1>
<p>Reproducing the procedure from the original BERT paper using the AdamW optimizer by Hugging Face. It corrects weight decay. I’ll be also using a linear scheduler with no warmup steps.</p>
<div class="cell" data-execution_count="222">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    model.parameters(),</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">5e-5</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>total_steps <span class="op">=</span> <span class="bu">len</span>(train_data_loader) <span class="op">*</span> EPOCHS</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> get_linear_schedule_with_warmup(</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    optimizer,</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    num_warmup_steps<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    num_training_steps<span class="op">=</span>total_steps</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss().to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="bert-authors-recommendations-for-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="bert-authors-recommendations-for-fine-tuning">BERT authors recommendations for fine-tuning</h2>
<table class="table">
<thead>
<tr class="header">
<th>Hyperparameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Batch size</td>
<td><code>16</code>, <code>32</code></td>
</tr>
<tr class="even">
<td>Learning rate (Adam)</td>
<td><code>5e-5</code>, <code>3e-5</code>, <code>2e-5</code></td>
</tr>
<tr class="odd">
<td>Number of epochs</td>
<td><code>2</code>, <code>3</code>, <code>4</code></td>
</tr>
</tbody>
</table>
<p>I’ll ignore the number of epochs recommendation but stick with the rest.</p>
<blockquote class="blockquote">
<p>NOTE: Increasing the batch size reduces the training time significantly, but gives lower accuracy.</p>
</blockquote>
<div class="cell" data-execution_count="223">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>  model <span class="op">=</span> model.train()</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>  losses <span class="op">=</span> <span class="bu">list</span>([])</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>  correct_predictions <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> data <span class="kw">in</span> data_loader:</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> data[<span class="st">'input_ids'</span>].to(device)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    attention_mask <span class="op">=</span> data[<span class="st">'attention_mask'</span>].to(device)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> data[<span class="st">'targets'</span>].to(device)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        input_ids<span class="op">=</span>input_ids,</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>        attention_mask<span class="op">=</span>attention_mask</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>    _, pred <span class="op">=</span> torch.<span class="bu">max</span>(output, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(output, targets)</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>    correct_predictions <span class="op">+=</span> torch.<span class="bu">sum</span>(pred <span class="op">==</span> targets)</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    losses.append(loss.item())</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    nn.utils.clip_grad_norm_(model.parameters(), max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>    scheduler.step()</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> correct_predictions <span class="op">/</span> n_examples, np.mean(losses)</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_model(model, data_loader, loss_fn, device, n_examples):</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>  model <span class="op">=</span> model.<span class="bu">eval</span>()</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>  losses <span class="op">=</span> <span class="bu">list</span>([])</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>  correct_predictions <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>  <span class="cf">with</span> torch.no_grad():</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> data <span class="kw">in</span> data_loader:</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>      input_ids <span class="op">=</span> data[<span class="st">'input_ids'</span>].to(device)</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>      attention_mask <span class="op">=</span> data[<span class="st">'attention_mask'</span>].to(device)</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>      targets <span class="op">=</span> data[<span class="st">'targets'</span>].to(device)</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>      output <span class="op">=</span> model(</span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>          input_ids<span class="op">=</span>input_ids,</span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a>          attention_mask<span class="op">=</span>attention_mask</span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a>      )</span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a>      _, pred <span class="op">=</span> torch.<span class="bu">max</span>(output, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a>      loss <span class="op">=</span> loss_fn(output, targets)</span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a>      correct_predictions <span class="op">+=</span> torch.<span class="bu">sum</span>(pred <span class="op">==</span> targets)</span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a>      losses.append(loss.item())</span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> correct_predictions <span class="op">/</span> n_examples, np.mean(losses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="6dda6fab-8d14-4796-ef7a-5f6492430077">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>best_accuracy <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCHS):</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="ss">f'EPOCH </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>EPOCHS<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>    train_acc, train_loss <span class="op">=</span> train_epoch(</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        train_data_loader,</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        loss_fn,</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>        optimizer,</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>        device,</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        scheduler,</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">len</span>(train_df)</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="ss">f'TRAIN LOSS: </span><span class="sc">{</span>train_loss<span class="sc">}</span><span class="ss">  |  ACCURACY: </span><span class="sc">{</span>train_acc<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>    val_acc, val_loss <span class="op">=</span> eval_model(</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>        val_data_loader,</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>        loss_fn,</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>        device,</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">len</span>(val_df)</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="ss">f'VALIDATION LOSS: </span><span class="sc">{</span>val_loss<span class="sc">}</span><span class="ss">  |  ACCURACY: </span><span class="sc">{</span>val_acc<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">'</span>)</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'train_acc'</span>].append(train_acc)</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'train_loss'</span>].append(train_loss)</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'val_acc'</span>].append(val_acc)</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'val_loss'</span>].append(val_loss)</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val_acc <span class="op">&gt;</span> best_accuracy:</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>      best_accuracy <span class="op">=</span> val_acc</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>      torch.save(model.state_dict(), <span class="st">'/content/drive/MyDrive/Colab Notebooks/models/best_model_state.bin'</span>)</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>  logger.success(<span class="ss">f'MODEL TRAINED WITH </span><span class="sc">{</span>EPOCHS<span class="sc">}</span><span class="ss"> EPOCHS AND ACHIEVED </span><span class="sc">{</span>best_accuracy<span class="sc">}</span><span class="ss"> OF BEST ACCURACY 🎉'</span>)</span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a>  logger.error(<span class="ss">f'ERROR: </span><span class="sc">{</span><span class="bu">str</span><span class="sc">.</span>upper(<span class="bu">str</span>(e))<span class="sc">}</span><span class="ch">\n</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2023-05-10 01:03:09.343 | INFO     | __main__:&lt;module&gt;:6 - EPOCH 1/8
2023-05-10 01:05:54.132 | INFO     | __main__:&lt;module&gt;:18 - TRAIN LOSS: 0.8204962819482314  |  ACCURACY: 0.6082677245140076
2023-05-10 01:06:01.636 | INFO     | __main__:&lt;module&gt;:28 - VALIDATION LOSS: 0.792832124413866  |  ACCURACY: 0.6268518567085266


2023-05-10 01:06:03.247 | INFO     | __main__:&lt;module&gt;:6 - EPOCH 2/8
2023-05-10 01:08:55.383 | INFO     | __main__:&lt;module&gt;:18 - TRAIN LOSS: 0.604567931265636  |  ACCURACY: 0.7361046671867371
2023-05-10 01:09:03.050 | INFO     | __main__:&lt;module&gt;:28 - VALIDATION LOSS: 0.906921948447372  |  ACCURACY: 0.6222221851348877


2023-05-10 01:09:03.057 | INFO     | __main__:&lt;module&gt;:6 - EPOCH 3/8
2023-05-10 01:11:57.064 | INFO     | __main__:&lt;module&gt;:18 - TRAIN LOSS: 0.4040720313233514  |  ACCURACY: 0.8418248891830444
2023-05-10 01:12:04.752 | INFO     | __main__:&lt;module&gt;:28 - VALIDATION LOSS: 1.0174239657141946  |  ACCURACY: 0.6472222208976746


2023-05-10 01:12:06.610 | INFO     | __main__:&lt;module&gt;:6 - EPOCH 4/8
2023-05-10 01:15:00.639 | INFO     | __main__:&lt;module&gt;:18 - TRAIN LOSS: 0.282453167449586  |  ACCURACY: 0.8996062874794006
2023-05-10 01:15:08.266 | INFO     | __main__:&lt;module&gt;:28 - VALIDATION LOSS: 1.3024714841987148  |  ACCURACY: 0.6314814686775208


2023-05-10 01:15:08.268 | INFO     | __main__:&lt;module&gt;:6 - EPOCH 5/8
2023-05-10 01:18:02.294 | INFO     | __main__:&lt;module&gt;:18 - TRAIN LOSS: 0.20513046395075143  |  ACCURACY: 0.9279758930206299
2023-05-10 01:18:09.958 | INFO     | __main__:&lt;module&gt;:28 - VALIDATION LOSS: 1.3614619554895344  |  ACCURACY: 0.6379629373550415


2023-05-10 01:18:09.961 | INFO     | __main__:&lt;module&gt;:6 - EPOCH 6/8</code></pre>
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>