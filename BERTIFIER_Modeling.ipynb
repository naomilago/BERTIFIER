{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "Ja6RLW9QREHV",
        "outputId": "7e6aaa2b-e3de-4726-e0bd-2d82c5ee8a2f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    '\n",
              "    @import url('https://fonts.googleapis.com/css2?family=Comfortaa:wght@400;500;700&display=swap');\n",
              "\n",
              "    body {\n",
              "        display: flex;\n",
              "        justify-content: center;\n",
              "        align-items: center;\n",
              "        min-height: 100vh;\n",
              "        margin: 0;\n",
              "    }\n",
              "\n",
              "    .container {\n",
              "        display: flex;\n",
              "        justify-content: center;\n",
              "        align-items: center;\n",
              "        gap: 20px;\n",
              "        margin-top: 30px;\n",
              "    }\n",
              "\n",
              "    img {\n",
              "        max-width: 100%;\n",
              "        height: auto;\n",
              "    }\n",
              "\n",
              "    h1 {\n",
              "        margin: 0;\n",
              "        size: 20px;\n",
              "    }\n",
              "\n",
              "\n",
              "</style>\n",
              "\n",
              "<div class='container'>\n",
              "    <img src='https://gcdnb.pbrd.co/images/hgNPk95VoyK6.png?o=1', width=200>\n",
              "    <h1>Modeling</h1>\n",
              "</div>\n",
              "<br>\n",
              "<br>\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title &nbsp;\n",
        "\n",
        "from IPython.display import display_html\n",
        "\n",
        "style = ''''\n",
        "    @import url('https://fonts.googleapis.com/css2?family=Comfortaa:wght@400;500;700&display=swap');\n",
        "\n",
        "    body {\n",
        "        display: flex;\n",
        "        justify-content: center;\n",
        "        align-items: center;\n",
        "        min-height: 100vh;\n",
        "        margin: 0;\n",
        "    }\n",
        "\n",
        "    .container {\n",
        "        display: flex;\n",
        "        justify-content: center;\n",
        "        align-items: center;\n",
        "        gap: 20px;\n",
        "        margin-top: 30px;\n",
        "    }\n",
        "\n",
        "    img {\n",
        "        max-width: 100%;\n",
        "        height: auto;\n",
        "    }\n",
        "\n",
        "    h1 {\n",
        "        margin: 0;\n",
        "        size: 20px;\n",
        "    }\n",
        "\n",
        "'''\n",
        "\n",
        "display_html(f'''\n",
        "<style>\n",
        "    {style}\n",
        "</style>\n",
        "\n",
        "<div class='container'>\n",
        "    <img src='https://gcdnb.pbrd.co/images/hgNPk95VoyK6.png?o=1', width=200>\n",
        "    <h1>Modeling</h1>\n",
        "</div>\n",
        "<br>\n",
        "<br>\n",
        "''', raw=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YhehykAURvpK"
      },
      "source": [
        "# Setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "KJXT3XKzR3DA"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers demoji loguru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx_pYkDhRJ8H",
        "outputId": "61a60c92-4f6d-4eaf-8c7a-a0072bc02023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset imported successfully ðŸŽ‰\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "import plotly.express as px\n",
        "from sklearn import metrics\n",
        "from textwrap import wrap\n",
        "from loguru import logger\n",
        "from torch import cuda\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import numpy as np\n",
        "import string\n",
        "import demoji\n",
        "import torch\n",
        "import nltk\n",
        "\n",
        "from transformers import (\n",
        "    get_linear_schedule_with_warmup,\n",
        "    BertTokenizer,\n",
        "    BertConfig,\n",
        "    BertModel\n",
        ")\n",
        "\n",
        "from torch.utils.data import (\n",
        "    SequentialSampler,\n",
        "    RandomSampler,\n",
        "    DataLoader,\n",
        "    Dataset\n",
        ")\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        ")\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/reviews_preprocessed.csv')\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "if df.all != None:\n",
        "    print('Dataset imported successfully ðŸŽ‰')\n",
        "else:\n",
        "    print('Dataset import failed â˜¹')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lp6MpwWXYu5"
      },
      "source": [
        "# Quick EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShdiQ3NpYJI1",
        "outputId": "a5e125f7-2bca-4ddb-caec-52378dd3a724"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 206,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "OZbnbUMXXaN5"
      },
      "outputs": [],
      "source": [
        "stopwords = set(stopwords.words('portuguese'))\n",
        "punctuations = set(string.punctuation)\n",
        "\n",
        "tokens_frequencies = Counter([item for sublist in df['tokens'] for item in sublist.split() if item not in stopwords and item not in punctuations])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "LMhJMPfcYYOp",
        "outputId": "722c9be7-e42c-4939-e330-58535ef09fee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"1c30e6e4-219d-4985-9ee9-bdba5d26fe46\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1c30e6e4-219d-4985-9ee9-bdba5d26fe46\")) {                    Plotly.newPlot(                        \"1c30e6e4-219d-4985-9ee9-bdba5d26fe46\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Item=%{x}<br>Quantidade=%{marker.color}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":[4181,2826,2354,2018,1799,1434,1333,1209,1164,1131],\"coloraxis\":\"coloraxis\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"'app',\",\"'pedido',\",\"'ma',\",\"'aplicativo',\",\"'pra',\",\"'entrega',\",\"'bom',\",\"['o',\",\"'vezes',\",\"'op\\u00e7\\u00e3o',\"],\"xaxis\":\"x\",\"y\":[4181,2826,2354,2018,1799,1434,1333,1209,1164,1131],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#f2f5fa\"},\"error_y\":{\"color\":\"#f2f5fa\"},\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"baxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#506784\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"header\":{\"fill\":{\"color\":\"#2a3f5f\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#f2f5fa\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#f2f5fa\"},\"geo\":{\"bgcolor\":\"rgb(17,17,17)\",\"lakecolor\":\"rgb(17,17,17)\",\"landcolor\":\"rgb(17,17,17)\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#506784\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"dark\"},\"paper_bgcolor\":\"rgb(17,17,17)\",\"plot_bgcolor\":\"rgb(17,17,17)\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"radialaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"yaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"zaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"}},\"shapedefaults\":{\"line\":{\"color\":\"#f2f5fa\"}},\"sliderdefaults\":{\"bgcolor\":\"#C8D4E3\",\"bordercolor\":\"rgb(17,17,17)\",\"borderwidth\":1,\"tickwidth\":0},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"caxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"updatemenudefaults\":{\"bgcolor\":\"#506784\",\"borderwidth\":0},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Item\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Quantidade\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Quantidade\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"relative\",\"title\":{\"text\":\"Frequ\\u00eancia de itens comentados\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1c30e6e4-219d-4985-9ee9-bdba5d26fe46');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "aux = pd.DataFrame(tokens_frequencies.most_common(10), columns=['Item', 'Quantidade'])\n",
        "\n",
        "fig = px.bar(\n",
        "    aux,\n",
        "    x='Item',\n",
        "    y='Quantidade',\n",
        "    color='Quantidade'\n",
        ")\n",
        "\n",
        "fig.layout.template = 'plotly_dark'\n",
        "fig.layout.title = 'FrequÃªncia de itens comentados'\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "_UDHK8GAlQ5X",
        "outputId": "611e1d94-21da-4150-b4ad-7d69d44001ea"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"ef6ec8ae-66ff-43b2-9bf6-d0f06fb74d94\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ef6ec8ae-66ff-43b2-9bf6-d0f06fb74d94\")) {                    Plotly.newPlot(                        \"ef6ec8ae-66ff-43b2-9bf6-d0f06fb74d94\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"variable=0<br>value=%{x}<br>count=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"orange\",\"pattern\":{\"shape\":\"\"}},\"name\":\"0\",\"nbinsx\":100,\"offsetgroup\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[355,441,403,445,489,400,462,339,312,289,416,260,536,508,360,493,358,216,397,387,471,390,364,469,397,465,520,327,369,523,427,267,478,545,384,379,453,486,357,446,408,342,443,414,291,425,333,300,348,345,482,323,582,482,538,338,382,500,453,230,488,357,302,320,369,532,368,302,540,334,343,493,325,441,533,503,428,533,398,330,536,502,382,325,469,523,463,274,242,468,572,313,462,316,390,257,354,246,423,472,160,536,367,431,110,147,228,59,247,165,206,71,307,423,43,46,48,94,126,434,44,498,176,96,164,213,174,19,141,33,41,165,216,104,208,52,590,171,37,258,522,64,193,23,19,65,200,462,403,53,130,146,165,188,343,124,102,104,68,134,85,479,63,313,579,96,206,235,40,188,172,183,47,130,144,157,90,239,98,89,50,128,325,51,77,312,117,86,66,42,66,155,128,110,58,520,318,104,310,133,419,362,357,494,510,481,513,470,433,528,484,349,459,373,472,363,437,478,416,410,401,479,385,449,460,560,638,454,401,341,382,354,349,411,260,247,263,361,464,428,328,320,479,262,382,383,498,254,484,340,278,371,537,406,393,264,589,220,293,255,502,520,234,507,206,510,385,561,191,350,484,293,312,377,318,298,216,197,243,355,231,183,208,189,249,217,283,123,202,191,289,295,262,174,260,184,568,211,267,143,200,128,86,127,43,44,18,30,140,184,210,191,61,183,354,106,361,23,118,61,28,141,68,93,487,113,7,190,185,101,149,64,118,98,176,341,44,82,225,84,46,105,122,173,27,262,95,7,112,260,103,64,165,143,61,35,316,123,175,229,48,6,206,34,104,94,42,174,217,93,278,208,130,87,12,247,382,7,100,31,90,15,143,320,355,197,262,129,56,428,133,14,113,65,189,121,25,241,263,283,469,508,461,449,484,551,397,486,466,499,508,494,511,400,575,363,436,430,627,457,353,411,373,486,779,329,537,331,380,463,372,339,558,461,359,432,321,377,248,390,418,510,511,323,344,303,353,236,464,271,479,481,311,539,258,286,194,343,317,462,542,353,288,272,299,567,252,209,337,213,558,301,245,299,368,467,499,212,505,169,222,221,268,296,209,193,260,203,167,410,213,208,224,151,263,292,177,148,275,206,442,271,313,446,314,119,363,371,83,406,350,192,177,164,258,155,214,329,207,283,318,326,319,231,229,268,171,254,317,201,362,166,419,228,196,208,153,228,274,151,297,263,205,362,309,122,173,212,191,262,531,157,223,312,197,164,152,173,542,218,494,438,372,368,494,536,568,376,477,439,217,105,515,333,296,174,185,378,384,128,140,292,144,118,121,188,446,335,362,197,252,574,248,130,336,510,376,437,107,131,46,20,64,81,107,29,61,169,235,117,27,29,96,7,19,138,7,86,49,76,19,14,222,104,34,177,6,67,35,23,10,35,79,16,93,9,174,43,26,286,9,9,54,49,13,48,7,124,54,59,100,52,7,39,86,101,24,60,132,23,22,7,15,20,50,9,296,311,127,60,61,36,212,116,124,38,128,98,164,47,209,119,14,43,58,7,260,167,41,88,85,321,359,121,9,9,14,107,124,143,119,461,7,28,11,499,36,279,68,183,7,53,48,296,30,19,11,193,16,101,45,147,7,221,91,55,22,31,10,25,47,7,43,285,12,59,268,95,505,539,22,59,9,258,30,72,130,7,208,102,51,390,377,26,37,206,21,54,34,70,305,21,83,46,64,8,71,467,7,194,140,418,104,93,7,157,15,148,464,7,65,7,21,78,7,294,29,7,54,13,140,85,188,125,43,99,120,36,272,18,546,466,464,428,556,448,510,513,461,480,414,502,514,363,418,380,382,478,413,337,624,436,543,356,499,459,333,578,528,232,213,349,522,226,210,304,193,248,243,210,212,381,350,339,463,203,467,217,319,428,373,426,357,281,398,320,196,157,465,255,302,190,321,305,306,253,512,202,215,187,310,156,197,230,436,279,155,241,175,201,243,144,195,190,210,115,109,246,172,399,163,282,154,191,172,232,263,432,190,313,105,69,25,21,17,86,42,49,59,151,62,20,30,56,74,9,20,99,7,43,62,108,7,21,175,16,32,27,76,49,33,38,105,13,7,58,11,23,10,83,106,11,47,56,7,35,62,64,9,113,21,83,96,7,18,321,6,15,32,17,7,10,7,7,79,151,62,17,46,210,223,6,20,29,144,75,30,21,113,129,87,26,14,9,7,7,102,89,26,73,28,22,7,43,7,7,42,9,201,20,507,247,269,232,437,359,237,282,351,250,257,194,185,345,145,338,353,378,278,270,226,378,194,549,192,545,246,187,253,476,404,342,210,171,214,388,281,295,177,234,163,157,191,150,281,163,228,246,116,159,127,205,140,178,392,238,196,202,203,265,264,333,177,210,139,511,300,284,186,361,224,226,337,167,384,98,250,155,102,156,146,143,218,184,170,206,213,147,193,143,157,323,457,265,167,194,225,116,192,177,43,9,35,8,6,9,23,67,34,54,9,13,19,91,54,7,27,8,19,48,93,7,51,16,7,20,8,31,49,30,55,41,67,50,7,15,7,49,33,31,24,75,74,7,56,9,86,106,53,11,20,29,13,34,17,93,45,26,56,14,82,41,47,31,53,16,7,42,53,13,49,13,16,7,9,15,35,66,63,6,36,7,77,9,14,41,136,246,19,19,14,9,55,6,10,66,9,56,32,9,523,544,228,253,329,220,330,434,232,190,337,178,434,276,307,178,552,204,194,146,246,187,268,417,445,297,373,405,434,208,512,178,343,264,215,476,235,191,302,195,136,244,165,231,273,203,237,393,505,115,216,169,182,165,302,216,404,436,375,338,443,527,375,161,191,418,373,297,338,139,425,536,426,343,130,262,126,198,309,441,235,317,397,290,118,83,105,103,123,89,142,403,129,147,140,233,557,88,500,353,25,176,19,307,41,244,79,89,544,57,198,11,51,552,129,146,399,74,142,434,20,151,291,499,204,37,403,204,86,178,48,523,42,130,232,89,190,131,147,27,28,194,434,42,109,520,187,233,15,171,198,44,456,43,178,57,103,139,32,337,61,11,121,31,16,44,36,220,123,136,97,34,36,65,63,178,228,196,112,83,329,121,417,77,266,8,118,75,75,302,19,137,51,76,369,61,7,124,276,27,344,430,326,188,305,130,500,293,373,347,169,294,238,185,160,524,145,427,238,188,409,286,152,472,437,62,488,384,263,260,353,390,291,231,361,181,273,192,486,157,211,170,325,147,232,74,89,283,145,223,472,341,310,199,261,228,279,275,326,220,468,284,307,229,152,203,301,240,198,257,206,207,227,157,286,193,151,239,177,147,124,226,129,200,147,208,250,165,168,149,99,306,149,224,71,181,127,222,391,197,305,42,62,188,83,145,52,137,27,430,48,9,130,84,10,60,344,238,160,89,524,31,326,40,169,153,28,99,185,50,36,21,99,26,188,183,32,24,9,143,74,71,71,55,500,84,30,97,177,152,46,71,232,12,58,152,56,343,157,10,44,20,192,79,40,94,187,61,488,42,147,134,48,38,64,8,325,73,45,384,148,25,132,65,192,52,147,181,104,58,75,263,156,231,62,7,208,230,124,28,515,331,201,379,157,334,176,214,120,298,109,471,400,493,379,162,319,236,252,170,246,267,478,453,504,298,71,173,147,106,305,136,341,310,137,106,105,314,238,325,72,245,234,522,223,207,280,243,211,266,252,238,224,214,243,143,289,139,113,101,111,133,213,126,72,316,98,147,99,185,196,154,203,209,260,303,209,302,196,182,186,182,185,241,244,192,245,160,229,420,171,66,146,142,241,238,187,238,235,247,125,249,123,140,114,141,165,345,142,473,140,167,115,486,90,47,68,516,162,173,382,504,144,415,507,46,108,397,558,155,151,157,526,268,458,114,492,508,283,514,524,432,490,292,284,543,383,389,464,502,502,275,294,399,229,302,320,250,115,111,173,284,334,329,320,282,329,288,213,371,300,112,325,114,367,379,123,73,152,98,428,376,341,277,537,346,61,502,288,535,318,248,237,130,442,133,109,174,84,114,101,238,73,52,515,39,37,68,47,101,120,20,126,157,75,74,64,27,106,14,106,201,331,7,173,90,298,46,61,34,72,48,109,23,72,106,176,71,40,138,11,108,7,23,66,101,75,93,86,18,265,33,108,334,22,214,37,96,111,105,61,45,21,113,14,76,162,26,44,106,37,99,70,379,46,59,137,64,21,43,165,135,61,24,59,147,36,72,59,98,87,34,38,64,108,86,62,319,246,86,75,67,147,32,98,65,60,12,298,114,54,61,133,35,136,173,38,248,54,51,165,504,60,215,57,109,64,114,128,243,379,60,73,139,103,280,125,74,144,176,133,22,71,45,53,185,40,23,91,453,91,41,224,69,84,185,22,174,40,234,252,247,243,316,52,114,13,245,205,305,82,44,12,62,27,213,223,143,10,7,111,139,48,244,142,76,211,85,106,31,130,39,20,98,214,238,42,8,18,207,445,492,217,153,521,158,224,79,349,85,77,127,350,72,73,77,108,393,216,246,131,171,303,476,259,256,241,403,85,153,207,258,169,209,266,205,328,235,107,71,178,205,167,275,193,117,192,55,157,187,203,208,439,211,144,157,170,266,160,150,241,160,215,212,170,135,226,159,119,178,148,115,160,206,148,132,455,202,103,197,470,121,65,138,103,475,134,218,143,526,102,356,42,407,141,82,251,194,64,303,129,77,21,59,79,55,17,34,77,42,7,55,86,85,47,141,237,38,73,42,21,117,45,69,72,131,91,38,79,64,83,33,14,102,331,51,35,7,31,9,51,29,108,35,60,8,55,15,24,78,45,95,18,7,40,54,23,59,217,85,60,22,68,16,17,167,22,63,71,21,41,42,79,23,78,153,124,16,169,25,7,63,56,14,24,52,31,58,9,42,107,27,7,54,31,11,36,54,80,28,371,396,171,234,304,178,128,167,190,266,198,125,574,149,168,216,99,126,381,145,169,113,142,124,142,174,137,216,131,237,146,124,98,121,78,108,76,74,96,113,106,59,103,93,97,83,106,113,69,75,78,78,78,207,50,62,79,178,117,154,76,152,141,46,227,52,62,54,105,393,53,232,50,63,72,147,173,305,141,120,65,83,108,79,40,87,61,142,44,71,62,115,85,123,82,336,257,422,298,149,53,135,7,101,16,21,64,26,61,174,33,7,7,25,41,7,21,188,20,36,34,13,34,18,24,97,53,38,58,7,63,54,27,16,137,21,21,34,33,14,24,41,16,48,7,20,10,65,7,35,13,53,39,29,39,15,20,61,14,167,50,7,19,14,102,20,56,9,13,27,9,23,7,9,17,17,9,56,52,21,15,24,7,84,9,16,68,19,91,9,104,31,31,7,16,10,40,9,20,33,358,478,338,431,214,281,413,322,194,311,308,362,246,402,276,393,285,391,343,463,544,162,390,262,297,173,324,477,498,425,233,361,501,559,474,264,212,326,196,292,288,225,513,346,442,332,226,338,376,311,186,280,540,200,208,458,259,236,483,252,293,407,513,211,178,280,451,170,289,329,239,253,249,235,166,395,198,333,230,576,169,345,162,164,229,107,110,480,468,328,322,200,179,184,331,227,155,213,299,500,72,338,139,173,10,19,73,183,37,101,96,21,37,208,16,169,70,100,324,70,103,95,194,362,358,25,46,184,63,480,29,27,12,41,55,72,476,73,43,413,30,89,311,204,16,19,54,107,17,365,14,100,114,42,22,158,226,297,169,378,239,116,98,111,12,44,26,322,10,17,30,902,30,150,58,74,102,431,43,214,30,362,62,119,50,540,28,281,165,198,105,402,112,225,12,193,105,276,202,71,136,199,209,193,110,415,316,101,235,326,138,208,295,149,229,125,338,314,149,512,84,154,84,112,131,263,131,79,127,166,109,93,245,351,380,165,188,240,370,238,205,134,272,386,130,166,324,212,186,189,221,194,51,141,280,234,157,82,365,277,305,27,375,485,209,594,298,54,145,125,86,157,448,145,298,54,125,59,261,189,127,592,278,517,109,64,80,36,491,192,314,146,461,111,52,82,309,264,121,220,12,61,104,27,199,82,193,110,91,26,84,65,136,209,138,54,338,208,22,82,36,109,397,55,33,150,166,22,57,112,80,86,41,64,131,263,36,84,54,229,179,109,154,26,82,19,205,8,9,111,20,54,34,28,127,125,52,26,9,14,235,52,93,131,166,21,10,9,125,51,25,130,149,101,77,22,81,62,116,79,25,149,370,141,6,38,295,12,59,16,15,72,22,189,261,125,49,6,237,221,497,364,488,239,159,201,390,142,201,342,144,119,69,156,51,241,77,69,195,91,549,183,189,96,492,98,494,305,118,41,313,158,225,57,70,394,243,406,226,264,174,224,141,248,545,139,297,153,120,92,122,134,98,88,62,76,102,231,270,317,258,463,105,278,335,116,239,172,236,452,286,464,51,174,252,102,249,73,354,207,151,69,244,299,102,209,273,190,214,122,93,47,25,57,70,281,276,197,248,246,428,177,389,377,197,176,164,205,195,331,153,257,348,227,329,243,191,198,138,223,263,278,340,228,171,198,199,269,189,187,144,270,321,213,180,205,196,257,342,231,322,150,238,177,246,193,163,176,236,218,347,189,173,166,256,252,211,261,261,205,207,174,176,272,170,235,201,129,203,173,209,233,352,186,228,290,133,70,139,114,101,135,212,124,210,278,232,283,176,101,152,142,251,126,258,44,335,167,178,119,239,158,327,6,25,91,38,69,45,41,70,33,118,10,7,57,9,98,102,28,159,41,7,156,65,142,20,51,364,57,70,8,85,497,69,59,549,69,224,41,488,77,70,119,7,174,7,18,20,96,22,201,51,144,31,18,62,6,23,47,7,94,28,7,201,7,9,70,28,36,26,35,44,16,7,26,335,48,36,9,33,203,141,61,62,33,76,21,12,35,46,26,7,7,27,6,39,87,50,28,102,6,20,7,81,7,30,53,45,75,24,47,23,95,17,39,116,342,129,113,29,98,20,120,9,32,48,42,33,29,73,11,70,139,195,6,14,102,22,35,12,21,25,134,183,17,42,88,141,21,14,93,346,7,16,76,72,95,16,89,44,170,122,9,151,45,92,153,105,87,25,260,26,57,122,39,38,7,189,23,11,7,390,10,241,99,40,354,17,6,18,7,22,36,33,396,43,79,16,453,498,619,150,207,173,100,77,107,137,89,218,174,83,173,109,78,389,307,285,185,115,117,127,141,159,29,292,72,189,234,99,62,267,284,501,324,134,137,206,297,159,294,98,49,53,542,98,60,31,298,239,32,81,45,138,102,228,159,142,136,137,99,129,69,60,198,324,298,57,311,211,179,255,54,91,47,237,210,303,150,73,186,261,53,44,34,38,136,176,181,194,205,359,310,165,450,151,311,252,44,18,80,12,498,53,48,29,7,20,34,22,89,77,9,9,115,38,72,453,137,19,218,107,99,24,12,36,36,9,9,159,207,542,98,63,53,99,9,31,60,44,12,47,109,49,7,6,77,26,22,25,619,174,13,150,10,116,300,235,78,9,173,134,7,7,25,9,13,21,46,62,8,81,159,6,9,117,24,9,91,83,173,14,21,45,10,127,6,32,100,22,17,14,16,98,7,40,17,5,307,352,318,110,549,559,88,88,291,109,89,119,134,116,66,101,52,128,173,456,138,157,92,137,80,206,238,214,528,129,535,534,67,312,81,52,234,60,57,53,401,75,404,205,100,55,60,58,337,88,324,268,34,94,41,264,74,120,103,89,35,45,38,55,357,35,88,90,56,68,66,74,62,64,72,20,459,410,44,33,94,76,219,55,131,31,108,48,45,51,75,32,182,64,47,217,78,46,94,21,15,29,15,22,12,89,71,20,21,38,7,7,56,52,7,75,50,140,25,7,30,156,109,9,17,6,30,14,9,13,46,18,21,98,35,7,98,13,7,41,9,18,7,68,35,88,15,7,18,57,7,307,6,16,7,9,42,22,100,16,116,115,26,41,15,6,20,9,34,9,20,8,39,22,9,28,9,7,35,8,9,110,7,7,9,7,20,31,19,32,7,34,7,41,21,35,28,9,13,18,340,511,507,397,442,461,509,430,345,529,500,436,390,361,318,425,364,539,201,368,335,389,222,289,398,502,438,517,333,547,308,380,338,482,262,502,270,312,283,444,432,234,315,474,314,369,251,265,181,283,325,373,283,412,317,326,278,414,250,244,523,306,344,582,571,207,387,573,224,500,241,217,209,401,391,340,164,286,269,511,312,232,204,213,162,177,188,203,105,181,173,276,220,242,160,114,336,283,384,184,92,589,45,472,111,164,18,23,177,62,234,67,211,32,19,181,122,136,181,358,110,21,368,23,40,79,283,119,539,12,68,408,41,31,52,340,220,149,189,26,128,27,489,188,68,130,48,114,80,81,81,127,125,227,46,59,17,162,177,582,130,131,519,289,541,214,364,325,75,103,17,114,114,217,412,64,39,203,30,362,19,444,48,492,511,129,98,146,248,30,437,123,41,42,196,200,81,284,143,391,497,399,570,525,355,365,389,495,562,420,469,122,183,393,266,151,243,128,157,376,530,305,358,448,204,193,166,213,156,265,129,231,106,155,209,116,182,346,138,260,125,147,97,97,406,210,43,207,158,181,88,244,74,103,109,129,92,129,124,65,102,119,70,143,224,245,236,278,209,257,212,148,198,205,137,167,125,113,135,339,46,429,64,440,158,115,391,116,366,483,74,201,207,519,105,272,528,146,80,328,157,74,55,74,46,106,60,19,35,125,39,138,50,495,339,97,147,70,43,80,129,183,122,62,36,97,201,365,43,151,128,11,266,51,153,57,92,73,43,28,8,129,420,24,74,243,49,73,40,193,38,469,28,166,119,50,30,29,19,112,11,29,16,50,562,116,103,126,389,9,182,31,156,47,158,65,204,250,18,56,38,155,121,448,358,64,20,124,19,88,129,102,12,87,109,213,34,19,45,27,491,419,383,401,434,490,269,538,227,265,245,211,171,126,178,181,354,192,172,254,143,133,343,220,249,196,147,570,185,146,178,121,143,170,165,226,198,137,89,124,178,160,93,85,444,82,75,61,90,85,77,72,131,369,54,202,199,73,86,174,146,196,50,95,106,67,343,241,160,158,244,318,187,212,178,164,100,117,139,232,149,102,143,175,175,129,84,64,379,78,527,284,123,104,392,372,362,442,343,56,369,110,43,106,408,338,99,275,41,77,57,32,41,321,94,465,43,112,117,545,109,313,70,430,354,69,273,114,298,514,111,78,338,356,319,367,481,404,275,386,443,341,296,502,294,364,402,424,323,227,538,291,339,320,305,262,455,271,367,325,313,320,341,307,256,227,318,271,258,427,304,429,396,226,250,235,306,210,366,405,205,300,551,291,217,269,330,570,57,468,377,237,210,72,421,228,312,563,441,268,57,21,85,75,44,41,147,68,73,23,181,41,63,131,19,126,490,77,43,48,57,9,30,90,77,93,245,67,211,41,124,121,89,84,37,178,57,227,9,171,43,41,72,85,202,32,56,74,54,61,6,56,82,269,56,40,52,165,29,7,52,34,63,50,146,137,21,220,10,117,7,104,69,57,102,192,150,39,185,95,34,29,20,226,80,99,36,538,23,21,66,78,172,57,65,178,160,132,265,178,27,133,254,64,42,170,110,198,249,343,124,86,81,64,196,106,146,7,70,9,28,9,135,143,29,42,45,44,11,42,143,99,39,38,92,47,106,67,60,36,52,88,354,174,109,123,111,111,19,196,94,34,33,49,98,68,86,126,68,369,78,28,72,55,87,46,175,11,343,44,82,109,139,39,68,175,28,53,130,244,110,53,74,93,444,62,59,114,216,45,86,25,40,76,36,54,81,112,343,30,457,456,380,381,327,257,509,369,243,218,173,290,196,130,153,198,219,133,185,162,511,128,129,178,118,79,357,195,468,442,87,207,151,101,273,82,431,189,100,90,91,56,90,133,60,106,88,214,64,91,164,296,145,140,189,110,36,127,88,67,103,90,75,54,95,82,72,48,63,118,340,129,255,300,133,132,144,82,54,169,90,71,124,48,50,143,82,143,72,141,505,354,62,107,115,121,36,381,386,49,22,36,51,62,29,118,44,17,26,38,164,56,83,10,56,19,49,39,25,20,34,107,257,31,130,218,42,100,101,24,23,10,48,90,106,13,28,185,21,21,21,16,88,381,21,78,32,64,90,34,6,91,20,63,82,246,29,290,60,139,196,133,102,7,62,243,73,7,24,48,327,442,153,60,61,509,6,207,30,17,21,162,110,17,151,87,12,40,12,129,39,178,82,54,91,103,95,129,83,76,523,217,469,211,300,211,178,257,264,206,140,117,214,218,218,195,130,135,196,161,194,153,341,205,96,85,134,266,166,95,120,91,172,438,193,117,94,79,100,116,122,130,84,140,111,159,131,110,64,78,67,84,63,52,66,57,72,127,96,95,95,54,104,42,64,59,118,50,131,349,111,70,38,94,101,113,106,136,36,158,39,46,126,101,103,103,98,60,132,128,84,145,202,78,80,71,114,101,71,93,7,17,61,7,16,30,16,7,7,56,111,7,178,15,23,72,16,40,24,9,25,138,35,39,16,32,60,97,41,7,73,50,29,14,10,63,39,16,23,9,20,24,53,13,30,16,16,7,41,19,9,57,28,23,16,32,91,23,41,36,22,7,33,57,44,9,22,110,7,7,7,15,68,19,31,44,64,34,29,36,7,53,56,12,46,54,13,21,28,26,7,7,18,14,21,28,33,31,36,501,410,433,435,322,131,220,298,194,238,199,187,309,537,417,173,195,409,291,283,451,298,107,129,427,260,345,107,100,151,73,91,173,156,369,400,219,237,385,259,352,295,272,258,282,200,217,281,205,267,178,410,163,372,155,494,367,377,178,419,341,453,396,547,532,349,525,530,457,360,466,45,130,434,560,380,399,114,374,471,372,513,409,454,483,507,488,429,463,340,448,474,443,392,347,399,460,506,416,511,45,173,220,107,345,131,100,117,410,45,74,15,155,271,151,53,194,199,60,59,107,129,185,404,202,469,168,403,187,173,427,435,294,298,73,128,156,137,180,51,245,527,238,91,322,50,273,65,195,124,143,360,129,46,369,267,32,165,160,64,205,56,206,141,457,309,30,112,271,471,157,136,417,107,148,60,13,120,12,387,192,245,288,281,123,715,31,282,442,65,107,272,309,85,36,54,385,239,380,230,409,299,461,190,197,378,406,344,131,336,526,351,328,366,444,527,523,308,318,330,415,379,339,493,407,517,410,220,411,313,259,265,271,312,362,485,293,357,450,282,201,404,296,154,260,436,438,280,398,274,494,355,332,543,430,537,372,529,220,483,278,319,405,422,236,524,248,200,431,404,476,341,413,445,211,344,386,533,260,542,338,523,228,411,318,431,458,230,355,485,520,195,424,290,486,470,494,239,499,288,299,182,197,163,40,190,461,180,154,72,378,45,220,131,243,277,26,196,61,126,147,404,225,372,75,147,206,350,450,240,175,126,138,241,329,111,292,266,58,57,188,10,92,140,167,314,121,208,54,156,355,72,244,151,97,66,137,69,470,493,470,328,407,147,189,145,200,48,67,470,132,499,214,73,206,371,118,121,147,111,204,75,344,177,205,35,386,55,415,69,587,77,194,209,91,58,267,59,40,46,518,167,453,545,488,350,359,512,435,478,394,516,480,531,443,308,384,313,326,338,338,456,447,290,288,310,304,391,378,270,316,481,360,283,445,360,300,485,298,257,393,331,298,377,365,270,493,249,475,280,234,457,214,209,162,381,304,549,418,346,236,204,431,474,421,264,320,263,287,329,416,188,375,260,193,275,223,350,303,265,234,223,279,245,310,414,373,300,320,268,350,487,433,262,290,324,445,391,407,220,230,192,218,219,402,277,234,316,245,184,410,354,505,234,386,339,176,309,340,444,555,287,207,224,295,202,226,196,239,229,187,296,381,221,256,209,209,129,321,267,250,170,222,476,265,164,373,250,240,202,344,165,337,238,282,202,276,129,316,306,274,325,225,316,194,273,215,400,154,220,415,251,306,412,158,387,357,142,225,222,282,544,264,577,238,135,250,169,301,263,245,198,168,254,300,228,164,193,188,240,516,167,518,151,140,176,51,67,99,31,393,47,129,24,88,68,88,9,66,245,7,229,154,31,227,336,352,31,154,10,124,32,275,132,272,354,90,169,40,23,8,38,80,203,60,295,64,110,91,136,96,200,512,142,179,62,38,475,98,44,81,120,11,53,21,16,17,474,96,7,59,57,189,57,89,52,55,487,400,41,135,283,126,28,7,300,7,17,30,134,359,140,53,43,120,443,57,252,22,21,156,216,72,176,344,148,12,45,78,43,82,96,116,123,7,216,146,13,108,218,120,57,14,205,198,53,73,219,42,46,12,513,199,164,143,9,95,32,96,160,77,9,135,387,260,241,99,39,303,52,273,177,134,50,90,109,169,18,96,95,7,325,112,391,72,64,20,20,20,211,44,124,256,77,83,14,238,111,196,35,144,55,433,20,50,27,20,58,64,47,226,74,276,9,66,88,41,90,124,55,332,383,358,407,309,328,504,339,488,289,355,355,559,338,251,259,420,294,271,327,355,537,322,233,563,332,408,316,522,298,250,448,518,541,184,248,301,342,161,306,183,349,244,203,449,425,195,252,438,259,183,286,209,282,534,344,215,289,189,355,277,267,337,377,363,277,336,228,206,223,273,303,294,223,384,195,194,464,264,353,172,263,197,232,360,247,240,300,214,134,228,272,284,232,157,186,168,190,291,265,19,128,96,70,44,138,52,40,147,243,6,220,360,116,203,96,97,468,20,48,64,68,110,363,69,41,28,35,145,16,141,201,172,51,223,121,34,21,152,333,58,522,16,31,57,237,80,136,67,306,14,30,7,169,76,7,117,208,82,7,68,50,173,99,52,7,107,7,7,70,71,282,129,29,104,35,7,7,45,123,16,208,116,24,7,141,117,58,23,15,30,140,30,7,262,108,12,360,20,16,518,442,480,395,347,549,397,526,294,306,442,527,357,473,349,422,306,268,218,308,322,478,333,263,440,208,461,226,195,577,518,284,214,219,538,181,517,233,340,548,304,262,253,323,249,201,182,237,252,217,473,282,520,194,556,322,362,218,302,359,268,163,57,474,199,173,156,236,192,392,241,215,165,271,179,258,263,318,323,183,271,148,165,216,290,225,362,245,162,203,195,218,158,406,253,136,196,183,470,174,10,17,57,45,9,201,7,39,7,22,181,35,28,60,25,88,14,7,9,9,28,55,59,7,17,7,49,7,7,24,7,27,38,7,43,20,95,79,102,7,50,31,15,29,142,18,268,19,73,31,16,108,11,16,9,49,16,118,51,64,132,112,40,6,16,58,13,34,208,27,19,47,34,9,7,77,122,90,7,7,6,53,10,16,48,179,19,7,9,105,149,36,21,122,174,7,86,14,34,88,509,505,370,450,480,428,357,447,501,376,467,500,384,350,461,514,430,449,403,326,452,346,560,516,477,425,503,507,556,445,462,450,358,475,339,431,308,502,375,451,426,520,440,486,411,459,529,380,439,362,523,539,466,540,458,386,473,438,398,371,389,564,514,285,458,493,457,360,397,483,464,329,504,480,539,457,444,335,538,310,336,338,515,381,297,499,291,284,317,368,530,295,556,491,536,424,199,284,231,240,84,524,22,111,97,37,444,39,297,221,30,499,21,19,514,201,56,9,25,41,458,52,219,146,90,381,69,406,457,331,97,71,251,239,491,107,50,109,98,45,55,502,66,285,420,90,22,31,66,112,77,169,55,37,126,8,483,212,136,62,111,185,493,78,103,406,131,102,216,53,128,47,138,92,93,530,55,126,78,186,298,50,538,120,39,165,317,55,531,75,205,295,217,416,154,501,24,33,77,160,521,468,391,367,377,515,458,378,441,469,389,422,367,459,370,326,504,404,392,427,521,366,361,392,276,418,324,233,305,248,309,514,138,134,349,181,82,442,524,103,228,100,168,333,77,510,442,354,496,369,511,383,363,375,363,475,374,499,436,507,423,539,514,197,253,344,508,496,389,404,499,458,479,494,546,460,523,299,518,384,550,467,184,435,474,383,367,369,484,471,516,258,496,326,394,400,369,384,531,298,392,33,44,82,8,103,276,134,135,138,418,305,37,127,77,18,102,248,47,67,206,285,41,181,324,38,228,233,100,119,44,132,138,309,184,105,46,197,461,382,499,32,155,349,41,168,47,84,35,123,163,514,524,333,120,312,74,160,130,35,511,79,150,85,202,153,75,16,92,118,85,197,170,13,48,379,62,273,81,417,41,72,159,67,235,139,44,262,390,31,321,68,204,247,186,199,327,40,144,434,376,436,526,535,458,386,462,405,499,321,361,341,424,435,477,424,319,495,381,193,228,166,397,352,309,335,308,323,500,134,111,98,254,465,389,163,89,527,516,554,314,450,471,495,353,467,412,140,420,393,485,463,310,356,553,380,357,321,519,347,292,353,248,478,404,518,522,502,527,371,385,492,539,283,521,391,361,433,332,485,490,518,567,370,388,544,499,271,464,331,390,517,341,386,392,479,357,515,429,474,287,549,429,528,397,273,425,400,375,488,365,279,459,410,569,541,491,401,427,411,387,507,501,307,428,399,498,428,465,358,445,511,491,410,446,397,462,346,382,450,478,343,328,543,445,316,419,307,484,456,266,288,380,291,351,298,304,395,348,210,300,257,477,282,297,373,264,504,393,295,426,365,286,365,314,324,224,286,373,498,327,475,238,234,438,444,533,499,362,551,473,319,276,246,307,509,408,283,377,443,13,67,37,166,42,98,381,30,224,97,76,41,49,98,228,7,193,75,53,495,86,228,51,28,321,319,111,24,89,192,134,54,53,465,37,7,309,60,40,106,50,335,45,352,73,500,156,397,140,64,51,235,18,43,7,91,361,308,7,41,22,254,163,308,94,323,49,62,289,93,74,165,30,95,35,23,124,95,148,202,167,50,341,58,194,79,149,111,192,160,153,57,34,16,56,82,203,153,109,17,148,18,58,6,59,82,6,76,161,228,101,146,266,154,94,8,144,126,186,75,554,103,60,80,365,516,185,287,72,79,101,8,8,89,155,65,211,7,118,195,134,90,166,81,42,132,261,517,9,48,104,71,60,48,50,87,99,53,19,518,195,9,195,132,62,55,73,213,116,554,212,30,76,84,44,92,13,166,327,79,102,157,74,106,116,128,127,59,104,169,58,90,229,39,204,55,71,154,207,78,444,431,359,420,537,389,447,523,556,503,378,394,342,500,434,140,185,194,143,444,260,361,166,385,305,222,220,101,153,126,147,100,132,75,71,86,242,174,140,66,147,198,67,98,426,159,56,347,508,392,528,434,385,322,470,65,546,543,289,449,292,566,377,454,363,983,378,388,300,497,483,534,498,356,516,513,384,380,278,163,324,474,112,398,335,509,286,490,389,360,345,336,365,265,240,312,317,484,496,451,60,98,16,9,140,500,75,147,185,16,45,147,101,71,60,26,143,100,260,8,194,33,20,9,16,434,166,153,222,82,49,40,21,10,35,488,188,65,7,16,7,67,17,30,132,6,7,38,61,166,444,43,66,126,33,7,86,56,60,219,34,28,60,26,54,49,64,7,381,37,16,63,14,59,61,30,72,140,39,177,37,19,41,159,51,24,41,69,61,7,124,13,37,385,47,198,163,49,162,120,396,343,450,507,302,197,92,399,132,77,369,217,128,124,77,125,104,108,119,102,95,54,49,71,124,92,73,60,120,46,43,112,195,231,167,105,32,195,87,53,32,155,140,390,182,367,51,527,22,544,148,380,140,598,394,156,560,420,491,36,37,518,480,416,503,416,99,66,553,116,398,362,364,477,463,423,346,483,351,543,458,400,314,378,349,374,444,315,428,540,312,474,473,385,514,499,472,427,405,509,41,13,15,132,18,7,77,92,13,46,23,29,15,135,11,14,7,26,9,8,8,13,16,13,14,45,20,105,99,77,7,56,16,45,13,66,197,9,32,35,48,9,106,112,36,31,16,62,14,13,102,26,22,43,7,54,42,31,128,35,34,7,41,77,95,399,16,65,140,28,53,7,20,40,58,13,48,49,31,13,31,22,13,132,18,33,15,7,17,16,7,9,22,25,45,124,14,21,13,9,502,398,500,392,463,340,535,504,559,522,443,439,375,525,794,479,517,542,463,522,433,387,383,335,462,531,365,310,338,345,412,295,426,333,531,500,393,524,317,302,335,457,486,410,507,506,360,227,312,491,248,370,523,463,287,457,422,494,280,281,488,445,341,519,444,433,327,293,324,320,476,275,474,474,301,348,160,520,544,225,414,438,231,323,509,513,344,527,340,574,306,293,317,239,508,509,368,511,228,188,330,62,95,227,335,57,324,199,76,160,11,462,110,124,225,268,19,295,266,124,22,407,52,243,90,103,489,345,170,436,184,77,214,149,166,59,31,311,19,383,476,33,280,166,310,426,511,188,239,112,69,94,80,109,180,217,293,540,449,112,144,185,166,228,33,19,43,128,25,79,137,42,173,30,217,159,126,134,279,473,193,167,41,231,28,242,130,387,67,141,270,40,11,560,50,158,15,279,88,264,485,403,479,444,410,512,506,388,360,370,525,373,357,494,410,494,362,336,362,318,481,490,458,333,285,297,239,574,167,141,212,355,512,118,395,243,256,170,178,126,278,541,410,155,531,510,460,534,314,301,341,427,496,214,180,139,209,214,156,54,492,251,214,176,226,463,414,88,501,430,405,336,292,331,533,443,459,451,528,502,314,285,518,566,457,317,413,582,340,491,289,122,315,246,496,332,239,337,431,528,12,20,54,158,141,161,126,362,239,336,333,301,243,118,492,214,256,17,209,156,170,167,297,463,203,39,315,315,57,24,574,26,318,122,285,285,212,60,119,458,490,481,178,141,155,72,355,180,57,134,139,278,214,226,93,85,21,541,48,90,510,84,132,163,314,206,195,239,176,88,45,534,301,50,30,531,86,246,341,95,200,512,184,77,496,51,251,35,73,214,166,100,243,410,52,149,32,177,460,21,539,406,447,338,353,403,451,413,404,227,238,232,215,506,495,510,412,366,145,557,234,224,107,252,432,141,313,208,159,447,126,55,555,82,131,217,86,132,247,319,179,378,188,213,86,124,185,374,517,338,514,533,512,526,438,510,382,317,358,540,378,514,360,465,412,122,311,491,397,456,343,543,391,533,436,349,387,24,53,524,133,276,442,497,506,457,503,445,533,349,461,499,349,371,346,401,304,444,322,451,393,358,454,299,252,333,274,327,389,337,282,433,425,475,549,439,326,349,387,398,318,241,463,536,483,334,463,323,276,341,511,363,555,564,527,418,241,327,329,409,298,384,355,306,315,276,416,270,301,370,359,556,260,421,305,288,335,268,257,412,593,456,390,490,501,298,269,308,256,291,437,425,493,376,396,511,318,465,307,507,516,296,283,303,316,321,357,286,450,309,242,461,296,272,217,521,311,328,286,387,250,55,232,227,126,510,135,159,24,82,53,17,177,122,81,145,234,131,132,63,56,11,224,49,76,404,217,107,14,65,108,53,57,187,86,413,208,506,17,59,107,215,238,23,432,86,137,141,43,27,366,412,86,54,689,22,94,188,134,366,29,124,109,495,12,555,382,179,133,23,118,247,52,196,555,16,122,41,17,18,128,77,57,132,105,67,120,313,447,319,76,106,213,135,252,378,51,105,71,59,185,311,40,23,92,557,211,111,58,143,56,69,175,92,22,272,255,196,164,128,369,152,138,49,167,30,195,50,118,76,107,176,92,103,118,39,236,284,145,123,118,90,178,214,52,107,51,152,142,24,91,144,82,52,140,38,48,168,200,229,28,71,89,64,87,269,66,366,281,197,431,49,374,164,143,162,44,74,56,239,225,131,139,204,238,115,29,13,44,7,314,271,109,93,78,232,168,178,141,23,369,499,416,234,372,449,464,197,305,201,559,179,297,231,171,183,148,198,186,180,229,106,218,135,186,169,159,220,199,161,223,83,72,92,110,124,106,56,156,200,142,144,184,118,116,175,82,497,450,466,310,507,515,505,521,305,172,156,502,413,381,481,356,389,350,57,561,541,524,522,428,432,254,364,523,576,490,503,534,284,141,317,519,377,368,506,269,342,396,365,506,535,277,390,341,332,556,316,540,329,175,559,143,20,149,305,106,20,416,18,108,234,66,56,73,183,372,92,122,110,83,89,62,98,99,171,72,449,20,197,21,54,67,159,201,79,26,297,32,198,139,148,82,57,464,7,78,179,58,24,135,223,20,41,89,68,186,87,33,7,44,30,52,7,26,106,186,7,124,180,231,9,7,68,161,138,58,33,52,22,66,7,84,184,62,63,142,101,172,44,128,58,254,171,199,60,118,220,88,46,505,281,314,345,270,283,253,286,371,349,147,114,138,311,184,136,160,111,141,209,164,187,117,106,300,217,262,52,133,143,124,249,263,95,158,251,287,209,154,109,111,75,72,91,88,113,78,102,125,71,68,123,86,112,100,123,55,91,173,111,183,359,452,158,56,112,75,117,116,62,152,68,77,93,399,46,552,438,305,60,38,103,367,135,363,341,34,66,33,58,35,114,450,51,443,311,394,391,432,365,147,18,25,58,443,9,33,9,16,15,28,16,184,15,16,7,51,7,314,35,22,20,82,38,66,17,114,40,38,114,41,281,21,136,17,15,8,70,253,75,225,16,16,79,38,35,31,28,16,7,33,111,8,30,23,32,19,58,75,117,164,14,52,9,22,14,136,62,61,72,48,177,18,47,30,117,16,58,17,7,62,92,26,187,6,14,48,41,23,8,29,9,30,138,23,9,262,34,9,23,526,568,345,431,336,494,242,315,531,298,305,288,305,257,335,508,418,402,443,440,370,456,409,178,324,250,444,342,178,223,198,195,216,214,210,208,177,239,189,237,235,444,339,174,233,176,248,278,221,541,162,183,536,350,311,274,260,312,215,479,167,181,312,204,175,216,200,205,263,193,224,227,243,163,155,201,293,247,310,287,425,541,366,504,540,390,173,165,276,117,277,154,165,153,237,164,124,355,202,142,27,9,111,24,208,16,100,187,160,70,57,26,319,86,230,126,184,85,12,69,94,90,42,77,8,138,59,75,54,59,365,177,75,131,37,69,114,339,44,17,308,84,363,176,58,124,119,257,242,69,19,271,526,144,49,149,241,119,237,75,97,202,276,191,7,15,112,298,74,223,136,117,125,21,198,125,162,113,196,135,178,124,226,75,149,236,295,200,216,108,210,24,32,56,48,21,444,105,317,133,418,472,494,316,323,246,437,307,185,390,226,490,265,340,206,183,216,203,169,258,141,399,318,297,164,98,328,105,383,119,154,471,148,157,158,172,97,77,91,76,90,165,80,145,265,185,141,202,268,190,219,184,70,189,176,93,99,195,124,476,359,505,316,505,532,137,417,552,491,494,550,380,535,493,319,486,443,267,429,267,328,277,270,480,269,147,139,281,267,65,72,345,540,70,509,164,115,138,67,362,31,56,164,185,30,37,6,44,55,418,60,93,28,42,65,28,89,66,105,34,233,390,316,142,36,60,70,72,172,183,97,235,246,99,63,213,226,90,76,87,494,36,44,85,47,165,77,9,11,23,14,67,437,80,323,70,145,472,72,45,141,107,198,82,83,119,91,107,154,74,68,98,307,410,114,41,164,19,96,157,39,176,169,46,471,125,19,45,12,124,139,39,34,64,206,148,42,158,14,38,370,448,159,162,197,178,526,535,341,242,198,268,172,162,101,226,162,131,136,79,94,251,163,126,194,260,196,133,174,101,162,173,127,182,212,144,132,113,116,310,141,492,40,486,45,272,303,325,472,243,171,429,226,272,102,307,398,68,204,92,278,122,44,253,138,206,9,444,48,319,112,347,42,196,39,37,240,508,193,72,46,222,77,67,195,212,194,190,36,81,306,271,169,282,511,404,290,463,203,62,226,227,163,183,576,269,262,168,157,182,31,51,226,177,271,158,219,297,362,255,320,183,182,224,237,267,207,249,233,149,246,220,304,273,311,601,128,191,304,155,224,212,220,153,236,232,147,222,115,287,123,181,334,354,517,132,226,129,419,188,146,337,271,235,191,238,176,236,247,160,172,317,161,203,179,92,518,219,200,261,203,197,137,122,164,182,117,275,228,273,128,43,169,218,133,157,154,209,23,222,72,92,162,159,43,136,45,9,94,79,9,17,57,35,44,101,73,163,74,14,174,62,172,9,162,7,212,37,39,242,178,198,18,12,196,370,81,51,23,126,182,36,197,18,42,11,7,113,101,195,39,26,40,29,7,75,60,131,47,226,102,268,162,14,127,448,68,7,12,11,123,10,404,48,310,122,492,49,162,132,28,171,138,341,28,45,112,98,77,101,92,161,58,43,35,72,193,141,38,29,24,89,58,46,29,173,163,67,31,56,251,44,67,18,34,56,119,12,17,23,29,7,6,526,133,260,48,78,27,59,535,47,26,50,37,18,196,43,194,18,6,18,15,144,7,34,116,21,26,183,212,271,25,72,133,9,42,14,40,71,20,16,9,16,168,181,157,20,21,7,56,72,9,46,125,235,117,60,38,38,147,132,444,16,15,25,36,63,145,10,68,45,122,5,17,52,42,61,87,93,461,494,206,326,216,275,297,236,100,179,213,196,215,220,78,153,501,171,65,73,107,254,72,166,117,383,241,348,265,64,73,211,90,293,69,414,256,52,88,411,87,91,84,83,526,199,111,74,425,457,170,92,571,187,301,46,213,233,501,205,164,229,214,264,243,245,207,199,133,474,183,219,172,307,184,244,212,141,154,226,146,145,168,223,242,135,143,179,112,182,473,257,175,193,269,165,202,180,199,60,90,89,7,19,53,72,46,27,206,34,216,461,29,213,179,20,153,69,9,494,100,12,9,7,9,78,38,107,7,26,60,9,7,16,166,275,73,23,15,13,65,9,10,32,326,12,7,23,18,12,9,21,52,16,199,28,51,7,15,7,103,56,28,28,64,220,7,6,73,30,84,69,27,9,6,32,501,40,69,91,83,319,215,236,61,51,39,58,16,37,54,62,61,49,92,7,38,10,5,442,474,302,214,214,196,104,124,333,147,163,88,81,146,40,86,61,134,180,62,70,65,74,170,98,430,284,213,110,197,215,62,89,102,70,37,117,394,60,175,111,87,73,46,436,84,108,85,45,119,45,66,145,42,641,184,66,92,318,62,69,35,34,30,22,36,27,47,26,53,73,48,76,79,21,146,381,31,33,107,238,25,100,59,56,41,280,86,114,137,87,196,55,137,70,48,97,111,91,367,16,85,12,18,27,16,127,73,9,47,24,7,22,46,20,7,10,7,33,21,101,7,9,84,7,9,8,7,9,7,38,159,7,34,21,18,38,16,19,41,16,9,146,20,9,32,7,10,9,15,26,9,30,68,13,33,19,21,184,17,7,6,9,8,13,39,34,7,7,9,70,9,9,40,9,29,13,22,9,23,15,9,13,48,81,7,7,7,7,7,7,41,43,43,88,29,9,111,16,306,539,553,291,270,363,524,475,466,261,264,318,262,315,193,308,304,250,295,210,181,328,536,245,197,281,405,301,272,302,206,263,245,187,148,211,256,128,305,181,378,177,155,317,388,307,317,217,233,155,450,598,273,299,406,301,138,164,207,436,149,514,197,160,157,159,249,232,129,296,437,209,373,126,245,322,212,397,94,148,295,346,372,296,236,144,165,141,101,191,113,85,148,191,140,79,169,302,134,104,217,97,76,172,295,232,141,60,62,114,28,25,101,53,315,176,191,164,51,524,25,49,94,116,129,264,17,59,233,165,115,79,65,40,129,69,120,85,31,536,328,193,181,47,58,69,21,197,91,139,126,346,64,245,207,138,77,22,159,27,29,236,107,280,79,63,19,31,25,70,22,433,78,38,55,25,169,317,88,148,148,41,29,43,113,11,22,31,261,155,126,23,45,47,56,27,36,24,143,38,431,457,211,381,210,210,333,448,316,263,163,145,574,505,246,139,105,141,299,181,182,278,169,143,97,126,129,66,156,200,138,93,232,237,162,107,168,155,92,175,358,137,455,157,379,417,81,505,477,368,499,389,299,464,289,475,507,481,334,313,344,517,526,469,385,468,382,487,461,270,360,130,297,365,545,480,364,512,239,440,400,196,280,151,102,92,115,369,127,109,391,317,141,66,83,71,311,537,197,466,92,102,28,30,92,43,143,64,139,311,105,109,37,461,71,138,187,83,56,102,163,103,141,71,263,98,210,156,457,17,30,164,52,129,86,51,145,211,81,29,129,58,66,107,51,111,93,126,200,210,169,431,66,333,97,108,574,181,381,26,151,76,141,358,21,151,232,137,38,155,148,51,12,311,168,246,39,239,316,147,100,74,278,127,182,133,115,98,44,117,115,67,42,237,86,53,91,16,193,115,516,289,353,298,201,255,204,117,200,343,184,241,125,189,159,111,118,231,98,123,53,59,139,129,215,183,312,541,98,336,120,50,211,296,124,142,268,2115,365,500,142,182,447,123,455,422,473,91,254,99,162,473,481,565,372,507,549,360,501,314,332,524,449,199,258,342,256,282,246,356,469,388,354,269,74,483,280,200,382,372,203,162,85,394,116,207,530,324,273,487,339,312,394,214,317,386,443,405,278,462,115,513,132,50,107,107,55,64,217,329,410,257,267,350,126,227,96,98,497,419,487,187,230,256,264,462,252,343,329,284,110,193,264,289,379,301,276,284,215,122,253,251,214,336,352,310,198,136,167,223,162,323,202,319,99,275,230,303,176,160,247,357,170,220,75,224,274,240,118,311,351,366,248,265,219,174,236,236,475,293,250,220,268,255,292,418,339,243,202,194,185,199,276,135,355,158,278,211,194,306,125,117,200,353,99,343,184,118,98,99,241,41,101,51,53,201,59,129,91,54,107,55,159,264,55,123,63,50,204,9,231,123,64,119,111,21,289,246,107,50,98,516,95,189,336,132,115,255,139,85,42,121,298,162,153,541,36,17,312,74,48,183,7,116,142,196,15,7,96,356,42,124,29,142,42,93,74,110,215,78,122,75,115,126,38,117,120,296,88,64,51,136,182,75,61,211,98,36,83,203,227,168,162,60,36,268,52,36,94,162,144,178,23,206,38,487,79,223,37,94,116,95,152,13,26,86,56,117,118,113,54,98,112,121,329,173,286,160,129,97,81,134,24,59,96,121,77,29,167,117,394,30,259,14,184,447,207,53,30,127,145,204,93,158,73,25,44,118,225,122,92,21,13,97,90,230,123,27,145,32,95,118,124,150,39,198,241,68,257,185,7,59,360,7,116,133,175,16,29,36,416,350,557,430,507,287,258,225,184,168,133,120,307,268,166,106,123,107,109,107,300,280,104,44,127,96,46,99,90,97,94,121,211,337,266,223,69,414,253,548,97,244,283,66,354,324,443,455,266,512,332,513,532,409,352,317,363,271,491,340,409,325,296,210,272,483,328,330,282,302,386,428,370,253,298,449,232,515,245,449,356,304,466,391,421,126,362,217,477,174,435,270,319,341,108,181,218,249,97,248,46,7,65,29,44,32,104,225,123,66,46,557,127,22,268,244,430,96,65,97,166,14,109,6,97,16,31,106,150,78,121,507,79,107,7,19,94,40,16,168,69,73,258,99,90,67,184,120,44,21,143,42,107,67,38,74,17,287,31,60,59,16,307,184,88,133,168,34,97,18,337,173,42,498,12,80,37,56,21,39,59,62,83,47,41,58,12,86,99,138,19,9,38,7,47,62,90,280,61,7,520,373,483,290,391,262,260,204,171,197,168,642,133,263,130,392,144,135,180,177,167,517,267,144,147,104,87,550,108,171,181,259,136,97,109,154,104,102,90,110,314,86,297,385,108,76,68,57,62,175,91,115,76,79,84,57,89,66,83,178,104,93,70,74,95,83,139,94,62,77,102,84,99,61,52,64,81,49,64,97,50,69,206,182,301,200,202,160,170,126,107,67,42,37,52,49,76,75,133,65,43,88,111,26,16,16,68,23,43,16,61,7,9,7,24,49,22,31,11,49,38,15,16,104,42,22,13,20,9,9,9,7,34,47,14,41,31,15,15,52,16,27,7,16,16,260,16,18,13,7,28,57,9,32,27,20,17,13,45,27,33,16,7,23,27,34,16,196,53,58,17,16,24,13,26,62,16,16,30,147,8,191,45,31,57,22,16,385,16,29,97,16,16,13,7,13,23,108,14,7],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#f2f5fa\"},\"error_y\":{\"color\":\"#f2f5fa\"},\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"baxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#506784\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"header\":{\"fill\":{\"color\":\"#2a3f5f\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#f2f5fa\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#f2f5fa\"},\"geo\":{\"bgcolor\":\"rgb(17,17,17)\",\"lakecolor\":\"rgb(17,17,17)\",\"landcolor\":\"rgb(17,17,17)\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#506784\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"dark\"},\"paper_bgcolor\":\"rgb(17,17,17)\",\"plot_bgcolor\":\"rgb(17,17,17)\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"radialaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"yaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"zaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"}},\"shapedefaults\":{\"line\":{\"color\":\"#f2f5fa\"}},\"sliderdefaults\":{\"bgcolor\":\"#C8D4E3\",\"bordercolor\":\"rgb(17,17,17)\",\"borderwidth\":1,\"tickwidth\":0},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"caxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"updatemenudefaults\":{\"bgcolor\":\"#506784\",\"borderwidth\":0},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"count\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"relative\",\"title\":{\"text\":\"Distribution of token amount\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ef6ec8ae-66ff-43b2-9bf6-d0f06fb74d94');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokens_amount = [len(tokens) for tokens in df.tokens.values]\n",
        "\n",
        "fig = px.histogram(\n",
        "    tokens_amount,\n",
        "    nbins=100,\n",
        "    color_discrete_sequence=['orange']\n",
        ")\n",
        "\n",
        "fig.layout.title = 'Distribution of token amount'\n",
        "fig.layout.template = 'plotly_dark'\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "cv52_u_VdjcY"
      },
      "outputs": [],
      "source": [
        "df['sentimento'] = df.sentimento.map(dict({\n",
        "    'negative': 0,\n",
        "    'neutral': 1,\n",
        "    'positive': 2\n",
        "}))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o4EuVi9iUUiY"
      },
      "source": [
        "# Getting the BERT tokens for the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P6_U7IpUTQl",
        "outputId": "570d5cf6-bd32-453a-e03b-f2a71e8022d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INPUT IDS\n",
            "tensor([[  101,  1651, 22303,   117,  1966,   253,   222,  3515,   106,   133,\n",
            "           511,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0]])\n",
            "\n",
            "ATTENTION MASK\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "tt = 'OlÃ¡, esse Ã© um teste! <3'\n",
        "test_tokens = tokenizer.encode_plus(\n",
        "    tt,\n",
        "    max_length=32,\n",
        "    add_special_tokens=True,\n",
        "    return_attention_mask=True,\n",
        "    return_token_type_ids=True,\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "print('INPUT IDS')\n",
        "print(test_tokens['input_ids'])\n",
        "print('\\nATTENTION MASK')\n",
        "print(test_tokens['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "SGqxwVIHViFd"
      },
      "outputs": [],
      "source": [
        "def get_tokens(text):\n",
        "  encodes = tokenizer.encode_plus(\n",
        "      text,\n",
        "      max_length=512,\n",
        "      add_special_tokens=True,\n",
        "      return_attention_mask=True,\n",
        "      return_token_type_ids=True,\n",
        "      truncation=True,\n",
        "      padding='max_length',\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "\n",
        "  return dict({\n",
        "      'input_ids': encodes['input_ids'].squeeze(),\n",
        "      'attention_mask': encodes['attention_mask'].squeeze()\n",
        "  })\n",
        "\n",
        "df['input_ids'], df['attention_mask'] = df.texto_refinado.apply(lambda x: get_tokens(x)['input_ids']), df.texto_refinado.apply(lambda y: get_tokens(y)['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBxDw-P0WytX",
        "outputId": "e4a5bd0e-0df3-41d2-bb1b-f8403c6e2775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "512 512 512.0\n"
          ]
        }
      ],
      "source": [
        "sizes = list([])\n",
        "for i in df.input_ids:\n",
        "  sizes.append(i.size()[0])\n",
        "\n",
        "print(np.array(sizes).max(), np.array(sizes).min(), np.array(sizes).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdG7UBUIXI0V",
        "outputId": "5b4b5e94-cc57-45bd-e13e-e832c1d9cea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32])"
            ]
          },
          "execution_count": 214,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_tokens['input_ids'].squeeze().size()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gorv-toJZZY2"
      },
      "source": [
        "# Creating the Pytorch dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "RTVAiIH4XUDK"
      },
      "outputs": [],
      "source": [
        "class CustomDF(Dataset):\n",
        "  def __init__(self, text, targets, tokenizer, max_len):\n",
        "    self.text = text\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    text = str(self.text[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_len,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    return dict({\n",
        "        'text': text,\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'targets': torch.tensor(target, dtype=torch.int64)\n",
        "    })"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YhLDOjFIajiO"
      },
      "source": [
        "# Splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iC4XdaMdCWb",
        "outputId": "9a103f28-ec4c-4aa9-8952-550b6419c444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amount of entries for train: 8636\n",
            "Amount of entries for test: 1080\n",
            "Amount of entries for validation: 1080\n"
          ]
        }
      ],
      "source": [
        "SEED = 20\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=.2, random_state=SEED)\n",
        "val_df, test_df = train_test_split(test_df, test_size=.5, random_state=SEED)\n",
        "\n",
        "print(f'Amount of entries for train: {train_df.shape[0]}')\n",
        "print(f'Amount of entries for test: {test_df.shape[0]}')\n",
        "print(f'Amount of entries for validation: {val_df.shape[0]}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qYRuJsxmdD01"
      },
      "source": [
        "# Creating a function to create dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "QV2_FgjGdG3z"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  dataset = CustomDF(\n",
        "      text=df.texto_refinado.to_numpy(),\n",
        "      targets=df.sentimento.to_numpy(dtype=int),\n",
        "      tokenizer=tokenizer,\n",
        "      max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "      dataset=dataset,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=2,\n",
        "      drop_last=True\n",
        "  )\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "MAX_LEN = 128\n",
        "\n",
        "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-QzqJATe8fe",
        "outputId": "14111521-2c11-4d85-a07e-10f283876b7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': ['a nova atualizaÃ§Ã£o pile_of_poo ficou horrÃ­vel navegaÃ§Ã£o app pÃ©ssima nÃ£o contar empresas pÃµem valor mÃ­nimo compra poder assim fazer venda casada obrigando cliente comprar precisa faturar valores entrega muitas vezes alto prÃ³prio pedido onde viu paga 30 00 entrega ??? loucura',\n",
              "  'pÃ©ssimo todos sentidos gostaria conhecer gÃªnio inventou fechar refrigetante papel poi entrega nao entorne aÃ­ pede pedido feito restaurante 5km distÃ¢ncia tendo lado ). resumindo sempre chega frio molhado jÃ¡ app permite cancelar pedido atendimento demora 3h poderia escrever ma limite caracteres permite',\n",
              "  'a navegaÃ§Ã£o site lenta aplicativo miui traz experiÃªncia conflitante muito demorado carrega fazer qualquer coisa fazer primeiro pedido beira frustraÃ§Ã£o difÃ­cil novo usuÃ¡rio entender acontecendo taxas cadastrar coisas bÃ¡sicas endereÃ§o nÃºmero telefone tanta lentidÃ£o dÃºvida chega questionar pedido realmente feito fora estÃ©tica excelente layout sÃ³ comprei desconto',\n",
              "  'pop',\n",
              "  'app legal porÃ©m muitos restaurantes gostaria pedir dizem estarem disponÃ­veis prÃ³pria cidade alguns pedidos limite forma pagamento cash cartÃ£o ), quero pagar achar melhor e lugares vendem coisa entregam outra ma culpam uber erro e preÃ§os alto promoÃ§Ãµes ilusÃ³rias app legal sim quando problema',\n",
              "  'infelizmente tido problemas todos pedidos recentes fiz jÃ¡ dei nova chance restaurantes pedindo outros apps pedidos vieram direitinho nÃ£o sei forma passado pedido restaurante confusa porÃ©m evitado selecionar itens adicionais promoÃ§Ãµes especÃ­ficas combo diferentes quanto valor entregue errado sempre ressarcida demora fica dor cabeÃ§a ter adaptar cardÃ¡pio pedido recebi',\n",
              "  'aplicativo bugado',\n",
              "  'bom ma falta un cupons',\n",
              "  'foi bom tempo mudanÃ§as usei',\n",
              "  'falta melhorar opÃ§Ãµes compartilhar pedido opÃ§Ãµes selecionar pedidos anteriores ter ir restaurante escolhar tudo denovo aplicativo falha muita vezes dando erro aplicativo informaÃ§Ã£o entrega 3 estrelas porque ... fim contas comida chega come beleza ma perguntar bom ... pra bom falta ainda entrega mÃ­nimo mÃ­nimo bem fraco',\n",
              "  'nÃ£o nd dizer pessoas nn sabe usar app ma fÃ¡cil',\n",
              "  'o app deixa pagar hora entrega ....',\n",
              "  'eu gosto app shopping ventura deram batata salgada socorro puro sal ksksk voltei loja fechados sei reclamar',\n",
              "  'layout poderia prÃ¡tico mudar endereÃ§o trocar opÃ§Ãµes zera carrinho nÃ£o sei time usuÃ¡rio olhou',\n",
              "  'o app bom mas dei 4 estrelas poderem sempre melhorar',\n",
              "  'o app simplesmente horrÃ­vel consigo usar direito jÃ¡ faz tempo aparece opÃ§Ãµes restaurantes prÃ³ximos pÃ¡gina inicial preciso ir busca entrar cada categoria ver agora digito nome restaurante aparece contar cupons comprei aparecendo todos restaurantes usar',\n",
              "  'excelente',\n",
              "  'o app bom ma vou pegar cupons sempre aparece atingiu limite cupons ma atingi sugiro arrume bug',\n",
              "  'excelente app tudo fÃ¡cil rapido',\n",
              "  'antigamente otimo app agora quer entra app ruim',\n",
              "  'nÃ£o achei opÃ§Ã£o contato restaurante precisei modificar pedido apÃ³s fazÃª lo consegui',\n",
              "  'estÃ¡ hora colocarem campos endereÃ§os tipo padrÃ£o casa outro alternativo assim facilita vida pessoas velhas tÃªm dificuldades fora lixo fica histÃ³rico endereÃ§o',\n",
              "  'gostava bastante opÃ§Ã£o pagamento online android Ã©poca 10 cashback pago via picpay aÃ­ tiraram es 10 %, ma tudo bem ), ma infelizmente tiraram sei motivo procedimento ficou menos prÃ¡tico uma pena gostaria saber justificativa retirada opÃ§Ã£o pagamento online',\n",
              "  'nÃ£o gostei pago pix vcs estornam pagar crÃ©dito pra dar certo pedido amo esfihas habbs',\n",
              "  'fiz compra supermercado entregador conseguiu passa la caixa poi algum problema sistema app tive cancelar pedido vou ter esperar 10 dia receber reembolso ainda vou ficar comida casa poi precisando des produtos urgentemente bem ainda dinheiro reserva imagina passando dificuldade ?! ficaria passando fome causa incompetÃªncia desse aplicativo pÃ©ssimo nÃ£o caiam nessa roubada',\n",
              "  'precisa melhorar nÃ£o opÃ§Ã£o pedir troco isso atrapalha clientes entregadores poderia ter aplicativo opÃ§Ã£o troco prÃ³prio restaurante mandar porque facilitava ter ir atrÃ¡s destrocar dinheiro tomar tempo entregadores comeÃ§ando dinheiro ainda',\n",
              "  'gosto serviÃ§o',\n",
              "  'Ã© bom ma pode melhorar o app bom boa promoÃ§Ãµes o pode melhorar ter opÃ§Ã£o classificar estabelecimento porque compramos ter noÃ§Ã£o local bom ruim',\n",
              "  'frete caro preÃ§os conta alÃ©m valor mÃ­nimo pedido ter aumentado outros aplicativos conta desinstalei app',\n",
              "  'bom serviÃ§o simple utilizar prÃ¡tico',\n",
              "  'aplicativo proposta boa ma comprar modalidade peÃ§a retire hora pagamento opÃ§Ãµes pagamento dÃ©bito crÃ©dito simplesmente abrem toda outras formas pagamento abrem menos duas',\n",
              "  'essa atualizaÃ§Ã£o ta bugando app nao acesso'],\n",
              " 'input_ids': tensor([[  101,   123,   940,  ...,     0,     0,     0],\n",
              "         [  101, 20938,  1211,  ...,     0,     0,     0],\n",
              "         [  101,   123, 11300,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [  101,  4062,  2576,  ...,     0,     0,     0],\n",
              "         [  101, 16357,  4562,  ...,     0,     0,     0],\n",
              "         [  101,  1921, 14429,  ...,     0,     0,     0]]),\n",
              " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
              " 'targets': tensor([0, 0, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 0, 2, 0, 2, 0, 2, 1, 1, 0, 1, 1,\n",
              "         0, 1, 2, 2, 0, 2, 1, 0])}"
            ]
          },
          "execution_count": 218,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = next(iter(train_data_loader))\n",
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POZ9f1hPfGeV",
        "outputId": "6ef4ee8b-8632-423e-be04-d396b627aae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "print(train_data['input_ids'].size())\n",
        "print(train_data['attention_mask'].size())\n",
        "print(train_data['targets'].size())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KVZyirDgfO6l"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "i2m2OOvwfQE0"
      },
      "outputs": [],
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "    self.drop = nn.Dropout(p=.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    bmodel = self.bert(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    attentions = bmodel.attentions\n",
        "    cross_attentions = bmodel.cross_attentions\n",
        "    hidden_states = bmodel.hidden_states\n",
        "    last_hidden_state = bmodel.last_hidden_state\n",
        "    past_key_values = bmodel.past_key_values\n",
        "    pooler_output = bmodel.pooler_output\n",
        "\n",
        "    output = self.drop(pooler_output)\n",
        "\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PACsBLwgKaM",
        "outputId": "4a397f33-fcd5-4974-dc1c-55384a095787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 2]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The device will be using CUDA\n"
          ]
        }
      ],
      "source": [
        "print(df.sentimento.unique())\n",
        "class_names = list(['negative', 'neutral', 'positive'])\n",
        "\n",
        "model = SentimentClassifier(len(class_names))\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'The device will be using {str.upper(device)}')\n",
        "model = model.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WQSUpkc2jZ_b"
      },
      "source": [
        "# Training\n",
        "\n",
        "Reproducing the procedure from the original BERT paper using the AdamW optimizer by Hugging Face.\n",
        "It corrects weight decay. I'll be also using a linear scheduler with no warmup steps. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "rQd1WbZHjtIy"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 8\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=5e-5\n",
        ")\n",
        "\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FAe7bl_JnQM9"
      },
      "source": [
        "## BERT authors recommendations for fine-tuning\n",
        "\n",
        "| Hyperparameter | Value |\n",
        "| -------- | -------- |\n",
        "| Batch size | `16`, `32` |\n",
        "| Learning rate (Adam) | `5e-5`, `3e-5`, `2e-5` |\n",
        "| Number of epochs | `2`, `3`, `4` | \n",
        "\n",
        "I'll ignore the number of epochs recommendation but stick with the rest.\n",
        "\n",
        "> NOTE: \n",
        "> Increasing the batch size reduces the training time significantly, but gives lower accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "sK53BurrkNOr"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = list([])\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for data in data_loader:\n",
        "    input_ids = data['input_ids'].to(device)\n",
        "    attention_mask = data['attention_mask'].to(device)\n",
        "    targets = data['targets'].to(device)\n",
        "\n",
        "    output = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, pred = torch.max(output, dim=1)\n",
        "    loss = loss_fn(output, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(pred == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = list([])\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in data_loader:\n",
        "      input_ids = data['input_ids'].to(device)\n",
        "      attention_mask = data['attention_mask'].to(device)\n",
        "      targets = data['targets'].to(device)\n",
        "\n",
        "      output = model(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask\n",
        "      )\n",
        "\n",
        "      _, pred = torch.max(output, dim=1)\n",
        "      loss = loss_fn(output, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(pred == targets)\n",
        "      losses.append(loss.item())\n",
        "  \n",
        "  return correct_predictions / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjJzXGyvppuK",
        "outputId": "6dda6fab-8d14-4796-ef7a-5f6492430077"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-05-10 01:03:09.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mEPOCH 1/8\u001b[0m\n",
            "\u001b[32m2023-05-10 01:05:54.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mTRAIN LOSS: 0.8204962819482314  |  ACCURACY: 0.6082677245140076\u001b[0m\n",
            "\u001b[32m2023-05-10 01:06:01.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mVALIDATION LOSS: 0.792832124413866  |  ACCURACY: 0.6268518567085266\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m2023-05-10 01:06:03.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mEPOCH 2/8\u001b[0m\n",
            "\u001b[32m2023-05-10 01:08:55.383\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mTRAIN LOSS: 0.604567931265636  |  ACCURACY: 0.7361046671867371\u001b[0m\n",
            "\u001b[32m2023-05-10 01:09:03.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mVALIDATION LOSS: 0.906921948447372  |  ACCURACY: 0.6222221851348877\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m2023-05-10 01:09:03.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mEPOCH 3/8\u001b[0m\n",
            "\u001b[32m2023-05-10 01:11:57.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mTRAIN LOSS: 0.4040720313233514  |  ACCURACY: 0.8418248891830444\u001b[0m\n",
            "\u001b[32m2023-05-10 01:12:04.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mVALIDATION LOSS: 1.0174239657141946  |  ACCURACY: 0.6472222208976746\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m2023-05-10 01:12:06.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mEPOCH 4/8\u001b[0m\n",
            "\u001b[32m2023-05-10 01:15:00.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mTRAIN LOSS: 0.282453167449586  |  ACCURACY: 0.8996062874794006\u001b[0m\n",
            "\u001b[32m2023-05-10 01:15:08.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mVALIDATION LOSS: 1.3024714841987148  |  ACCURACY: 0.6314814686775208\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m2023-05-10 01:15:08.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mEPOCH 5/8\u001b[0m\n",
            "\u001b[32m2023-05-10 01:18:02.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mTRAIN LOSS: 0.20513046395075143  |  ACCURACY: 0.9279758930206299\u001b[0m\n",
            "\u001b[32m2023-05-10 01:18:09.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mVALIDATION LOSS: 1.3614619554895344  |  ACCURACY: 0.6379629373550415\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m2023-05-10 01:18:09.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mEPOCH 6/8\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "try:\n",
        "  for epoch in range(EPOCHS):\n",
        "    logger.info(f'EPOCH {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_data_loader,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        device,\n",
        "        scheduler,\n",
        "        len(train_df)\n",
        "    )\n",
        "\n",
        "    logger.info(f'TRAIN LOSS: {train_loss}  |  ACCURACY: {train_acc}')\n",
        "\n",
        "    val_acc, val_loss = eval_model(\n",
        "        model,\n",
        "        val_data_loader,\n",
        "        loss_fn,\n",
        "        device,\n",
        "        len(val_df)\n",
        "    )\n",
        "\n",
        "    logger.info(f'VALIDATION LOSS: {val_loss}  |  ACCURACY: {val_acc}\\n\\n')\n",
        "\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "    if val_acc > best_accuracy:\n",
        "      best_accuracy = val_acc\n",
        "      torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/models/best_model_state.bin')\n",
        "    \n",
        "  logger.success(f'MODEL TRAINED WITH {EPOCHS} EPOCHS AND ACHIEVED {best_accuracy} OF BEST ACCURACY ðŸŽ‰')\n",
        "\n",
        "except Exception as e:\n",
        "  logger.error(f'ERROR: {str.upper(str(e))}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTe6jWcPxmL4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
